{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload    \n",
    "%autoreload 2          \n",
    "\n",
    "import os \n",
    "import sys\n",
    "from pathlib import Path # To set data downloading path\n",
    "\n",
    "# Append ghostcoder folder to path \n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import ghostcoder\n",
    "\n",
    "from ghostcoder import GhostCoder\n",
    "from ghostcoder.utils import *\n",
    "from ghostcoder.graph import create_ghostcoder_agent, create_coder_agent, create_crawler_agent, create_retriever_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89124644",
   "metadata": {},
   "source": [
    "Here we will test and illustrate each subgraph in Ghostcoder, as an important component of BIA (bioinformatics agnet), mainly functions to complete the generation and execution of bioinformatics analysis codes. It contains five subgraphs. They are filemanager, retriever, coder and webcrawler executor respectively.\n",
    "\n",
    "\n",
    "### Omics data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8466b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#──────work_dir \n",
    "#  └───data\n",
    "#    └─Input_data.whatever\n",
    "# First lets download a scRNAseq data\n",
    "import scanpy as sc\n",
    "\n",
    "from ghostcoder.config import file_config\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Ghostcoder pre-set WORK_DIR and INPUT_DATA_DIR for continues bioinformatics tasks using one input data\n",
    "\n",
    "# Download scRNAseq data\n",
    "sc.settings.datasetdir = current_dir/ file_config.INPUT_DATA_DIR # Download data into data/ folder in current dir\n",
    "sc.datasets.pbmc3k()\n",
    "\n",
    "# Create a data description file to illustrate the scRNAseq data details \n",
    "data_des = \"The data used in this basic preprocessing and clustering tutorial was collected from bone marrow mononuclear cells of healthy human donors. The samples used in this tutorial were measured using the 10X Multiome Gene Expression and Chromatin Accessability kit.\"\n",
    "\n",
    "with open('data/data_description.txt','w') as f:\n",
    "    f.write(data_des)\n",
    "    \n",
    "# Set workdir as current dir\n",
    "file_config.WORK_DIR = current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb5572",
   "metadata": {},
   "source": [
    "### Set up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69382b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgi_api_key = 'sk-NYPE5WNGTK781Nox801a5934A9F84a03BfC525359eBaE7B6'\n",
    "bgi_api_base = 'http://10.224.28.80:3000/v1'\n",
    "\n",
    "openai_api_key = \"OYXxhyOyPDsJNABxugxjJJxntQ0B1EB0\"\n",
    "openai_api_base = \"https://api3.aitok.ai/openai/v1\"\n",
    "\n",
    "openai_chat_model = 'gpt-4o'\n",
    "openai_code_model = 'gpt-4o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Openai API for example, you can use other LLM provider as well\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# I will recommend use Openai API, I haven't tested any APIs from other suppliers yet\n",
    "def call_chatllm_openai(api_key, api_base, model_name):\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key = api_key,\n",
    "        openai_api_base=api_base,\n",
    "        model = model_name)\n",
    "    return llm\n",
    "\n",
    "# Setup up api keys \n",
    "# openai_api_key = \"\"\n",
    "# openai_api_base = \"\"\n",
    "# openai_chat_model = \"\" \n",
    "# openai_code_model = \"\"\n",
    "\n",
    "# Here, I recommend using an LLM with stronger coding capabilities as the code model, which is set to perform code-related work in all Ghostcoder graphs. Meanwhile, it is better to use LLM with stronger reasoning ability as the chat model.\n",
    "chat_model = call_chatllm_openai(openai_api_key, openai_api_base, openai_chat_model)\n",
    "code_model = call_chatllm_openai(openai_api_key, openai_api_base, openai_code_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28add431",
   "metadata": {},
   "source": [
    "### Set up Tavily search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ef582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_api = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7aaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Tavily\n",
    "#tavily_api = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6a85d",
   "metadata": {},
   "source": [
    "### File management and data perception by ghostcoder.filemanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fill manager will automatically set up file system and percept the input data \n",
    "# A initial file system status for a task with task_id should as follow: \n",
    "#─┬─────work_dir \n",
    "# ├─┬───data\n",
    "# │ └───Input_data.whatever\n",
    "# └─┬───task_id_1 // Work dir for every new tasks\n",
    "#   ├─┬─data  \n",
    "#   │ └─Input_data.whatever // A copy from work_dir/data/\n",
    "#   ├───figures // Where output figures will be saved\n",
    "#   └───results // Where processed data will be saved\n",
    "from ghostcoder.config import docker_config, file_config\n",
    "from ghostcoder.graph import create_filemanager_agent\n",
    "\n",
    "#  Set workdir as current dir\n",
    "current_dir = Path.cwd()\n",
    "file_config.WORK_DIR = current_dir\n",
    "\n",
    "\n",
    "# Initial graph\n",
    "file_management = create_filemanager_agent(\n",
    "        chat_model = chat_model, \n",
    "        code_model = code_model,\n",
    "        max_retry = 3,\n",
    "        )\n",
    "\n",
    "fm_input = {\n",
    "    \"task_id\" : \"Test\", # \n",
    "    \"docker_profile_dir\": docker_config.DOCKER_PROFILES_DIR, # use pre-set docker profiles, please read those docker images \n",
    "    \"max_iter\": 10,\n",
    "}\n",
    "\n",
    "fm_state = await file_management.ainvoke(fm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1f54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c47e4b32",
   "metadata": {},
   "source": [
    "### Native env and Docker executor, test along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ghostcoder.graph import create_executor_agent\n",
    "from ghostcoder.docker import get_docker_status\n",
    "from ghostcoder.utils import get_native_env_perception\n",
    "\n",
    "# Set up environment profiles\n",
    "env_profiles = {\n",
    "    \"task_dirs\":{\n",
    "        \"task_dir\": \"Test\",\n",
    "        \"data_dir\": \"data\",\n",
    "        \"figure_dir\": \"figures\",\n",
    "        \"output_dir\": \"results\",\n",
    "    },\n",
    "    \"docker status\": get_docker_status(),\n",
    "    \"native env languages\": get_native_env_perception(),\n",
    "}\n",
    "\n",
    "# Initial agent\n",
    "agent = create_executor_agent(\n",
    "        chat_model = chat_model, \n",
    "        code_model = code_model,\n",
    "        max_retry = 3,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Test executor with bash python and R\n",
    "test_bash_code =\"\"\"\n",
    "ls -al\n",
    "\"\"\"\n",
    "\n",
    "test_python_code = \"\"\"\n",
    "print(\"Hello World\")\n",
    "\"\"\"\n",
    "\n",
    "test_r_code =\"\"\"\n",
    "my_str <- \"Hello World\"\n",
    "print(my_str)\n",
    "\"\"\"\n",
    "\n",
    "for codeblock in [\n",
    "    test_bash_code, \n",
    "    test_python_code, \n",
    "    test_r_code\n",
    "    ]:\n",
    "    print(\"Test code executor with\\n\",codeblock)\n",
    "\n",
    "    exe_states = await agent.ainvoke(\n",
    "        {\n",
    "            \"generated_codeblock\":codeblock,\n",
    "            \"env_profiles\": env_profiles,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Agent detected coding language as: {exe_states['language']}\\nUse docker: {exe_states['use_docker']}\")\n",
    "    if exe_states['use_docker']:\n",
    "        print(f\"With docker image{exe_states['docker_image']}\")\n",
    "    print(f\"Code execute output:\\n{exe_states['execution_results']}\\n--------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc5881",
   "metadata": {},
   "source": [
    "### Web crawler, test along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037290a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: No content loaded.\n",
      "Attempt 2: No content loaded.\n",
      "Attempt 3: No content loaded.\n",
      "Attempt 1: No content loaded.\n",
      "Attempt 2: No content loaded.\n",
      "Attempt 3: No content loaded.\n"
     ]
    }
   ],
   "source": [
    "from ghostcoder.graph import create_crawler_agent\n",
    "\n",
    "\n",
    "# Initial agent\n",
    "agent = create_crawler_agent(\n",
    "        chat_model = chat_model, \n",
    "        code_model = code_model,\n",
    "        max_retry = 3,\n",
    "    )\n",
    "\n",
    "# Input \n",
    "query_context = \"I'm looking for R codes for analysis single cell trajectory using monocl3\"\n",
    "\n",
    "# Invoke\n",
    "crawl_state = agent.invoke(\n",
    "    {\n",
    "        \"query_context\": query_context,\n",
    "    }\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"--------\\nGenerated query for given context:\")\n",
    "for q in crawl_state['query_list']:\n",
    "    print(q)\n",
    "print(f\"Get total {len(crawl_state['useful_results'])} useful web search results\")\n",
    "print(\"Crawled and parsed web information as follow:\")\n",
    "print(crawl_state['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782cc4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_context', 'query_list', 'query_results', 'useful_results', 'crawled_webs', 'summary'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7fccfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tutorial \"Inferring single cell trajectories with Monocle3 (R)\" provides a comprehensive guide to performing trajectory analysis on single-cell RNA sequencing data using Monocle3 in R. It covers the entire process, including transforming AnnData objects, installing and using necessary R packages, performing batch correction, and identifying differential gene expression. Key components include data preprocessing, dimensionality reduction (using UMAP), clustering, trajectory inference, and cell-type assignment. Integration with tools like JupyterLab for interactive analysis is also discussed, alongside exporting data and visualizations. The tutorial aims to offer flexibility beyond the Monocle tools available in Galaxy and includes code snippets for installation and executing steps in R.\n"
     ]
    }
   ],
   "source": [
    "print(crawl_state['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cad5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Web-based information on related issues:  \n",
      "---\n",
      "### Page 0:  \n",
      "Inferring single cell trajectories with Monocle3 (R)\n",
      "Author(s) Julia Jakiela Editor(s) Helena Rasche Pavankumar Videm Wendi Bacon Reviewers\n",
      "Overview\n",
      "Creative Commons License: CC-BY\n",
      "Questions:\n",
      "How do I perform a trajectory analysis using Monocle3 in R?\n",
      "What should I do when Galaxy’s Monocle tools are not enough?\n",
      "How do I assign cell types to the clusters?\n",
      "How do I infer lineage relationships between clusters, without a time series?\n",
      "How do I perform batch correction and differential expression analysis in Monocle?\n",
      "Objectives:\n",
      "Identify which operations are necessary to transform an AnnData object into the files needed for Monocle\n",
      "Describe the Monocle3 functions in R\n",
      "Recognise steps that can be performed in R, but not with current Galaxy tools\n",
      "Repeat the Monocle3 workflow and choose appropriate parameter values\n",
      "Compare the outputs from Scanpy, Monocle in Galaxy and Monocle in R\n",
      "Describe differential expression analysis methods\n",
      "Requirements:\n",
      "Introduction to Galaxy Analyses\n",
      "tutorial Hands-on: Generating a single cell matrix using Alevin\n",
      "tutorial Hands-on: Combining single cell datasets after pre-processing\n",
      "tutorial Hands-on: Filter, plot and explore single-cell RNA-seq data with Scanpy\n",
      "Time estimation: 3 hours\n",
      "Supporting Materials:\n",
      "Datasets\n",
      "Jupyter Notebook RMarkdown Notebook\n",
      "FAQs\n",
      "instances Available on these Galaxies\n",
      "Possibly Working\n",
      "UseGalaxy.eu\n",
      "UseGalaxy.org\n",
      "UseGalaxy.org.au\n",
      "UseGalaxy.fr\n",
      "Published: Apr 12, 2023\n",
      "Last modification: Mar 11, 2025\n",
      "License: Tutorial Content is licensed under Creative Commons Attribution 4.0 International License. The GTN Framework is licensed under MIT\n",
      "purl PURL: https://gxy.io/GTN:T00336\n",
      "version Revision: 13\n",
      "Introduction\n",
      "This tutorial is the next one in the Single-cell RNA-seq: Case Study series. This tutorial focuses on trajectory analysis using monocle3, similar to the Monocle3 in Galaxy tutorial. However, in this tutorial we will use the R programming language that hides behind the user-friendly Galaxy tools. Sometimes you might encounter limitations when working with Galaxy tools, or you might want to make a wee modification that has to be done manually. It is therefore useful to be able to switch to R. If you do not feel confident using R, this tutorial is a good place to start. However, our tutorial is quite straightforward to follow and at the end you will feel like a programmer! On the other hand, if you are not confident with the biological or statistical theory behind trajectory analysis, check out the slide deck. With those resources (including the previous case study tutorials) you are well-equipped to go through this tutorial with ease. Let’s get started!\n",
      "Comment\n",
      "This tutorial is significantly based on the Monocle3 documentation.\n",
      "Optional: Get data into a Galaxy history\n",
      "In the Monocle3 in Galaxy tutorial, we showed that Monocle3 works great with annotated data, but what if your data is not annotated yet? Is it still possible to use Monocle? The answer is yes, Monocle can annotate cells according to their type.\n",
      "First, we need to retrieve the appropriate data. We will continue to work on the case study data from a mouse model of fetal growth restriction Bacon et al. 2018 (see the study in Single Cell Expression Atlas and the project submission). We will use the filtered AnnData object, before normalisation and annotation, generated in the filtering tutorial as step 20: Filtered Object. However for this tutorial, you don’t have to download any files on your computer or even import files to Galaxy! We will show you the whole analysis in R, starting from AnnData object. However, if you’d like to examine the datasets in a history, the instructions are below.\n",
      "Hands On: Optional data upload into Galaxy history\n",
      "You have three options for importing the input data into a Galaxy history.\n",
      "You can import a history from: input history; Import the files from Zenodo; or Import the files from the shared data library (GTN - Material -> single-cell -> Inferring single cell trajectories with Monocle3 (R)):\n",
      "Open the link to the shared history\n",
      "Click on the Import this history button on the top left\n",
      "Enter a title for the new history\n",
      "Click on Copy History\n",
      "https://zenodo.org/record/7455590/files/AnnData_filtered.h5ad\n",
      "As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a shared data library:\n",
      "Go into Libraries (left panel)\n",
      "Navigate to the correct folder as indicated by your instructor.\n",
      "On most Galaxies tutorial data will be provided in a folder named GTN - Material –> Topic Name -> Tutorial Name.\n",
      "Select the desired files\n",
      "Click on Add to History galaxy-dropdown near the top and select as Datasets from the dropdown menu\n",
      "In the pop-up window, choose\n",
      "“Select history”: the history you want to import the data to (or create a new one)\n",
      "Click on Import\n",
      "Copy the link location\n",
      "Click galaxy-upload Upload Data at the top of the tool panel\n",
      "Select galaxy-wf-edit Paste/Fetch Data\n",
      "Paste the link(s) into the text field\n",
      "Press Start\n",
      "Close the window\n",
      "Check that the datatype is h5ad\n",
      "Click on the galaxy-pencil pencil icon for the dataset to edit its attributes\n",
      "In the central panel, click galaxy-chart-select-data Datatypes tab on the top\n",
      "In the galaxy-chart-select-data Assign Datatype, select h5ad from “New type” dropdown\n",
      "Tip: you can start typing the datatype into the field to filter the dropdown menu\n",
      "Click the Save button\n",
      "Rename galaxy-pencil the history to your name of choice.\n",
      "Launching JupyterLab\n",
      "warning Please note: this is only currently available on the usegalaxy.eu site.\n",
      "Warning: Data uploads & JupyterLab\n",
      "There are a few ways of importing and uploading data into JupyterLab. You might find yourself accidentally doing this differently than the tutorial, and that’s ok. There are a few key steps where you will call files from a location - if these don’t work from you, check that the file location is correct and change accordingly!\n",
      "JupyterLab is a bit like RStudio but for other coding languages. What, you’ve never heard of RStudio? Then don’t worry, just follow the instructions! Saying that, for those that are confident with the R programming language and want to run this locally in the RStudio programming environment, we have included some extra details throughout that helped us run the code locally in RStudio. This tutorial is built to run in JupyterLab, however.\n",
      "Hands On: Launch JupyterLab\n",
      "Currently JupyterLab in Galaxy is available on Live.useGalaxy.eu, usegalaxy.org and usegalaxy.eu.\n",
      "Hands On: Run JupyterLab\n",
      "Interactive Jupyter Notebook. Note that on some Galaxies this is called Interactive JupyTool and notebook:\n",
      "Click Run Tool\n",
      "The tool will start running and will stay running permanently\n",
      "This may take a moment, but once the Executed notebook in your history is orange, you are up and running!\n",
      "On the left menu bar you should see the Interactive Tools Icon now. Click on it to open the Active Interactive Tools and locate the JupyterLab instance you started.\n",
      "Click on your JupyterLab instance (JupyTool interactive tool)\n",
      "If JupyterLab is not available on the Galaxy instance:\n",
      "Start Try JupyterLab\n",
      "Welcome to JupyterLab!\n",
      "Warning: Danger: You can lose data!\n",
      "Do NOT delete or close this notebook dataset in your history. YOU WILL LOSE IT!\n",
      "Open the notebook\n",
      "You have two options for how to proceed with this JupyterLab tutorial - you can run the tutorial from a pre-populated notebook, or you can copy and paste the code for each step into a fresh notebook and run it. The initial instructions for both options are below.\n",
      "Hands On: Option 1: Open the notebook directly in JupyterLab\n",
      "Open a Terminal in JupyterLab with File -> New -> Terminal\n",
      "Screenshot of the Launcher tab with an arrow indicating where to find Terminal.\n",
      "Run\n",
      "wget https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_monocle3-rstudio/single-cell-scrna-case_monocle3-rstudio.ipynb\n",
      "Select the notebook that appears in the list of files on the left.\n",
      "Remember that you can also download this notebook Jupyter Notebook from the galaxy_instance Supporting Materials in the Overview box at the beginning of this tutorial.\n",
      "Hands On: Option 2: Creating a notebook\n",
      "Select the R icon under Notebook\n",
      "R icon\n",
      "Save your file (File: Save, or click the galaxy-save Save icon at the top left)\n",
      "If you right click on the file in the folder window at the left, you can rename your file whateveryoulike.ipynb\n",
      "Warning: You should Save frequently!\n",
      "This is both for good practice and to protect you in case you accidentally close the browser. Your environment will still run, so it will contain the last saved notebook you have. You might eventually stop your environment after this tutorial, but ONLY once you have saved and exported your notebook (more on that at the end!) Note that you can have multiple notebooks going at the same time within this JupyterLab, so if you do, you will need to save and export each individual notebook. You can also download them at any time.\n",
      "Let’s crack on!\n",
      "Installation\n",
      "If you followed the tip tip above, you should already have your JupyterLab instance open. Before we start working on the tutorial notebook, we need to install required packages.\n",
      "Hands On: Installing the packages\n",
      "Navigate back to the Terminal\n",
      "In the Terminal tab open, write the following, one line at a time:\n",
      "conda install -y -c conda-forge -c bioconda r-monocle3\n",
      "conda install -y -c conda-forge -c bioconda anndata\n",
      "conda install -y -c conda-forge r-viridislite\n",
      "conda install -y -c conda-forge bioconductor-biomart\n",
      "If you are asked at any point Proceed ([y]/n)?, type y - surely we want to proceed!\n",
      "Installation will take a while, so in the meantime, when it’s running, you can open the notebook and follow the rest of this tutorial there!\n",
      "Monocle 3 runs in the R statistical computing environment. You will need R version 4.1.0 or higher, Bioconductor version 3.14, and monocle3 1.2.7 or higher to have access to the latest features. Here is the original code that you should run to install BiocManager and monocle3.\n",
      "# Install Bioconductor and some of its dependencies\n",
      "if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
      "install.packages(\"BiocManager\")\n",
      "BiocManager::install(version = \"3.14\")\n",
      "BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats',\n",
      "                       'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment',\n",
      "                       'SummarizedExperiment', 'batchelor', 'HDF5Array',\n",
      "                       'terra', 'ggrastr'))\n",
      "\n",
      "# Install monocle3 through the cole-trapnell-lab GitHub:\n",
      "install.packages(\"devtools\")\n",
      "devtools::install_github('cole-trapnell-lab/monocle3')\n",
      "library(monocle3)\n",
      "Warning: Installation errors\n",
      "It may happen that you will encounter some problems with installation of monocle3 when using RStudio Galaxy instance or RStudio Cloud. It might be due to using older versions of R or required packages or lack of required dependencies. If it happens, you would need to carefully read the error messages and follow the suggestions. If you are facing any difficulties with installation process, it is recommended that you consult additional online resources. It is more likely that RStudio Cloud or Galaxy tool would fail rather than local RStudio. To make your analysis stress-free, you can follow the Jupyter Notebook instead, which should not give you installation issues.\n",
      "Warning: Notebook-based tutorials can give different outputs\n",
      "The nature of coding pulls the most recent tools to perform tasks. This can - and often does - change the outputs of an analysis. Be prepared, as you are unlikely to get outputs identical to a tutorial if you are running it in a programming environment like a Jupyter Notebook or R-Studio. That’s ok! The outputs should still be pretty close.\n",
      "Setting up the environment and file upload\n",
      "Once the installation is done, we should load the needed packages into our notebook. Navigate back to your notebook. If you are using our prepopulated notebook, you can follow the tutorial from there. Otherwise, input the following into your fresh notebook.\n",
      "install.packages(\"Rcpp\")    # needed for reduce_dimension to avoid AnnoyAngular error; library(Rcpp) might work as well depending on the version\n",
      "library(monocle3)\n",
      "library(biomaRt)\n",
      "library(magrittr)     # needed for %>%\n",
      "library(viridisLite)\n",
      "install.packages(\"anndata\")\n",
      "Alright, the libraries are here - so now let’s get our AnnData file.\n",
      "Upload, view and modify the files\n",
      "We can download the file into Jupyter notebook also using a Zendo link. The first argument is the download link and the second one is the name under which the downloaded file is saved:\n",
      "download.file('https://zenodo.org/record/7455590/files/AnnData_filtered.h5ad', 'AnnData.h5ad')\n",
      "Then we have to read in this h5ad file:\n",
      "ann <- anndata::read_h5ad('AnnData.h5ad')\n",
      "Now we store all the information we need in AnnData object. However, Monocle uses cell_data_set class to hold expression data, which requires three input files: expression_matrix, cell_metadata and gene_metadata. Therefore, we have to somehow ‘transfer’ that information from our AnnData object to Monocle’s cell_data_set (cds). AnnData stores a data matrix X together with annotations of observations obs and variables var, so we can extract those parameters and use them for further analysis.\n",
      "expression_matrix <- ann$X\n",
      "cell_metadata <- ann$obs\n",
      "gene_metadata <- ann$var\n",
      "If you already have files containing the expression matrix, genes and cells metadata, you can upload them to JupyLab and generate a cds file from them instead. For example, if you first downloaded the files from Galaxy, your files will have the .tabular extension. In this case, we will use the read.delim() function to read them in. In this function, the first argument is the file path - in our case, the files are in the same folder as the notebook, so the file path is the same as the file name. You can always check that by right-clicking on the file and choosing Copy path. The second argument, row.names=1 takes the column number of the data file from which to take the row names.\n",
      "# read in the files\n",
      "cell_metadata <- read.delim('cells.tabular', row.names=1)\n",
      "gene_metadata <- read.delim('genes.tabular', row.names=1)\n",
      "expression_matrix <- read.delim('expression.tabular', row.names=1)\n",
      "Question\n",
      "Why should we set row.names=1?\n",
      "This allows us to ensure that the expression value matrix has the same number of columns as the cell_metadata has rows and the same number of rows as the gene_metadata has rows. Importantly, the row names of the cell_metadata object should match the column names of the expression matrix and the row names of the gene_metadata object should match the row names of the expression matrix.\n",
      "If you are working in RStudio Cloud, you have to click on ‘Upload’ button in the right bottom window toolbar and choose already downloaded data files to upload. You should now see all three files in this window. You might want to rename the files to make their names shorter.\n",
      "If you are using RStudio Galaxy tool, you can get data directly from your history by running:\n",
      "# get the files from Galaxy history\n",
      "file.copy(gx_get(2), \"cell_metadata\")\n",
      "file.copy(gx_get(3), \"gene_metadata\")\n",
      "file.copy(gx_get(4), \"expression_matrix\")\n",
      "The number in the brackets corresponds to the dataset number in your history, so make sure you put the right number for the corresponding file. We can specify the name of the fetched files in the quotation marks, as shown above. All three files should appear in the Files tab window.\n",
      "If you are using RStudio locally, then you don’t have to bother about uploading the files – just run file.choose() and choose the corresponding file which you want to get path to:\n",
      "# get the file path\n",
      "cells_path <- file.choose()\n",
      "genes_path <- file.choose()\n",
      "expression_path <- file.choose()\n",
      "You should now see the new variables in the Environment tab window.\n",
      "According to the Monocle3 documentation, the expression_matrix should have genes as rows and cells as columns. Let’s check if that’s the case here.\n",
      "head(expression_matrix,c(5, 5))           # preview the content of the file by calling its the first 5 rows by 5 columns\n",
      "We can see that in our matrix rows are cells and genes are columns, so we have to transpose the matrix simply using function t(). But before doing so, we will change its type from dataframe to matrix - this is Monocle’s requirement to generate cell_data_set afterwards.\n",
      "expression_matrix <- as.matrix(expression_matrix)   # change the type to matrix\n",
      "expression_matrix <- t(expression_matrix)           # transpose the matrix\n",
      "Another condition we have to satisfy is that one of the columns of the gene_metadata should be named “gene_short_name”, which represents the gene symbol for each gene. Some functions won’t work without that. Do we have such a column? Let’s check.\n",
      "head(gene_metadata)             # preview the top ten rows of the file to check the name of the column containing gene symbols\n",
      "The second column indeed contains gene symbols, but is called “Symbol” instead of “gene_short_name”. That can be easily changed by a simple assignment, as long as we know the number of the column that we want to modify. In our case the gene symbols are stored in column 2. We can access the column names by colnames().\n",
      "colnames(gene_metadata)[2] <- 'gene_short_name'     # change the column name\n",
      "colnames(gene_metadata)                             # see the changes\n",
      "Generating CDS object\n",
      "Now let’s store our files in one object – the cell_data_set. This is the main class used by Monocle to hold single cell expression data. The class is derived from the Bioconductor SingleCellExperiment class. Similar to Python’s AnnData, the cell_data_set stores a data matrix together with annotations of observations and variables. There are three ways of creating CDS object in monocle:\n",
      "Using new_cell_data_set() function with three data frames as arguments (not their paths!): expression matrix (can also be a sparseMatrix), cell metadata and gene metadata\n",
      "Using load_cellranger_data() function providing the path to the folder containing 10X Genomics Cell Ranger output files. This function takes an argument umi_cutoff that determines how many reads a cell must have to be included\n",
      "Using load_mm_data() function providing the paths to matrix file and two metadata files (features and cell information).\n",
      "In this tutorial we will use the first option:\n",
      "# create CDS object\n",
      "cds <- new_cell_data_set(expression_matrix, cell_metadata, gene_metadata)\n",
      "We are now ready to process our data!\n",
      "Since Monocle’s CDS object is analogous to Python’s AnnData, why don’t we use some kind of conversion between those two formats? There is indeed a package called sceasy that helps easy conversion of different single-cell data formats to each other. However, when we tested this conversion on our dataset and then used Monocle to plot the expression of genes, the plots were not correct – the expression was shown to be identical throughout the sample. For comparison, Seurat did well when plotting gene expression of the same converted object! Although conversion functions are very handy, you have to be aware that their output might be interpreted differently by certain packages. Therefore, to make sure that the analysis is reliable, we decided to generate CDS object directly using Monocle’s function.\n",
      "Additional step: adding genes symbols based on their IDs\n",
      "Warning: Additional step\n",
      "This step is not necessary for the dataset we are working on but some users might find it helpful when analysing their own data.\n",
      "If you remember the very first tutorial in the case study, we started with gene IDs and added gene symbols based on the Ensembl GTF file. But what if we didn’t have the genes symbols in our CDS object and wanted to add them now? Of course - it’s possible! We will also base this annotation on Ensembl - the genome database – with the use of the library BioMart. We will use the same archive as in the Alevin tutorial (Genome assembly GRCm38) to get the gene names. Please note that the updated version (GRCm39) is available, but some of the gene IDs are not in that EnsEMBL database. The code below is written in a way that it will work for the updated dataset too, but will produce ‘NA’ where the corresponding gene name couldn’t be found.\n",
      "cds_extra <- cds                                        # assign our CDS to a new object for the demonstration purpose\n",
      "head(rownames(fData(cds_extra)))                        # preview of the gene IDs as rownames\n",
      "# get relevant gene names\n",
      "library(\"biomaRt\")                                      # load the BioMart library\n",
      "ensembl.ids <- rownames(fData(cds_extra))               # fData() allows to access cds rowData table\n",
      "mart <- useEnsembl(biomart = \"ENSEMBL_MART_ENSEMBL\")    # connect to a specified BioMart database and dataset hosted by Ensembl\n",
      "ensembl_m = useMart(\"ensembl\", dataset=\"mmusculus_gene_ensembl\", host='https://nov2020.archive.ensembl.org') \t\n",
      "\n",
      "# The line above connects to a specified BioMart database and dataset within this database.\n",
      "# In our case we choose the mus musculus database and to get the desired Genome assembly GRCm38,\n",
      "# we specify the host with this archive. If you want to use the most recent version of the dataset, just run:\n",
      "# ensembl_m = useMart(\"ensembl\", dataset=\"mmusculus_gene_ensembl\")\n",
      "genes <- getBM(attributes=c('ensembl_gene_id','external_gene_name'),\n",
      "               filters = 'ensembl_gene_id',\n",
      "               values = ensembl.ids,\n",
      "               mart = ensembl_m)\n",
      "\n",
      "# The line above retrieves the specified attributes from the connected BioMart database;\n",
      "# 'ensembl_gene_id' are genes IDs,\n",
      "# 'external_gene_name' are the genes symbols that we want to get for our values stored in ‘ensembl.ids’.\n",
      "# see the resulting data\n",
      "head(genes)                          \n",
      "# replace IDs for gene names\n",
      "gene_names <- ensembl.ids\t \n",
      "count = 1 \t \n",
      "for (geneID in gene_names)\n",
      "{\n",
      " index <- which(genes==geneID)    # finds an index of geneID in the genes object created by getBM()\n",
      " if (length(index)==0)            # condition in case if there is no corresponding gene name in the chosen dataset\n",
      "  {\n",
      "    gene_names[count] <- 'NA'\n",
      "  }\n",
      "  else\n",
      "  {\n",
      "    gene_names[count] <- genes$external_gene_name[index] \t# replaces gene ID by the corresponding gene name based on the found geneID’s index\n",
      "  }\n",
      " count = count + 1                # increased count so that every element in gene_names is replaced\n",
      "}\n",
      "# store the gene names in our CDS object in a new column gene_short_name_extra\n",
      "fData(cds_extra)$gene_short_name_extra <- gene_names\n",
      "# see the changes\n",
      "fData(cds_extra)                    \n",
      "If you are working on your own data and it’s not mouse data, you can check available datasets for other species and just use relevant dataset in useMart() function.\n",
      "listDatasets(mart)                # available datasets\n",
      "Warning: Ensembl connection problems\n",
      "Sometimes you may encounter some connection issues with Ensembl. To improve performance Ensembl provides several mirrors of their site distributed around the globe. When you use the default settings for useEnsembl() your queries will be directed to your closest mirror geographically. In theory this should give you the best performance, however this is not always the case in practice. For example, if the nearest mirror is experiencing many queries from other users it may perform poorly for you. In such cases, the other mirrors should be chosen automatically.\n",
      "Monocle workflow\n",
      "Do you remember the Monocle workflow introduced in the previous tutorial? Here is a recap:\n",
      "Pre-processing\n",
      "Let’s start with normalisation and pre-processing that can be performed using the function preprocess_cds(). The argument num_dim is the number of principal components that will be used. You can check that you’re using enough PCs to capture most of the variation in gene expression across all the cells in the data set. Note that “PCA” is the default method of pre-processing in Monocle3, so although we can specify this in our function, we don’t have to.\n",
      "Note that this step can take awhile - around 5 or so minutes due to the high number of PCs calculated. Feel free to use a num_dim value around 100 to accelerate preprocessing and compare the results.\n",
      "# PCA pre-processing with 210 principal components\n",
      "cds_preprocessing <- preprocess_cds(cds, method = \"PCA\", num_dim = 210)\n",
      " # plot the variation in gene expression vs PCA components\n",
      "plot_pc_variance_explained(cds_preprocessing)                              \t\t                            \n",
      "The plot shows that actually using more than ~100 PCs captures only a small amount of additional variation. However, if we look at how the cells are plotted on 2D graph when using different values of PCs, it is easier to visualise how the num_dim actually affects the output. We will use the value of 210, which, compared to the results from the previous tutorial, makes the most sense for our dataset.\n",
      "Batch correction and Dimensionality reduction\n",
      "Our dataset actually comprises data from 7 samples, so there is a risk that batch effects will impact analysis. Batch effects are systematic differences in the transcriptome of cells measured in different experimental batches. However, we can use Monocle to deal with that! First, let’s check how our dataset looks in terms of batch effects. We can do that by colouring the cells by batch. This information is stored in our CDS object from the cell_metadata file. Before asking Monocle to plot anything, let’s check the exact column name of the batch information column.\n",
      "colnames(colData(cds_preprocessing)) \t          # check column names\n",
      "In our case it’s indeed ‘batch’, but your data might have another name (eg. “plate”, etc.), so make sure you put the correct argument value.\n",
      "In order to plot our cells on a 2D graph, we need to reduce the numbers of dimensions. The previous tutorial introduced the methods of dimensionality reduction in Monocle. You can replicate what we did in Galaxy to compare the output of dimensionality reduction using different methods, simply by changing the reduction_method argument. Options currently supported by Monocle are “UMAP”, “tSNE”, “PCA”, “LSI”, and “Aligned”. However, as for now, let’s just recall that UMAP gave the best results, so we will use UMAP here as well.\n",
      "# reduce dimension first\n",
      "cds_preprocessing_UMAP <- reduce_dimension(cds_preprocessing, reduction_method = \"UMAP\", preprocess_method = \"PCA\")\n",
      "# then produce a plot to check for batch effects\n",
      "plot_cells(cds_preprocessing_UMAP, color_cells_by=\"batch\", label_cell_groups=FALSE)\n",
      "We can see that upper and lower right branches mostly consist of N705 and N706, so indeed batch correction might be helpful. Let’s run this.\n",
      "# perform batch correction\n",
      "cds_batch <- align_cds(cds_preprocessing_UMAP, preprocess_method = \"PCA\", alignment_group = \"batch\")\n",
      "To see the changes, we have to run UMAP again, but this time on the aligned dataset. We will specify that preprocess_method as “Aligned” and not “PCA”. Monocle would use the “Aligned” argument automatically if no preprocess_method was specified.\n",
      "# dimensionality reduction after alignment\n",
      "cds_red_dim <- reduce_dimension(cds_batch, preprocess_method = \"Aligned\", reduction_method = \"UMAP\")  \n",
      "# see the batch correction effect on a plot\n",
      "plot_cells(cds_red_dim, color_cells_by=\"batch\", label_cell_groups=FALSE)\t\t\n",
      "Question\n",
      "Does your plot look the same as the one in the Figure?\n",
      "Your plot might be slightly different to the one shown in the Figure but this is fine, as long as you see analogical patterns. Some libraries that you’re using might have been updated, giving non-identical output. However, the principle behind the analysis is still the same, so you can peacefully follow the tutorial. Just keep your eyes open and… THINK!\n",
      "Do you see this? It’s amazing! Batch correction did a great job here! Now the dataset is nicely aligned, and the cells from all the samples are evenly spread throughout the whole dataset. It is worth mentioning that removing batch effects was done using mutual nearest neighbor alignment, a technique introduced by John Marioni’s lab (Haghverdi et al. 2018) and supported by Aaron Lun’s package batchelor. Also, due to the machine learning elements of the code of this technique - as well as the fact that packages are updated regularly - your plots may not look identical to the ones pictured here. Nevertheless, the interpretation should be the same - the batch corrected plot should show better batch distribution than the uncorrected one.\n",
      "Now we can move to the next step and perform dimensionality reduction.\n",
      "When creating graphs, we sometimes use labels and sometimes just a legend. You can choose whichever you think makes the data clear and readable.\n",
      "If you want to use a legend, then specify an argument label_cell_groups=FALSE in the function plot_cells().\n",
      "The labels are set automatically, but if you want to change their size (default labels are tiny), use the argument group_label_size.\n",
      "Clustering: clusters\n",
      "We want to get some information about cell types, don’t we? In order to do so, we have to cluster our cells first. Monocle uses a technique called “community detection” (Traag et al. 2019) to group cells. This approach was introduced by Levine et al. 2015 as part of the phenoGraph algorithm. Monocle also divides the cells into larger, more well separated groups called partitions, using a statistical test from Wolf et al. 2019, introduced as part of their PAGA algorithm.\n",
      "Clusters are particularly useful while trying to assign cells to a certain type, because they are based on the similarity in gene expression. The relationships between different clusters are analysed to identify possible trajectories.\n",
      "Partitions, meanwhile, are larger groups of cells that usually contain several clusters. Trajectory inference is performed only within one partition, so it is essential that all the cells that we want to analyse in pseudotime belong to the same partition.\n",
      "Therefore, let’s perform clustering and visualise the resulting clusters.\n",
      "# clustering\n",
      "cds_auto_cluster <- cluster_cells(cds_red_dim, reduction_method = \"UMAP\")\n",
      "# see the clusters\n",
      "plot_cells(cds_auto_cluster, reduction_method = \"UMAP\", color_cells_by = 'cluster', group_label_size = 5)     \n",
      "When using standard igraph louvain clustering, the value of resolution parameter is by default set to NULL, which means that it is determined automatically. Although the resulting clusters are OK, it would be nice to get some more granularity to identify cell types more specifically. The higher the resolution value, the more clusters we get. We will set the resolution value to 0.0002, but you are very welcome to try different values to see the changes.\n",
      "# clustering with changed resolution value\n",
      "cds_clustered <- cluster_cells(cds_red_dim, reduction_method = \"UMAP\", resolution = 0.0002)\n",
      "# see the new clusters\n",
      "plot_cells(cds_clustered, reduction_method = \"UMAP\", color_cells_by = 'cluster', group_label_size = 5) \t\n",
      "Clustering: partitions\n",
      "OK, what about partitions? They were also created during the clustering step and it’s important to check them before calculating the trajectory because it is performed only within one partition. It is essential that all the cells that we want to analyse in pseudotime belong to the same partition.\n",
      "# see the partitions\n",
      "plot_cells(cds_clustered, reduction_method = \"UMAP\", color_cells_by = 'partition', label_cell_groups=FALSE)\n",
      "While your plot might be slightly different due to package updates, we can see that there are 3 partitions identified in cds_clustered object. Ideally, we would like to combine partitions 1 and 2 to draw a trajectory through all those cells (we can ignore cells in outlier partition). Sometimes using the default values might result in multiple partitions while you only need one. Then you would have to change the q-value cutoff in partition_qval. The default is 0.05 and by increasing this value you can increase the span of partitions, meaning that you would get fewer partitions. When trying different values of q-value, you also have to check if the clusters didn’t change. It’s all about finding a balance between the value of resolution and partition_qval so that both clusters and partitions are satisfactory enough for downstream analysis. Let’s try that on our dataset.\n",
      "# changing the partition q-value\n",
      "cds_clustered <- cluster_cells(cds_red_dim, reduction_method = \"UMAP\", resolution = 0.0002, partition_qval = 1)\n",
      "# see the partitions with the changed q-value\n",
      "plot_cells(cds_clustered, reduction_method = \"UMAP\", color_cells_by = 'partition', label_cell_groups=FALSE)\n",
      "# check if clusters didn't change\n",
      "plot_cells(cds_clustered, reduction_method = \"UMAP\", color_cells_by = 'cluster', label_cell_groups=FALSE)\n",
      "Question\n",
      "Have the clusters change after changing the partition q-value?\n",
      "It might be the case that after changing partition q-value, you will notice that additional clusters appeared. In that situation, you might either play around the resolution and partition_qval values, go forward with the current clustering (adjusting the parameters accordingly), or check the other method of assigning cells to one partition given below.\n",
      "Now we have cells of interest in one partition, we still have reasonable clusters, so now we can learn the trajectory. However, in some cases even this method might not be enough. Then, there is a last resort… assigning cells to a partition manually.\n",
      "Additional step: assigning cells to one partition\n",
      "Warning: Additional step\n",
      "This step is not necessary for the dataset we are working on but some users might find it helpful when analysing their own data.\n",
      "Let’s assume we have 4 partitions that cannot be extended to one big partition using partition_qval (it might sound unreasonable but it does happen!) and we are desperate to have all the cells in one partition to draw a trajectory through all of them. We can simulate the initial dataset by setting partition_qval value to 0.0001.\n",
      "# simulate the dataset\n",
      "cds_partitions_extra <- cluster_cells(cds_red_dim, reduction_method = \"UMAP\", partition_qval = 0.0001)\t\t\n",
      "# see the simulated partitions\n",
      "plot_cells(cds_partitions_extra, reduction_method = \"UMAP\", color_cells_by = 'partition', label_cell_groups=FALSE)\n",
      "# store ‘1’ the number of times equal to the number of cells\n",
      "big_partition <- c(rep(1,length(cds_partitions_extra@colData@rownames))) \t\n",
      "# take the barcode of each cell and assign ‘1’ to each of them (now the ‘ones’ are named)\n",
      "names(big_partition) <- cds_partitions_extra@colData@rownames \t\n",
      "# convert from numeric to factor\n",
      "big_partition <- as.factor(big_partition)\n",
      "# assign the created barcodes-partition list to the location where information about partitions is stored in CDS object\n",
      "cds_partitions_extra@clusters$UMAP$partitions <- big_partition \t\n",
      "# see the new partition assignment\n",
      "plot_cells(cds_partitions_extra, reduction_method = \"UMAP\", color_cells_by = 'partition', label_cell_groups=FALSE)\n",
      "As you saw above, we used @ and $ operators to navigate in CDS object. What is the difference between them? $ operator is used to access one variable/column, while @ extracts the contents of a slot in an object with a formal (S4) class structure. If you use View() function on our CDS object, you will see the elements of type “S4” that you can access using @ operator. The suggestion list of elements that can be accessed should also pop up as you type the name of the object followed by the corresponding operator.\n",
      "Assigning cell types\n",
      "There are two main approaches to assigning cell types to clusters that we’ve just identified – supervised and unsupervised, both based on gene expression in each cluster.\n",
      "Supervised approach\n",
      "The supervised approach relies on prior knowledge of which cell types to expect. We can simply check the expression of marker genes specific to the expected cell types. Let’s then check the markers mentioned in the original paper Bacon et al. 2018.\n",
      "Marker Cell type Il2ra Double negative (early T-cell) Cd8b1, Cd8a, Cd4 Double positive (middle T-cell) Cd8b1, Cd8a, Cd4 - high Double positive (late middle T-cell) Itm2a Mature T-cell Aif1 Macrophages Hba-a1 RBC\n",
      "To plot the expression of all those genes across our dataset in one go, we will pass a vector with the names of those markers into a parameter genes in the plot_cells function.\n",
      "# expression of marker genes across the sample\n",
      "plot_cells(cds_clustered, genes=c('Il2ra','Cd8b1','Cd8a','Cd4','Itm2a','Aif1','Hba-a1'), reduction_method = \"UMAP\")\n",
      "Question: Genes, cell types and clusters\n",
      "Based on the gene expression graph that we just generated, the table above and your knowledge from the previous tutorials, how would you assign the clusters?\n",
      "Keep in mind that these results refer to our numbered clusters, while yours might be slightly different.\n",
      "Il2ra (DN): mostly expressed in cluster 4\n",
      "Cd8b1, Cd8a (DP middle): expressed in clusters 1, 6, and highly in cluster 2\n",
      "Cd4 (DP late): average expression in clusters 1, 6, 2 and high expression in cluster 5\n",
      "Itm2a (T-mat): expressed in cluster 3\n",
      "Aif1 (macrophages): barely anything here, minimal expression spread across the sample with some more cells in cluster 4 and 3 – not enough to form a distinct cluster though). In theory, we shouldn’t have any macrophages in our sample. If you remember from the previous tutorials, we actually filtered out macrophages from the sample during the processing step, because we worked on annotated data. When analysing unannotated data, we could only assign macrophages and then filter them out, provided that Monocle clusters them into a separate group. As you can see, it’s not the case here, so we will just carry on with the analysis, interpreting this as a contamination.\n",
      "Hba-a1 (RBC): appears throughout the entire sample in low numbers suggesting some background contamination of red blood cell debris in the cell samples during library generation, but also shows higher expression in a distinct tiny bit of cluster 3, at the border between clusters 1 and 5. However, it’s too small to be clustered into a separate group and filtered out in this case. If you remember, this gene was found to be expressed in the previous Scanpy tutorial also in low numbers across the sample, and in the other Monocle tutorial (using Galaxy tools and annotated data) algorithms allowed us to gather the cells expressing that gene into a distinct group. Our result now sits somewhere in between.\n",
      "Having identified which cluster corresponds to a specific cell type, we can finally run some code to add the annotation to our CDS object. First, we will create a new column called cell_type in colData() - this is where the information about the cells is stored (eg. batch, genotype, sex, etc) - and initialize it with the values of clusters. Then, we will get the dplyr package which will be used for cluster annotation.\n",
      "# just to keep the objects tidy and not overwrite them so that you can come back to any point of the analysis\n",
      "cds_annotated <- cds_clustered \t\n",
      "# create a new column ‘cell_type’ and initialise it with clusters values\n",
      "colData(cds_annotated)$cell_type <- as.character(clusters(cds_annotated)) \t\n",
      "# annotate clusters\n",
      "colData(cds_annotated)$cell_type <- dplyr::recode(colData(cds_annotated)$cell_type,\n",
      "                                                       '1'='DP-M1',   # double positive – middle T-cell (1st cluster)\n",
      "                                                       '2'='DP-M2',\t  # double positive – middle T-cell (2nd cluster)\n",
      "                                                       '3'='T-mat',\t  # mature T-cell\n",
      "                                                       '4'='DN',\t\t  # double negative – early T-cell\n",
      "                                                       '5'='DP-L',\t  # double positive – late middle T-cell\n",
      "                                                       '6'='DP-M3',\t  # double positive – middle T-cell (3rd cluster)\n",
      "                                                       '7'='Unknown') # no info for now, so call it ‘Unknown’\n",
      "                                                       '8'='Unknown') # no info for now, so call it ‘Unknown’\n",
      "# check the annotation\n",
      "plot_cells(cds_annotated, color_cells_by=\"cell_type\", label_cell_groups=FALSE)    \n",
      "Unsupervised approach\n",
      "But what if we don’t have any reference that we can use to assign our clusters? In that case, we will turn to the mentioned unsupervised approach - we will check what are the specifically expressed genes for each cluster. Then we can identify the cell types by looking up what cell types contain those genes. That’s a more tedious process, but sometimes can lead to exciting and unexpected results. We will use Monocle’s function top_markers() and store the information about specifically expressed genes for each cluster in the data frame marker_test.\n",
      "# find top marker genes in each cluster\n",
      "marker_test <- top_markers(cds_clustered, group_cells_by=\"cluster\", reduction_method = \"UMAP\", reference_cells=1000, cores=8)\n",
      "You can group the cells by any categorical variable in colData(cds_clustered). The parameter reference_cells is used to accelerate the marker significance test at some cost in sensitivity. It works by randomly selecting a specified number of cells and performing the marker significance test against a chosen set of cells. If your dataset is not massively big, you might skip this parameter as it wouldn’t help much.\n",
      "Question\n",
      "What are the variables stored in marker_test data frame and what do they mean?\n",
      "Those are 10 variables that you can check easily either using colnames(marker_test) or View(marker_test) which will also display all the corresponding values.\n",
      "gene_id - Ensembl gene ID\n",
      "gene_short_name - short name of the gene corresponding to its ID\n",
      "cell_group - a group to which the cell belongs, specified in the group_cells_by argument\n",
      "marker_score - numeric vector of marker scores as the fraction expressing scaled by the specificity. The value ranges from 0 to 1\n",
      "mean_expression - numeric vector of mean normalized expression of the gene in the cell group\n",
      "fraction_expressing - numeric vector of fraction of cells expressing the gene within the cell group\n",
      "specificity - numeric vector of a measure of how specific the gene’s expression is to the cell group based on the Jensen-Shannon divergence. The value ranges from 0 to 1.\n",
      "pseudo_R2 - numeric vector of pseudo R-squared values, a measure of how well the gene expression model fits the categorical data relative to the null model. The value ranges from 0 to 1.\n",
      "marker_test_p_value - numeric vector of likelihood ratio p-values; p-value is an area in the tail of a distribution that tells you the odds of a result happening by chance\n",
      "marker_test_q_value - numeric vector of likelihood ratio q-values; q-value is a p-value that has been adjusted for the False Discovery Rate (FDR)\n",
      "We can now use data in marker_test to rank the cells based on one of the specificity metrics and take the top gene(s) for each cluster. We will filter the expressing cells that constitute more than 10% of the cell group and we will take one gene in each cluster with the highest pseudo_R2 value (you can of course modify this value and choose more genes to be selected).\n",
      "# filter the ‘marker_test’ data frame\n",
      "top_specific_markers <- marker_test %>%\n",
      "                            dplyr::filter(fraction_expressing >= 0.10) %>%       # set the fraction of expressing cells\n",
      "                            dplyr::group_by(cell_group) %>%                      # set a group to which the cells belong\n",
      "                            dplyr::top_n(1, pseudo_R2)                           # set the number of top genes and the variable from 'marker_test' to rank by\n",
      "# store the names of the marker genes\n",
      "# you can also use IDs, the conversion to gene names should happen automatically when plotting\n",
      "top_marker_names <- unique(top_specific_markers %>% dplyr::pull(gene_short_name))\n",
      "Now we have all elements to plot the expression and fraction of cells that express found markers in each group.\n",
      "plot_genes_by_group(cds_clustered,                    # our CDS object\n",
      "                    top_marker_names,                 # a list of gene names to show in the plot\n",
      "                    group_cells_by=\"cluster\",         # how to group cells when labeling\n",
      "                    ordering_type=\"maximal_on_diag\")  # how to order the genes / groups on the dot plot\n",
      "If you notice that on your dot plot any cluster has more genes than you specified (here we set one gene per cluster top_n(1, pseudo_R2)), go back to the code and create a new cell, right after assigning top_specific_markers object and just before top_marker_names. In the new cell, paste and execute the following:\n",
      "top_specific_markers <- top_specific_markers %>%\n",
      "                           dplyr::distinct(pseudo_R2, .keep_all=TRUE)     # select only one row if there are multiple rows with the same value in 'pseudo_R2' column\n",
      "Then you can execute top_marker_names again and see how the plot looks like now. Better, right? This problem may arise when several genes in one cluster have the same values of specific variable from ‘marker_test’ (in our case we chose pseudo_R2). It might likely happen in small and quite unsignificant clusters.\n",
      "Look at this – we have identified some more marker genes specific to each cluster! However, sometimes it happens that the found genes are not as specific as one would expect, and they appear across the whole sample. Therefore, it is a good idea to plot all those marker genes and check how they appear in the bigger picture.\n",
      "# plot the identified genes to check their expression\n",
      "plot_cells(cds_clustered, genes=c('Pcdhgc4','Pcdhga5','Gm5559','Gm10359','Ccr9','Cd8b1','Plac8', 'Il2ra', 'Cd52', 'Tmsb10', 'Mki67', 'Hmgb2', 'Pclaf', 'Npm1'), reduction_method = \"UMAP\")\n",
      "Further steps from now would include reviewing literature and checking what cell types correspond to the genes expressed in each cluster. Then you can annotate your clusters in the same way as shown above. Once you have your clusters annotated, Monocle can generate a file of marker genes for the identified cell types. This file can be then used with Garnett, a software toolkit for automatically annotating cells. We will not go through this in the current tutorial, but we will generate the file of marker genes.\n",
      "# use ‘top_markers()’ again, now grouping cells by the assigned cell type\n",
      "assigned_marker_test <- top_markers(cds_annotated,\n",
      "                                             group_cells_by=\"cell_type\",\n",
      "                                             reference_cells=1000,\n",
      "                                             cores=8)\n",
      "# filter these markers according to how stringent you want to be\n",
      "garnett_markers <- assigned_marker_test %>%\n",
      "                        dplyr::filter(marker_test_q_value < 0.05 & specificity >= 0.25) %>%\n",
      "                        dplyr::group_by(cell_group) %>%\n",
      "                        dplyr::top_n(5, marker_score)\n",
      "# exclude genes that are good markers for more than one cell type:\n",
      "garnett_markers_filtered <- garnett_markers %>%\n",
      "                        dplyr::group_by(gene_short_name) %>%\n",
      "                        dplyr::filter(dplyr::n() == 1)\n",
      "# generate a file of marker genes\n",
      "generate_garnett_marker_file(garnett_markers_filtered, file=\"./marker_file.txt\")\n",
      "A new file should appear in the ‘Files’ window. If you click on it, you will see the cell types and their corresponding marker genes, satisfying your chosen conditions.\n",
      "Note that you can use the above block of code to generate file with the marker genes for unannotated CDS object to help you identify and check specifically expressed genes – you’d only have to change group_cells_by parameter from “cell_type” to “cluster”.\n",
      "If you are working in RStudio locally, you might want to try a great function choose_cells(), which allows you to make a subset of the CDS object containing only cells of interest to investigate certain clusters more in-depth. It only works in interactive mode, so can be only used locally - when you call it, then a pop-up window will appear where you can choose cells to subset.\n",
      "# create a CDS subset\n",
      "cds_subset <- choose_cells(cds_clustered)\n",
      "Now the chosen cluster is stored as a separate CDS object and you can analyse it independently, using the methods described above.\n",
      "Trajectory inference\n",
      "It’s time to perform trajectory analysis! First, let’s learn the trajectory graph. With an argument use_partition we can specify if we want to learn a disjoint graph (value TRUE - default) in each partition, or to learn a single graph (value FALSE) across all partitions. The thing is, we can visualise the cells in pseudotime only if they belong to one partition. This is why it is important to make sure that all the cells that you want to analyse in pseudotime belong to one partition.\n",
      "In one of the previous sections we’ve already prepared the partitions for trajectory inference - we assigned all the cells of interest into one partition by changing the q-value. As a result, we got two partitions – one containing only the cells that we labeled as ‘Unknown’ and another including all other cells. That is a perfect assignment – we’d like to focus only on maturation of T-cells, so we don’t want to connect those two partitions (therefore we specify use_partition=TRUE to get a disjoint graph) and we will work downstream only on the graph going through annotated T-cells.\n",
      "# learn trajectory graph\n",
      "cds_trajectory <- learn_graph(cds_annotated, use_partition=TRUE)\n",
      "# visualise the learned trajectory\n",
      "plot_cells(cds_trajectory,\n",
      "           color_cells_by = \"cell_type\",\n",
      "           label_cell_groups=FALSE,\n",
      "           label_groups_by_cluster=FALSE,\n",
      "           label_leaves=FALSE,\n",
      "           label_branch_points=FALSE)\n",
      "We have to tell Monocle where to start ordering the cells, ie. when we expect the analysed biological process to begin. Thanks to our biological knowledge, we know that the beginning of the trajectory should be at DN cluster. There are a couple of ways to specify the root cells:\n",
      "Option 1: Root nodes\n",
      "Here, you will use root_pr_nodes argument in order_cells() function.\n",
      "To find the names of the principal points, you have to plot the learned trajectory again, specifying label_principal_points = TRUE\n",
      "# specifying root cells: `root_pr_nodes` argument - check the principal points\n",
      "plot_cells(cds_trajectory,\n",
      "          color_cells_by = \"cell_type\",\n",
      "          label_cell_groups=FALSE,\n",
      "          label_groups_by_cluster=FALSE,\n",
      "          label_leaves=FALSE,\n",
      "          label_branch_points=FALSE,\n",
      "          label_principal_points = TRUE,       # set this to TRUE\n",
      "          graph_label_size=3)\n",
      "You can see now the principal points and their labels in the form Y_number. Pick the principal point in the cluster that you expect to be the beginning of the trajectory and type its name in the root_pr_nodes argument when calling order_cells() function.\n",
      "# specifying root cells: `root_pr_nodes` argument - use the relevant principal point\n",
      "cds_order_1 <- order_cells(cds_trajectory, root_pr_nodes='Y_14')\n",
      "There is also a helper function to identify the root principal points based on the annotated cell types. This function uses pr_graph_cell_proj_closest_vertex which is just a matrix with a single column that stores for each cell, the ID of the principal graph node it’s closest to.\n",
      "# a helper function to identify the root principal points\n",
      "get_correct_root_state <- function(cds, cell_phenotype, root_type){\n",
      "      cell_ids <- which(pData(cds)[, cell_phenotype] == root_type)\n",
      "\n",
      "      closest_vertex <- cds@principal_graph_aux[[\"UMAP\"]]$pr_graph_cell_proj_closest_vertex\n",
      "      closest_vertex <- as.matrix(closest_vertex[colnames(cds), ])\n",
      "      root_pr_nodes <- igraph::V(principal_graph(cds)[[\"UMAP\"]])$name[as.numeric(names\n",
      "                                                                    (which.max(table(closest_vertex[cell_ids,]))))]       \n",
      "      root_pr_nodes\n",
      "      }\n",
      "# call the function to automatically find the node in the principal graph where our DN cells reside\n",
      "DN_node_id = get_correct_root_state(cds_trajectory, cell_phenotype = 'cell_type', \"DN\")\n",
      "\n",
      "DN_node_id      # check the node found\n",
      "# order cells using the helper function output\n",
      "cds_order_1_helper <- order_cells(cds_trajectory, root_pr_nodes = DN_node_id)\n",
      "Option 2: Root cells\n",
      "Here, you will use root_cells argument in order_cells() function.\n",
      "Specify a vector of starting cell IDs. You can provide only one cell as well as all cells of a given type.\n",
      "# find the names of all cells belonging to a certain type, identified as a beginning of a trajectory\n",
      "starting_cell_type <- 'DN'\n",
      "index <- which(cds_trajectory@colData$cell_type == starting_cell_type)\n",
      "DN_cells <- colnames(cds_trajectory)[index]\n",
      "# alternatively, if you work on unannotated data, you can use the number of the cluster that should be used as the beginning of the trajectory and pass it in the ‘root_cells’ argument\n",
      "starting_cluster <- colnames(cds_trajectory[,clusters(cds_trajectory) == 4])\n",
      "# order cells\n",
      "cds_order_2 <- order_cells(cds_trajectory, root_cells = DN_cells)\n",
      "If you are working in RStudio locally, you can use order_cells() function in the interactive mode. The pop-up window should appear and then you can simply select root nodes in the cluster with the cell type identified as the beginning of the trajectory.\n",
      "# specifying root cells: pop up window\n",
      "cds_order_1 <- order_cells(cds_trajectory)\n",
      "Plotting in pseudotime\n",
      "You can use any cds_order object for the downstream analysis. Let’s pick one and assign it to an object with a shorter and more general name.\n",
      "cds_order <- cds_order_1_helper\n",
      "The function order_cells() calculates pseudotime for each cell, so we can now visualise the T-cell maturation process in pseudotime! Additionally, we can access it and store in our CDS object for further analysis.\n",
      "# plot cells in pseudotime\n",
      "plot_cells(cds_order,\n",
      "           color_cells_by = \"pseudotime\",\n",
      "           label_cell_groups=FALSE,\n",
      "           label_leaves=FALSE,\n",
      "           label_branch_points=FALSE)\n",
      "# access pseudotime calculated for each cell and store it alongside cell metadata\n",
      "pseudotime <- pseudotime(cds_order) \t\n",
      "cds_order@colData$pseudotime <- pseudotime \t\t \n",
      "We can now see how our hard work has come together to give a final pseudotime trajectory analysis, which starts at double negative cells, then gently switches to double positives: from middle to late T-cells, and ends up on mature T-cells.\n",
      "Differential expression analysis\n",
      "There are two approaches for more advanced differential analysis in Monocle:\n",
      "Regression analysis: using fit_models(), you can evaluate whether each gene depends on variables such as time, treatments, etc.\n",
      "Graph-autocorrelation analysis: using graph_test(), you can find genes that vary over a trajectory or between clusters.\n",
      "In this section, for some examples, we will work on the subset of our CDS object because the computation time for the whole dataset would take quite some time. Let’s make the subset CDS containing the information about the genes listed in the table in the previous section, so instead having 15395 elements, it will have only 7.\n",
      "# make the subset CDS\n",
      "test_genes=c('Il2ra','Cd8b1','Cd8a','Cd4','Itm2a','Aif1','Hba-a1')\n",
      "cds_subset <- cds_order[rowData(cds_order)$gene_short_name %in% test_genes,]\n",
      "Some more plotting\n",
      "Monocle also provides some easy ways to plot the expression of a small set of genes grouped by the factors you use during differential analysis. For example plot_genes_violin() allows us to create violin plots which are quite common in the field. Therefore let’s visualise how the gene expression changes between the cell types.\n",
      "# produce violin plots\n",
      "plot_genes_violin(cds_subset, group_cells_by=\"cell_type\", ncol=2)\n",
      "When analysing the above violin plots, we will realise that the results are consistent with the detailed analysis in the previous tutorials, as well as findings in current tutorial.\n",
      "Another great function plot_genes_in_pseudotime() takes a small set of genes and shows their dynamics as a function of pseudotime.\n",
      "plot_genes_in_pseudotime(cds_subset, color_cells_by=\"cell_type\", min_expr=0.5)\n",
      "Regression analysis - advanced\n",
      "We will use a function which fits a generalized linear model for each gene in a CDS object. We have to specify the model formula which is any term that exists as a column in colData(). We want to test genes that differ between cell types and batches, so we will use “~cell_type + batch” argument. Then, we extract a table of coefficients from each model. coefficient_table() tests whether each coefficient differs significantly from zero under the Wald test.\n",
      "# fit a generalized linear model for each gene\n",
      "gene_fits <- fit_models(cds_subset, model_formula_str = \"~cell_type + batch\")\n",
      "# extract a table of coefficients\n",
      "fit_coefs <- coefficient_table(gene_fits) \t\n",
      "# preview the content of 'fit_coefs'\n",
      "head(fit_coefs)\n",
      "If you inspect the fit_coefs object, you will notice that the table includes one row for each term of each gene’s model. We generally don’t care about the intercept term, so we can filter it out. In this way, we will be able to control for the chosen factors. To focus on only one variable, you have to check the term column in the fit_coefs and pass this as an argument for filtering. Then, you should also filter the results with q_value < 0.05 to control the false discovery rate.\n",
      "# filter out Intercept term\n",
      "no_intercept_coefs <- fit_coefs %>% dplyr::filter(term != \"(Intercept)\") \t\n",
      "# extract results for DP-M1 cells only\n",
      "DP_M1_coefs <- fit_coefs %>% dplyr::filter(term == \"cell_typeDP-M1\") \t\n",
      "# control the false discovery rate and choose only several variables to store\n",
      "DP_M1_coefs_filtered <- DP_M1_coefs %>% dplyr::filter (q_value < 0.05) %>%\n",
      "  dplyr::select(gene_short_name, term, q_value, estimate)\t\t\n",
      "# view the resulting table\n",
      "DP_M1_coefs_filtered\n",
      "The resulting table shows the genes that differ depending on the chosen term. Maybe this function is not very helpful in the case of our dataset, but may be useful when analysing unannotated data or choosing another term from colData().\n",
      "Graph-autocorrelation analysis - advanced\n",
      "Alongside regression analysis, Monocle also provides another way of finding genes that vary between groups of cells. The function graph_test() uses a statistic from spatial autocorrelation analysis called Moran’s I, which Cao et al. 2018 showed to be effective in finding genes that vary in single-cell RNA-seq datasets. Let’s try to perform this step on our full dataset (be patient!).\n",
      "# run autocorrelation test\n",
      "graph_test_res <- graph_test(cds_order, neighbor_graph=\"knn\", cores=8)\n",
      "The output data frame has a bunch of statistical values that you can use to rank the genes by, for example morans_I column, which ranges from -1 to +1. A value of 0 indicates no effect, while +1 indicates perfect positive autocorrelation and suggests that nearby cells have very similar values of a gene’s expression.\n",
      "We will now try to associate genes with clusters by grouping genes into modules that have similar patterns of expression. You can call find_gene_modules(), which essentially runs UMAP on the genes (as opposed to the cells) and then groups them into modules using Louvain community analysis.\n",
      "# get gene IDs for which q-value < 0.05\n",
      "pr_deg_ids <- row.names(subset(graph_test_res, q_value < 0.05)) \t\n",
      "# group genes into modules\n",
      "gene_module_df <- find_gene_modules(cds_order[pr_deg_ids,], resolution=1e-2) \t\n",
      "Now we can show the aggregate expression of all genes in each module across all the clusters. Monocle provides a simple utility function called aggregate_gene_expression for this purpose:\n",
      "# aggregate expression of genes in each module\n",
      "cell_group_df <- tibble::tibble(cell=row.names(colData(cds_order)),\n",
      "                                cell_group=clusters(cds_order)[colnames(cds_order)])\n",
      "agg_mat <- aggregate_gene_expression(cds_order, gene_module_df, cell_group_df)\n",
      "row.names(agg_mat) <- stringr::str_c(\"Module \", row.names(agg_mat))\n",
      "colnames(agg_mat) <- stringr::str_c(\"Cluster \", colnames(agg_mat))\n",
      "We can now use this data to create a heatmap. Don’t worry if yours look a little bit different that the one shown in this tutorial. The general features should be maintained though.\n",
      "# create a heatmap\n",
      "pheatmap::pheatmap(agg_mat, cluster_rows=TRUE, cluster_cols=TRUE,\n",
      "                   scale=\"column\", clustering_method=\"ward.D2\",\n",
      "                   fontsize=6)\n",
      "You can also visualise the modules using plot_cells() function. We’ve chosen some modules to see how the differences on a heatmap correlate with the expression shown on our starting plot.\n",
      "Question\n",
      "Which modules to plot?\n",
      "This is totally up to you! It might be the case that you got different numbering of modules, so then using the numbers specified in the code below won’t make much sense. Just look at your heatmap, compare the differences between modules and think which ones would be the most interesing to visualise.\n",
      "# see the chosen modules across the whole sample\n",
      "plot_cells(cds_order,\n",
      "           genes=gene_module_df %>% dplyr::filter(module %in% c(40, 39, 36, 17)),\n",
      "           group_cells_by=\"cluster\",\n",
      "           color_cells_by=\"cluster\",\n",
      "           show_trajectory_graph=FALSE)\n",
      "With the visualisation methods above, you can now come back to the generated data frame gene_module_df, filter genes that belong to the module of interest and check their functions to get some more evidence for the correct biological interpretation.\n",
      "3D plotting\n",
      "Let’s have some fun at the end! That was quite a long and insightful analysis – you definitely deserve to look at some nice, rotatable, cool plot now! Essentially the workflow is the same as we followed in two dimensions. The crucial part is to specify the dimensionality of the reduced space with the max_components parameter in reduce_dimension() function. The default is 2, but if we want to see our data in 3D, we will change that value to 3. From there, you can just repeat the next steps in 3D… or just reward yourself for completing this tutorial by toggling this beautiful 3D plot!\n",
      "# reduce dimension to 3D\n",
      "cds_3d <- reduce_dimension(cds_order, preprocess_method = 'Aligned', max_components = 3)\n",
      "# see the resulting 3D plot\n",
      "plot_cells_3d(cds_3d, color_cells_by=\"cell_type\")\n",
      "Export your data, figures, and notebook\n",
      "Don’t forget to save and export your data! First, we will get Jupyter to see those as files.\n",
      "Export plots\n",
      "If you want to export your plot, you have to make sure that you assigned it to an object. For example, if you want to save the plot of cells in pseudotime, simply assign the function you used to generate this plot to an object. Here we call this object plot_pseudotime, like so:\n",
      "plot_pseudotime <- plot_cells(cds_order,\n",
      "           color_cells_by = \"pseudotime\",\n",
      "           label_cell_groups=FALSE,\n",
      "           label_leaves=FALSE,\n",
      "           label_branch_points=FALSE)\n",
      "Then, if you want to save the plot as PDF:\n",
      "pdf(\"plot_pseudotime.pdf\")             # open the graphical device and specify the directory and the name of the output pdf file\n",
      "plot_pseudotime                        # specify the object that your plot is assigned to\n",
      "dev.off()                              # close the graphical device\n",
      "The procedure is very similar if you want to export the file as PNG (or analogically JPEG – just replace png with jpeg):\n",
      "png(\"plot_pseudotime.png\",            # open the graphical device and specify the directory and the name of the output png file\n",
      "width=600, height=400)                # optionally you can specify the width and height of the final plot\n",
      "plot_pseudotime                       # specify the object that your plot is assigned to\n",
      "dev.off()                             # close the graphical device\n",
      "However, it often happens that the quality of the exported PNG and JPEG files is not perfect. For best results, we recommend exporting to SVG:\n",
      "svg(\"plot_pseudotime.svg\")             # open the graphical device and specify the directory and the name of the output svg file\n",
      "plot_pseudotime                        # specify the object that your plot is assigned to\n",
      "dev.off()                              # close the graphical device\n",
      "You can do the same with any plot that you want to save! You will find the saved figures in the left panel of your JupyterLab. You can right-click on them and download directly onto your computer. You can also push them into your Galaxy history. To do so, you have to change Kernel to Python3 (either click on Kernel -> Change Kernel... in the upper left corner of your JupyterLab or click on the displayed current kernel in the upper right corner and change it).\n",
      "Check in the upper right corner that selected kernel is Python3, and run the following:\n",
      "put(\"marker_file.txt\")\n",
      "put(\"plot_pseudotime.pdf\")\n",
      "put(\"plot_pseudotime.png\")\n",
      "In this way you can push all the files you’ve saved into your Galaxy history. You can also do the same with this notebook. The cell below will only work if you haven’t changed the name of the notebook. If you renamed it, simply type its new name in the parenthesis.\n",
      "put(\"single-cell-scrna-case_monocle3-rstudio.ipynb\")\n",
      "Now you can go check your Galaxy history to make sure your files have all made it back into your Galaxy history.\n",
      "After Jupyter\n",
      "Congratulations! You’ve made it through Jupyter!\n",
      "Hands On: Closing JupyterLab\n",
      "Click User: Active Interactive Tools\n",
      "Tick the box of your Jupyter Interactive Tool, and click Stop\n",
      "If you want to run this notebook again, or share it with others, it now exists in your history. You can use this ‘finished’ version just the same way as you downloaded the directions file and uploaded it into the Jupyter environment.\n",
      "Conclusion\n",
      "If you’re following the Case Study tutorials from the beginning, you have already experienced what it’s like to analyse and question a dataset, potentially without clear cut-offs or clear answers. The Monocle in Galaxy tutorial was focused more on explaining each step of the trajectory analysis and interpreting the results in the biological context. The current tutorial aims at showing the variety of methods that can be used when Galaxy’s Monocle tools are not enough. It shows the potential of batch correction, differential expression analysis and flexibility when using different functions. It’s also a guide for those who would like to understand what is happening ‘behind the scenes’ when clicking on Galaxy buttons.\n",
      "You've Finished the Tutorial\n",
      "Please also consider filling out the Feedback Form as well!\n",
      "Key points\n",
      "Monocle3 in R gives more flexibility when it comes to differential expression analysis and plotting, but Galaxy offers great reproducibility and ease of analysis.\n",
      "Comparing the output of several different methods applied on the same dataset might be useful to confirm the results, to ensure that the findings are reliable and even sometimes to find a new piece of information.\n",
      "Frequently Asked Questions\n",
      "Have questions about this tutorial? Have a look at the available FAQ pages and support channels\n",
      "Tutorial FAQs\n",
      "Topic FAQs\n",
      "Galaxy FAQs\n",
      "GTN Matrix Chat\n",
      "Galaxy Help Forum\n",
      "Useful literature\n",
      "Further information, including links to documentation and original publications, regarding the tools, analysis techniques and the interpretation of results described in this tutorial can be found here.\n",
      "References\n",
      "Levine, J. H., E. F. Simonds, S. C. Bendall, K. L. Davis, E.-ad D. Amir et al., 2015 Data-Driven Phenotypic Dissection of AML Reveals Progenitor-like Cells that Correlate with Prognosis. Cell 162: 184–197. 10.1016/j.cell.2015.05.047\n",
      "Bacon, W. A., R. S. Hamilton, Z. Yu, J. Kieckbusch, D. Hawkes et al., 2018 Single-Cell Analysis Identifies Thymic Maturation Delay in Growth-Restricted Neonatal Mice. Frontiers in Immunology 9: 10.3389/fimmu.2018.02523\n",
      "Cao, J., M. Spielmann, X. Qiu, X. Huang, D. M. Ibrahim et al., 2018 The single-cell transcriptional landscape of mammalian organogenesis. Nature 566: 496–502. 10.1038/s41586-019-0969-x https://10.1038/s41586-019-0969-x\n",
      "Haghverdi, L., A. T. L. Lun, M. D. Morgan, and J. C. Marioni, 2018 Batch effects in single-cell RNA-sequencing data are corrected by matching mutual nearest neighbors. Nature Biotechnology 36: 421–427. 10.1038/nbt.4091\n",
      "Traag, V. A., L. Waltman, and N. J. van Eck, 2019 From Louvain to Leiden: guaranteeing well-connected communities. Sci Rep 9: 10.1038/s41598-019-41695-z\n",
      "Wolf, F. A., F. K. Hamey, M. Plass, J. Solana, J. S. Dahlin et al., 2019 PAGA: graph abstraction reconciles clustering with trajectory inference through a topology preserving map of single cells. Genome Biol 20: 10.1186/s13059-019-1663-x\n",
      "Feedback\n",
      "Did you use this material as an instructor? Feel free to give us feedback on how it went. Did you use this material as a learner or student? Click the form below to leave feedback.\n",
      "Citing this Tutorial\n",
      "Julia Jakiela, Inferring single cell trajectories with Monocle3 (R) (Galaxy Training Materials). https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_monocle3-rstudio/tutorial.html Online; accessed TODAY\n",
      "Hiltemann, Saskia, Rasche, Helena et al., 2023 Galaxy Training: A Powerful Framework for Teaching! PLOS Computational Biology 10.1371/journal.pcbi.1010752\n",
      "Batut et al., 2018 Community-Driven Data Analysis Training for Biology Cell Systems 10.1016/j.cels.2018.05.012\n",
      "\n",
      "\n",
      "@misc{single-cell-scrna-case_monocle3-rstudio,\n",
      "author = \"Julia Jakiela\",\n",
      "\ttitle = \"Inferring single cell trajectories with Monocle3 (R) (Galaxy Training Materials)\",\n",
      "\tyear = \"\",\n",
      "\tmonth = \"\",\n",
      "\tday = \"\",\n",
      "\turl = \"\\url{https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-case_monocle3-rstudio/tutorial.html}\",\n",
      "\tnote = \"[Online; accessed TODAY]\"\n",
      "}\n",
      "@article{Hiltemann_2023,\n",
      "\tdoi = {10.1371/journal.pcbi.1010752},\n",
      "\turl = {https://doi.org/10.1371%2Fjournal.pcbi.1010752},\n",
      "\tyear = 2023,\n",
      "\tmonth = {jan},\n",
      "\tpublisher = {Public Library of Science ({PLoS})},\n",
      "\tvolume = {19},\n",
      "\tnumber = {1},\n",
      "\tpages = {e1010752},\n",
      "\tauthor = {Saskia Hiltemann and Helena Rasche and Simon Gladman and Hans-Rudolf Hotz and Delphine Larivi{\\`{e}}re and Daniel Blankenberg and Pratik D. Jagtap and Thomas Wollmann and Anthony Bretaudeau and Nadia Gou{\\'{e}} and Timothy J. Griffin and Coline Royaux and Yvan Le Bras and Subina Mehta and Anna Syme and Frederik Coppens and Bert Droesbeke and Nicola Soranzo and Wendi Bacon and Fotis Psomopoulos and Crist{\\'{o}}bal Gallardo-Alba and John Davis and Melanie Christine Föll and Matthias Fahrner and Maria A. Doyle and Beatriz Serrano-Solano and Anne Claire Fouilloux and Peter van Heusden and Wolfgang Maier and Dave Clements and Florian Heyl and Björn Grüning and B{\\'{e}}r{\\'{e}}nice Batut and},\n",
      "\teditor = {Francis Ouellette},\n",
      "\ttitle = {Galaxy Training: A powerful framework for teaching!},\n",
      "\tjournal = {PLoS Comput Biol}\n",
      "}\n",
      "\n",
      "                   \n",
      "Funding\n",
      "These individuals or organisations provided funding support for the development of this resource\n",
      "Logo\n",
      "EOSC-Life\n",
      "EOSC-Life has received funding from the European Union’s Horizon 2020 programme under grant agreement number 824087\n",
      "Congratulations on successfully completing this tutorial!\n",
      "Do you want to extend your knowledge?\n",
      "Follow one of our recommended follow-up trainings:\n",
      "tutorial Hands-on: Importing files from public atlases\n",
      "slides Slides: GO Enrichment Analysis on Single-Cell RNA-Seq Data\n",
      "tutorial Hands-on: GO Enrichment Analysis on Single-Cell RNA-Seq Data\n",
      "You can use Ephemeris's shed-tools install command to install the tools used in this tutorial.\n",
      "shed-tools install [-g GALAXY] [-a API_KEY] -t <(curl https://training.galaxyproject.org/training-material/api/topics/single-cell/tutorials/scrna-case_monocle3-rstudio/tutorial.json | jq .admin_install_yaml -r)\n",
      "Alternatively you can copy and paste the following YAML\n",
      "---\n",
      "install_tool_dependencies: true\n",
      "install_repository_dependencies: true\n",
      "install_resolver_dependencies: true\n",
      "tools: []\n",
      "No feedback has been recieved yet for this training. Be the first one by filling in the feedback form.\n",
      "GTN\n",
      "The GTN provides learners with a free, open repository of online training materials, with a focus on hands-on training that aims to be directly applicable for learners. We aim to connect researchers and learners with local trainers, and events worldwide.\n",
      "We promote FAIR and Open Science practices worldwide, are committed to the accessibility of this platform and training for everyone.\n",
      "About Us\n",
      "About\n",
      "Code of Conduct\n",
      "Accessibility\n",
      "100% FAIR Training\n",
      "Collaborative Development\n",
      "Page\n",
      "purlPURL: gxy.io/GTN:T00336\n",
      "Content licensed under Creative Commons Attribution 4.0 International License\n",
      "github Edit on GitHub\n",
      "github View Changes on GitHub\n",
      "Support\n",
      "Galaxy FAQs\n",
      "Galaxy Help Forum\n",
      "GTN Slack Chat\n",
      "GTN Matrix Chat\n",
      "Galaxy Matrix Chat\n",
      "Framework\n",
      "Revision 3df99a6\n",
      "MIT Licensed\n",
      "Jekyll(4.3.2 | production)\n",
      "Follow Us!\n",
      "Mastodon\n",
      "Bluesky\n",
      "Publications\n",
      "Hiltemann et al. 2023\n",
      "Batut et al. 2018\n",
      "Citing Us\n",
      "\n",
      "---\n",
      "### Page 1:  \n",
      "AhmedRezk59 / single-cell-trajectory-analysis Public\n",
      "Notifications\n",
      "Fork 0\n",
      "Star 0\n",
      "0 stars 0 forks Branches Tags Activity\n",
      "Star\n",
      "Notifications\n",
      "\n",
      "---\n",
      "### Page 2:  \n",
      "Please enable cookies.\n",
      "Sorry, you have been blocked\n",
      "You are unable to access classcentral.com\n",
      "Why have I been blocked?\n",
      "This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.\n",
      "What can I do to resolve this?\n",
      "You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.\n",
      "Cloudflare Ray ID: 93fbc3338d3d67f1 • Your IP: 23.172.200.70 • Performance & security by Cloudflare\n",
      "\n",
      "---\n",
      "### Page 3:  \n",
      "\n",
      "---\n",
      "### Page 4:  \n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(crawl_state['crawled_webs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc205344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_code_blocks(\"```bash\\nRscript test.R\\n```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7463f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor\n",
    "from autogen_core.code_executor import CodeBlock\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "async with DockerCommandLineCodeExecutor(\n",
    "    image = \"r-base:latest\", \n",
    "    work_dir=\"Test\", # Path in local native env\n",
    "    #bind_dir=\"./\", # Path inside the docker\n",
    "\n",
    "    ) as executor:  # type: ignore\n",
    "    print(\n",
    "        await executor.execute_code_blocks(\n",
    "            code_blocks=[\n",
    "                CodeBlock(\n",
    "                    language=\"bash\",  # 'bash', 'shell', 'sh', 'pwsh', 'powershell', 'ps1', 'python'\n",
    "                    code=\"Rscript script.R\"),\n",
    "            ],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_core.code_executor import CodeBlock\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    work_dir = '.'\n",
    ")\n",
    "\n",
    "exeres = await executor.execute_code_blocks(\n",
    "    code_blocks= [CodeBlock(\n",
    "        language='bash',\n",
    "        code='ls'\n",
    "    )],\n",
    "    cancellation_token=CancellationToken(),\n",
    ")\n",
    "\n",
    "exeres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docker_status_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9427ac",
   "metadata": {},
   "source": [
    "### Model preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9397571",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"123123\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../ghostcoder/docker/BIA_dockers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Tavily\n",
    "tavily_api = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "adata = sc.datasets.pbmc3k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial graph\n",
    "agent = GhostCoder(\n",
    "    chat_model = chat_model, \n",
    "    code_model = code_model, \n",
    "    )\n",
    "\n",
    "# Draw graph\n",
    "agent.draw_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a915b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test agent\n",
    "task = \"Quality control of the data. wherein the genes are targeted for labeling, the mitochondrial genes (e.g., beginning with “MT-”), the ribosomal genes (e.g., beginning with “RPS” or “RPL”), and the hemoglobin genes (using regular expression matching, e.g., ^HB[^P]); next, common quality control metrics for each cell, including total counts, number of genes detected, and the proportion of total counts represented by a specific group of genes (e.g., mitochondrial genes), were calculated using Scanpy's calculate_qc_metrics() function, and a The log1p transformation is applied to these metrics to optimize the data distribution. Subsequently, the QC metrics of each cell are visualized by violin plots and scatter plots to assess the overall quality of the data. Finally, a threshold is set based on the visualization results to exclude the cells with fewer than 100 genes and genes occurring in fewer than 3 cells to ensure the quality of the data for downstream analyses.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Ghost coder\n",
    "agent.Run(\n",
    "    task, \n",
    "    input_wrap = input_variable_wrapper([adata])\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIA-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
