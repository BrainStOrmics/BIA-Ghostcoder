{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ccf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload    \n",
    "%autoreload 2    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from pathlib import Path # To set data downloading path\n",
    "\n",
    "# Append ghostcoder folder to path \n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import ghostcoder\n",
    "\n",
    "# For visualize the graph\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "bgi_key = 'sk-NYPE5WNGTK781Nox801a5934A9F84a03BfC525359eBaE7B6'\n",
    "bgi_base = 'http://10.224.28.80:3000/v1'\n",
    "\n",
    "openai_key = \"OYXxhyOyPDsJNABxugxjJJxntQ0B1EB0\"\n",
    "openai_base = \"https://api3.aitok.ai/openai/v1\"\n",
    "\n",
    "openai_chat_model = 'gpt-4o'\n",
    "openai_code_model = 'gpt-4o'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89124644",
   "metadata": {},
   "source": [
    "Here we will test and illustrate each subgraph in Ghostcoder, as an important component of BIA (bioinformatics agnet), mainly functions to complete the generation and execution of bioinformatics analysis codes. It contains five subgraphs. They are filemanager, retriever, coder and webcrawler executor respectively.\n",
    "\n",
    "\n",
    "### Omics data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8466b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#──────work_dir \n",
    "#  └───data\n",
    "#    └─Input_data.whatever\n",
    "# First lets download a scRNAseq data\n",
    "import scanpy as sc\n",
    "\n",
    "from ghostcoder.config import file_config\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Ghostcoder pre-set WORK_DIR and INPUT_DATA_DIR for continues bioinformatics tasks using one input data\n",
    "\n",
    "# Download scRNAseq data\n",
    "sc.settings.datasetdir = current_dir/ file_config.INPUT_DATA_DIR # Download data into data/ folder in current dir\n",
    "sc.datasets.pbmc3k()\n",
    "\n",
    "# Create a data description file to illustrate the scRNAseq data details \n",
    "data_des = \"The data used in this basic preprocessing and clustering tutorial was collected from bone marrow mononuclear cells of healthy human donors. The samples used in this tutorial were measured using the 10X Multiome Gene Expression and Chromatin Accessability kit.\"\n",
    "\n",
    "with open('data/data_description.txt','w') as f:\n",
    "    f.write(data_des)\n",
    "    \n",
    "# Set workdir as current dir\n",
    "file_config.WORK_DIR = current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb5572",
   "metadata": {},
   "source": [
    "### Set up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Openai API for example, you can use other LLM provider as well\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# I will recommend use Openai API, I haven't tested any APIs from other suppliers yet\n",
    "def call_chatllm_openai(key, api_base, model_name):\n",
    "    llm = ChatOpenAI(\n",
    "        api_key = key,\n",
    "        base_url= api_base,\n",
    "        model = model_name,\n",
    "        temperature= 0,\n",
    "        max_retries = 3,\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "# # Setup up api keys \n",
    "# openai_key = \"\"\n",
    "# openai_base = \"\"\n",
    "# openai_chat_model = \"\" \n",
    "# openai_code_model = \"\"\n",
    "\n",
    "# Here, I recommend using an LLM with stronger coding capabilities as the code model, which is set to perform code-related work in all Ghostcoder graphs. Meanwhile, it is better to use LLM with stronger reasoning ability as the chat model.\n",
    "chat_model = call_chatllm_openai(openai_key, openai_base, openai_chat_model)\n",
    "code_model = call_chatllm_openai(openai_key, openai_base, openai_code_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28add431",
   "metadata": {},
   "source": [
    "### Set up Tavily search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7aaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ghostcoder.config import tavily_config\n",
    "# Initial Tavily\n",
    "# Currently, all web search functional is based on Tavily search, it's provide 10000 query/month free credit, more see https://app.tavily.com/home\n",
    "tavily_api = \"\"\n",
    "\n",
    "# You can set up Tavily by os environ\n",
    "os.environ[\"TAVILY_API_KEY\"] = tavily_api\n",
    "# or with config, which work inside Ghostcoder only\n",
    "tavily_config.TAVILY_API_KEY = tavily_api\n",
    "\n",
    "# Set up max results of each Tavily query\n",
    "tavily_config.MAX_RESULTS = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6a85d",
   "metadata": {},
   "source": [
    "### File management and data perception by ghostcoder.filemanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fill manager will automatically set up file system and percept the input data \n",
    "# A initial file system status for a task with task_id should as follow: \n",
    "#─┬─────work_dir \n",
    "# ├─┬───data\n",
    "# │ └───Input_data.whatever\n",
    "# └─┬───task_id_1 // Work dir for every new tasks\n",
    "#   ├─┬─data  \n",
    "#   │ └─Input_data.whatever // A copy from work_dir/data/\n",
    "#   ├───figures // Where output figures will be saved\n",
    "#   └───results // Where processed data will be saved\n",
    "from ghostcoder.config import docker_config, file_config\n",
    "from ghostcoder.graph import create_filemanager_agent\n",
    "\n",
    "#  Set workdir as current dir\n",
    "current_dir = Path.cwd()\n",
    "file_config.WORK_DIR = current_dir\n",
    "\n",
    "\n",
    "# Initial graph\n",
    "manager = create_filemanager_agent(\n",
    "        chat_model = chat_model, \n",
    "        code_model = code_model,\n",
    "        max_retry = 3,\n",
    "        )\n",
    "\n",
    "fm_input = {\n",
    "    \"task_id\" : \"Test\", # \n",
    "    \"docker_profile_dir\": docker_config.DOCKER_PROFILES_DIR, # use pre-set docker profiles, please read those docker images \n",
    "    \"max_iter\": 10,\n",
    "}\n",
    "\n",
    "fm_state = await manager.ainvoke(fm_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph [optional], if failed try to run the cell again \n",
    "Image(manager.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e4b32",
   "metadata": {},
   "source": [
    "### Native env and Docker executor, test along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ghostcoder.graph import create_executor_agent\n",
    "from ghostcoder.docker import get_docker_status\n",
    "from ghostcoder.utils import get_native_env_perception\n",
    "\n",
    "#  Set workdir as current dir\n",
    "current_dir = Path.cwd()\n",
    "file_config.WORK_DIR = current_dir\n",
    "\n",
    "# Set up environment profiles\n",
    "env_profiles = {\n",
    "    \"task_dirs\":{\n",
    "        \"task_dir\": \"Test\",\n",
    "        \"data_dir\": \"data\",\n",
    "        \"figure_dir\": \"figures\",\n",
    "        \"output_dir\": \"results\",\n",
    "    },\n",
    "    \"docker status\": get_docker_status(),\n",
    "    \"native env languages\": get_native_env_perception(),\n",
    "}\n",
    "\n",
    "# Initial agent\n",
    "agent = create_executor_agent(\n",
    "        chat_model = chat_model, \n",
    "        code_model = code_model,\n",
    "        max_retry = 3,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Test executor with bash python and R\n",
    "test_bash_code =\"\"\"\n",
    "ls -al\n",
    "\"\"\"\n",
    "\n",
    "test_python_code = \"\"\"\n",
    "print(\"Hello World\")\n",
    "\"\"\"\n",
    "\n",
    "test_r_code =\"\"\"\n",
    "my_str <- \"Hello World\"\n",
    "print(my_str)\n",
    "\"\"\"\n",
    "\n",
    "for codeblock in [\n",
    "    test_bash_code, \n",
    "    test_python_code, \n",
    "    test_r_code\n",
    "    ]:\n",
    "    print(\"Test code executor with\\n\",codeblock)\n",
    "\n",
    "    exe_states = await agent.ainvoke(\n",
    "        {\n",
    "            \"generated_codeblock\":codeblock,\n",
    "            \"env_profiles\": env_profiles,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Agent detected coding language as: {exe_states['language']}\\nUse docker: {exe_states['use_docker']}\")\n",
    "    if exe_states['use_docker']:\n",
    "        print(f\"With docker image{exe_states['docker_image']}\")\n",
    "    print(f\"Code execute output:\\n{exe_states['execution_results']}\\n--------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph [optional], if failed try to run the cell again \n",
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc5881",
   "metadata": {},
   "source": [
    "### Web crawler, test along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "037290a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: No content loaded.\n",
      "Attempt 2: No content loaded.\n",
      "Attempt 3: No content loaded.\n",
      "Attempt 1: No content loaded.\n",
      "Attempt 2: No content loaded.\n",
      "Attempt 3: No content loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api3.aitok.ai/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Generated query for given context:\n",
      "R code for single cell trajectory analysis using Monocle3\n",
      "Monocle3 single cell trajectory analysis R script\n",
      "How to perform single cell trajectory analysis with Monocle3 in R\n",
      "Get total 5 useful web search results\n",
      "Crawled and parsed web information as follow:\n",
      "The tutorial \"Inferring single cell trajectories with Monocle3 (R)\" provides a comprehensive guide on performing trajectory analysis using Monocle3 in R. It covers the transformation of an AnnData object into Monocle-compatible files, installation of necessary packages, and the Monocle3 workflow, including pre-processing, batch correction, dimensionality reduction, clustering, and trajectory inference. The tutorial also explains how to assign cell types to clusters using both supervised and unsupervised approaches and perform differential expression analysis. Key R code snippets from the tutorial include:\n",
      "\n",
      "```r\n",
      "# Install Bioconductor and some of its dependencies\n",
      "if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
      "  install.packages(\"BiocManager\")\n",
      "BiocManager::install(version = \"3.14\")\n",
      "BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats',\n",
      "                       'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment',\n",
      "                       'SummarizedExperiment', 'batchelor', 'HDF5Array',\n",
      "                       'terra', 'ggrastr'))\n",
      "\n",
      "# Install monocle3 through the cole-trapnell-lab GitHub:\n",
      "install.packages(\"devtools\")\n",
      "devtools::install_github('cole-trapnell-lab/monocle3')\n",
      "library(monocle3)\n",
      "\n",
      "# Create CDS object\n",
      "cds <- new_cell_data_set(expression_matrix, cell_metadata, gene_metadata)\n",
      "\n",
      "# Pre-processing\n",
      "cds_preprocessing <- preprocess_cds(cds, method = \"PCA\", num_dim = 210)\n",
      "\n",
      "# Batch correction\n",
      "cds_batch <- align_cds(cds_preprocessing_UMAP, preprocess_method = \"PCA\", alignment_group = \"batch\")\n",
      "\n",
      "# Clustering\n",
      "cds_clustered <- cluster_cells(cds_red_dim, reduction_method = \"UMAP\", resolution = 0.0002)\n",
      "\n",
      "# Trajectory inference\n",
      "cds_trajectory <- learn_graph(cds_annotated, use_partition=TRUE)\n",
      "\n",
      "# Order cells\n",
      "cds_order_1_helper <- order_cells(cds_trajectory, root_pr_nodes = DN_node_id)\n",
      "\n",
      "# Plotting in pseudotime\n",
      "plot_cells(cds_order,\n",
      "           color_cells_by = \"pseudotime\",\n",
      "           label_cell_groups=FALSE,\n",
      "           label_leaves=FALSE,\n",
      "           label_branch_points=FALSE)\n",
      "```\n",
      "\n",
      "The tutorial emphasizes the flexibility of Monocle3 in R for differential expression analysis and plotting, while also highlighting the reproducibility and ease of use offered by Galaxy tools.\n"
     ]
    }
   ],
   "source": [
    "from ghostcoder.graph import create_crawler_agent\n",
    "\n",
    "# Initial agent\n",
    "agent = create_crawler_agent(\n",
    "        chat_model = chat_model, \n",
    "        code_model = code_model,\n",
    "        max_retry = 3,\n",
    "    )\n",
    "\n",
    "# Input \n",
    "query_context = \"I'm looking for R codes for analysis single cell trajectory using monocl3\"\n",
    "\n",
    "# Invoke\n",
    "crawl_state = agent.invoke(\n",
    "    {\n",
    "        \"query_context\": query_context,\n",
    "    }\n",
    "    )\n",
    "\n",
    "# Print results\n",
    "print(\"--------\\nGenerated query for given context:\")\n",
    "for q in crawl_state['query_list']:\n",
    "    print(q)\n",
    "print(f\"Get total {len(crawl_state['useful_results'])} useful web search results\")\n",
    "print(\"Crawled and parsed web information as follow:\")\n",
    "print(crawl_state['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph [optional], if failed try to run the cell again \n",
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31896960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52991ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2336778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIA-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
