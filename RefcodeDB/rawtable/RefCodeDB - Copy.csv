Omics,Task,Description,Instruction,Tools,Language,CodeBlock,Source,Reviewed
scRNA-seq,Quality control,"The single-cell RNA sequencing analysis was conducted using the Scanpy toolkit in Python. The workflow involved annotating mitochondrial, ribosomal, and hemoglobin genes based on established naming conventions. Following annotation, key quality control metrics such as gene counts per cell, total transcript counts, and the percentage of mitochondrial counts were calculated. These metrics were then visualized through violin and scatter plots to assess cell quality distributions and identify outlier populations, informing subsequent filtering steps.","The single-cell RNA sequencing analysis was performed using the Scanpy toolkit in Python. The analysis began with a publicly available dataset of 3,000 Peripheral Blood Mononuclear Cells (PBMCs), which was loaded as an AnnData object. This data structure contains the gene expression matrix, with cells as rows and genes as columns, along with associated metadata for both cells and genes.

For quality control, we first annotated specific gene sets. Mitochondrial genes were identified by searching for gene names starting with the prefix ""MT-"". Similarly, ribosomal genes were identified by prefixes ""RPS"" or ""RPL"", and hemoglobin genes were identified by the prefix ""HB"", excluding phosphoglycerate mutases. These annotations were stored as boolean flags in the gene-level metadata.

Following annotation, quality control metrics were calculated for each cell using the `calculate_qc_metrics` function. This step computed several metrics, including the total number of genes detected per cell (`n_genes_by_counts`), the total number of transcript counts per cell (`total_counts`), and the percentage of counts derived from the mitochondrial, ribosomal, and hemoglobin gene sets defined previously. The results were log-transformed using a `log(1+x)` transformation and added directly to the cell-level metadata of the AnnData object for subsequent filtering and analysis.

To visually inspect the quality control metrics and determine appropriate filtering thresholds, we generated several plots. Violin plots were created to visualize the distribution of `n_genes_by_counts`, `total_counts`, and the percentage of mitochondrial counts (`pct_counts_mt`) across all cells. These plots were generated on a multi-panel layout with a jitter value of 0.4 to show individual cell data points. Additionally, a scatter plot was generated to examine the relationship between the total counts and the number of genes per cell, with each cell colored by its corresponding percentage of mitochondrial counts. This visualization aids in identifying outlier cells, such as those with high mitochondrial content, which may be indicative of cellular stress or apoptosis.","scanpy, anndata",python,"#----------------
# Import necessary modules
#----------------
import scanpy as sc
import anndata as ad

# Set figure parameters for decent plotting
sc.settings.set_figure_params(dpi=80, facecolor='white')

#----------------
# Data Input: load a sample dataset
#----------------
adata = sc.datasets.pbmc3k()
# Data format: 
# adata is an AnnData object containing the following core structure 
# x: core data matrix (cells × genes), usually a sparse matrix or numpy array
# obs: cell-level metadata (DataFrame), stores properties for each cell
# var: gene-level metadata (DataFrame), stores attributes for each gene


#----------------
# Annotate genes for QC metrics and calculate metrics
#----------------

# Mark mitochondrial genes
adata.var[""mt""] = adata.var_names.str.startswith(""MT-"")

# Mark ribosomal genes
adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL""))

# Mark hemoglobin genes
adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]"")

# Calculate QC metrics
sc.pp.calculate_qc_metrics(
    adata, 
    qc_vars=[""mt"", ""ribo"", ""hb""],
    inplace=True,
    log1p=True
)

#----------------
# Create QC visualization plots
#----------------
# Violin plots for key metrics
sc.pl.violin(
    adata,
    keys=[""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],
    jitter=0.4,
    multi_panel=True
)

# Scatter plot showing counts vs genes with mitochondrial percentage
sc.pl.scatter(
    adata, 
    x=""total_counts"",
    y=""n_genes_by_counts"",
    color=""pct_counts_mt""
)

#----------------
# Data output 
#----------------
adata.write('pbmc3k_qc.h5ad')",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,T
scRNA-seq,Quality control,"The single-cell analysis was conducted in Python with the Scanpy library. The workflow involved annotating key gene sets, including mitochondrial, ribosomal, and hemoglobin genes. Quality control metrics such as gene counts and mitochondrial percentages were computed for each cell. A critical step included batch-aware doublet detection using the Scrublet algorithm to calculate doublet scores and predictions. Finally, these QC metrics and doublet scores were jointly visualized using violin and scatter plots to comprehensively identify low-quality cells and technical artifacts prior to downstream analysis.","The single-cell analysis was conducted in Python using the Scanpy library. The process started with the loading of the standard 3k Peripheral Blood Mononuclear Cell (PBMC) dataset. To enable batch-specific processing, a single sample identifier was assigned to all cells in the dataset.

For quality control, genes were first categorized based on their function. Mitochondrial genes were identified by the prefix ""MT-"", ribosomal genes by the prefixes ""RPS"" or ""RPL"", and hemoglobin genes by the prefix ""HB"". Following this annotation, a suite of quality control metrics was computed for each cell. These metrics included the number of unique genes detected, the total number of transcript counts, and the percentage of reads mapping to the previously defined mitochondrial, ribosomal, and hemoglobin gene sets. The count-based metrics were log-transformed to normalize their distributions.

To address potential technical artifacts, we implemented a doublet detection procedure using the Scrublet algorithm. The algorithm was applied on a per-sample basis, using the assigned sample identifier as the `batch_key`, to estimate a sample-specific doublet score for each cell and make a binary prediction of its doublet status.

Finally, to guide subsequent filtering steps, the results of the quality control and doublet detection were visualized. Violin plots were used to display the distributions of the number of genes per cell, total counts, and the percentage of mitochondrial counts. To explore the relationships between these metrics, scatter plots were generated showing total counts versus the number of genes, with cells colored by either their mitochondrial DNA percentage or their predicted doublet score. This allowed for a comprehensive assessment of cell quality and the identification of potential outliers and technical artifacts prior to downstream analysis.","scanpy, anndata",python,"#----------------
# Package Load
#----------------
import scanpy as sc
import anndata as ad

#----------------
# Data Input
#----------------
# Load built-in PBMC 3k dataset
adata = sc.datasets.pbmc3k()

# Add dummy sample identifier required for batch-level analysis
adata.obs[""sample""] = ""sample1""

#----------------
# Preprocessing & Quality Control
#----------------
# Flag mitochondrial, ribosomal and hemoglobin genes
adata.var[""mt""] = adata.var_names.str.startswith(""MT-"")       # Mitochondrial genes
adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL""))  # Ribosomal genes
adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]"")   # Hemoglobin genes

# Calculate quality control metrics
sc.pp.calculate_qc_metrics(
    adata, 
    qc_vars=[""mt"", ""ribo"", ""hb""],
    inplace=True,
    log1p=True
)

# Detect potential doublets using Scrublet
sc.pp.scrublet(adata, batch_key=""sample"")

#----------------
# Visualization
#----------------
# Configure plotting parameters
sc.settings.set_figure_params(dpi=80, facecolor=""white"")

# QC distributions
sc.pl.violin(
    adata,
    keys=[""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],
    jitter=0.4,
    multi_panel=True,
    title=""Quality Control Distributions""
)

# Relationship between read depth and mitochondrial content
sc.pl.scatter(
    adata,
    x=""total_counts"",
    y=""n_genes_by_counts"",
    color=""pct_counts_mt"",
    title=""Gene Counts vs. Total Reads (MT% Colored)""
)

# Doublet visualization
for color_field in [""predicted_doublet"", ""doublet_score""]:
    sc.pl.scatter(
        adata,
        x=""total_counts"",
        y=""n_genes_by_counts"",
        color=color_field,
        title=f""Doublet Detection: {color_field.replace('_', ' ').title()}""
    )

#----------------
# Data Output
#----------------
# Save processed data for future analysis
adata.write(""processed_pbmc3k.h5ad"", compression=""gzip"")",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,F
scRNA-seq,Quality control,"The analysis was performed in Python using the Scanpy library, beginning with the annotation of mitochondrial, ribosomal, and hemoglobin gene sets. A comprehensive suite of quality control metrics was calculated, including transcript and gene counts, and percentages from annotated genes. A robust adaptive filtering strategy based on the median absolute deviation (MAD) was employed to flag both general and mitochondrial-specific outliers. Following the removal of these flagged cells, the quality of the filtered dataset was confirmed by visualizing the improved distributions of key metrics, ensuring a homogenous cell population for downstream analysis.","The analysis was performed in Python using the Scanpy library. Single-cell gene expression data was loaded from a 10x Genomics filtered feature-barcode matrix file. To ensure compatibility with downstream tools, any duplicate gene names were made unique.

A comprehensive quality control pipeline was implemented to identify and remove low-quality cells. First, specific gene sets were annotated to serve as quality indicators. Mitochondrial genes, identified by the prefix ""MT-"", were used to assess cell stress and viability. Ribosomal genes, marked by ""RPS"" or ""RPL"" prefixes, and hemoglobin genes, identified by the ""HB"" prefix, were also annotated. Following annotation, quality control metrics were calculated for each cell, including the total transcript counts, the number of genes detected, the percentage of counts from mitochondrial, ribosomal, and hemoglobin genes, and the percentage of counts from the top 20 most highly expressed genes. These count-based metrics were log-transformed to stabilize variance.

To filter out outlier cells, we employed a robust adaptive thresholding strategy based on the median absolute deviation (MAD). Cells were flagged as general outliers if their log-transformed total counts, log-transformed gene counts, or the percentage of counts in the top 20 genes fell more than five MADs from the median of the respective metric. A separate, more stringent filter was applied for mitochondrial content, where cells were flagged as mitochondrial outliers if their percentage of mitochondrial counts was more than three MADs from the median or exceeded an absolute threshold of 8%.

Cells that were flagged as either a general outlier or a mitochondrial outlier were removed from the dataset. The effectiveness of this filtering was confirmed through visualization. The distribution of total counts in the filtered dataset was examined to ensure the removal of cells with extremely low or high library sizes. Additionally, violin and scatter plots were generated to visualize the distribution of mitochondrial gene percentages and to inspect the relationship between total counts and the number of detected genes, confirming that the remaining cells formed a more homogenous and high-quality population for subsequent analysis.","scanpy, numpy, seaborn, scipy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for numerical operations, data handling, and plotting
import numpy as np
import scanpy as sc
import seaborn as sns
from scipy.stats import median_abs_deviation

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
# Verbosity 0 supresses most of the Scanpy output messages.
sc.settings.verbosity = 0
# Set figure parameters for high-quality plots.
sc.settings.set_figure_params(dpi=80, facecolor=""white"", frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load the single-cell dataset from a 10x Genomics HDF5 file
adata = sc.read_10x_h5(filename=""filtered_feature_bc_matrix.h5"")

# Ensure that all gene names (variable names) are unique to prevent issues in downstream analysis
adata.var_names_make_unique()

#--------------------------------
# Quality Control (QC) and Filtering
#--------------------------------

# Identify different sets of genes for quality control metrics
# MT- genes are mitochondrial genes, which can indicate cell stress or damage if highly expressed.
adata.var[""mt""] = adata.var_names.str.startswith(""MT-"")
# RPS and RPL genes are ribosomal genes.
adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL""))
# HB genes are hemoglobin genes, which can indicate contamination from red blood cells.
adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]"")

# Calculate QC metrics for each cell.
# This computes metrics like total counts, number of genes, and percentages of mt, ribo, and hb genes.
sc.pp.calculate_qc_metrics(
    adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, percent_top=[20], log1p=True
)

# Define a function to identify outlier cells based on the Median Absolute Deviation (MAD).
# MAD is a robust measure of variability and is less sensitive to extreme outliers than standard deviation.
def is_outlier(adata, metric: str, nmads: int):
    """"""
    Identifies outliers for a given metric in the AnnData object.

    Args:
        adata: AnnData object containing the data.
        metric: The observation metric to check for outliers (e.g., 'log1p_total_counts').
        nmads: The number of median absolute deviations to use as the threshold.

    Returns:
        A boolean Series indicating whether each cell is an outlier.
    """"""
    M = adata.obs[metric]
    # An observation is an outlier if it is nmads MADs away from the median.
    outlier = (M < np.median(M) - nmads * median_abs_deviation(M)) | \
              (np.median(M) + nmads * median_abs_deviation(M) < M)
    return outlier

# Identify outlier cells based on general QC metrics.
# Here, we mark cells as outliers if they are outliers for total counts, number of genes, or
# the percentage of counts in the top 20 genes.
adata.obs[""outlier""] = (
    is_outlier(adata, ""log1p_total_counts"", 5) |
    is_outlier(adata, ""log1p_n_genes_by_counts"", 5) |
    is_outlier(adata, ""pct_counts_in_top_20_genes"", 5)
)

# Identify cells with high mitochondrial gene content.
# Cells are marked as outliers if their mitochondrial content is more than 3 MADs from the median
# or if it exceeds a hard threshold of 8%.
adata.obs[""mt_outlier""] = is_outlier(adata, ""pct_counts_mt"", 3) | (adata.obs[""pct_counts_mt""] > 8)

# Print the number of cells before filtering
print(f""Total number of cells: {adata.n_obs}"")

# Filter out the low-quality cells identified in the previous steps.
# A .copy() is used to create a new, clean AnnData object.
adata = adata[(~adata.obs.outlier) & (~adata.obs.mt_outlier)].copy()

# Print the number of cells remaining after filtering
print(f""Number of cells after filtering of low-quality cells: {adata.n_obs}"")


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the distribution of total counts per cell after filtering.
# This helps confirm that cells with extremely low or high counts have been removed.
sns.displot(adata.obs[""total_counts""], bins=100, kde=False)

# Create a violin plot to show the distribution of mitochondrial gene percentages.
# This plot helps to visually inspect the effectiveness of the mitochondrial content filtering.
sc.pl.violin(adata, ""pct_counts_mt"")

# Create a scatter plot to visualize the relationship between total counts,
# number of genes, and mitochondrial content.
# This plot is useful for identifying any remaining artifacts or relationships in the data.
sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt"")


#--------------------------------
# Data Output
#--------------------------------

# Save the filtered and processed AnnData object to a file.
# This file can be used for subsequent downstream analyses like normalization,
# clustering, and differential expression.
adata.write(""filtered_quality_control.h5ad"")
",https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html,F
scRNA-seq,Quality control,"The single-cell analysis was conducted in Python using the Scanpy library. An initial quality control step removed low-quality cells via an adaptive filtering strategy based on the median absolute deviation (MAD) of key metrics like gene counts and mitochondrial percentage. Subsequently, doublet detection was performed on the filtered data using the Scrublet algorithm on a per-sample basis. The pipeline's effectiveness was confirmed through extensive visualization, including a preliminary UMAP embedding generated specifically to assess the distribution of predicted doublets across the cellular landscape.","The single-cell analysis was conducted using the Scanpy library in Python. Data was loaded from a 10x Genomics filtered feature-barcode matrix HDF5 file, and gene names were systematically made unique to prevent conflicts in downstream steps.

A multi-step quality control process was implemented to remove low-quality cells and technical artifacts. First, cellular quality was assessed. Genes were annotated as mitochondrial (prefix ""MT-""), ribosomal (prefixes ""RPS-"", ""RPL-""), or hemoglobin (prefix ""HB-"") to serve as indicators of cell stress or contamination. Following annotation, standard quality metrics were computed for each cell, including total transcript counts, number of detected genes, and the percentage of counts from the annotated gene sets. These metrics were log-transformed to prepare for filtering. A robust filtering strategy based on the median absolute deviation (MAD) was applied. Cells were flagged as outliers if their log-transformed total counts, gene counts, or percentage of counts in the top 20 genes deviated by more than five MADs from the median. Additionally, cells were flagged for high mitochondrial content if their mitochondrial percentage exceeded three MADs from the median or surpassed an absolute threshold of 8%. Any cell flagged as an outlier by these criteria was removed from the dataset.

Next, we performed doublet detection on the filtered data to identify and remove remaining technical artifacts. The Scrublet algorithm was executed on a per-sample basis to calculate a doublet score for each cell, which estimates its likelihood of being a doublet based on simulating transcriptomic profiles from randomly paired cells.

The results of both the quality control filtering and doublet detection were thoroughly inspected through visualization. The distributions of total counts and mitochondrial content were visualized using histograms and violin plots to confirm the effectiveness of the cell filtering. The relationship between total counts and gene counts was examined using a scatter plot. To evaluate the doublet detection results, we plotted a histogram of the Scrublet-derived doublet scores. Furthermore, a preliminary Uniform Manifold Approximation and Projection (UMAP) embedding was computed for the express purpose of visualizing the doublet scores and binary doublet predictions across the two-dimensional representation of the data. This allowed for a visual assessment of whether predicted doublets were enriched between distinct cell clusters, as is often expected.","scanpy, numpy, seaborn, scipy, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for numerical operations, data handling, and plotting
import numpy as np
import scanpy as sc
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import median_abs_deviation

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
# Verbosity 0 supresses most of the Scanpy output messages.
sc.settings.verbosity = 0
# Set figure parameters for high-quality plots.
sc.settings.set_figure_params(dpi=80, facecolor=""white"", frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load the single-cell dataset from a 10x Genomics HDF5 file
adata = sc.read_10x_h5(filename=""filtered_feature_bc_matrix.h5"")

# Ensure that all gene names (variable names) are unique
adata.var_names_make_unique()

#--------------------------------
# Quality Control (QC) and Filtering
#--------------------------------

# Identify different sets of genes for quality control metrics
# MT- genes are mitochondrial. High percentage can indicate cell stress.
adata.var[""mt""] = adata.var_names.str.startswith(""MT-"")
# RPS and RPL genes are ribosomal.
adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL""))
# HB genes are hemoglobin. High percentage can indicate red blood cell contamination.
adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]"")

# Calculate QC metrics for each cell.
sc.pp.calculate_qc_metrics(
    adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, percent_top=[20], log1p=True
)

# Define a function to identify outlier cells based on Median Absolute Deviation (MAD)
def is_outlier(adata, metric: str, nmads: int):
    """"""
    Identifies outliers for a given metric in the AnnData object.

    Args:
        adata: AnnData object containing the data.
        metric: The observation metric to check for outliers (e.g., 'log1p_total_counts').
        nmads: The number of median absolute deviations to use as the threshold.

    Returns:
        A boolean Series indicating whether each cell is an outlier.
    """"""
    M = adata.obs[metric]
    mad = median_abs_deviation(M)
    median_val = np.median(M)
    # An observation is an outlier if it is nmads MADs away from the median.
    outlier = (M < median_val - nmads * mad) | (M > median_val + nmads * mad)
    return outlier

# Identify outlier cells based on general QC metrics
adata.obs[""outlier""] = (
    is_outlier(adata, ""log1p_total_counts"", 5) |
    is_outlier(adata, ""log1p_n_genes_by_counts"", 5) |
    is_outlier(adata, ""pct_counts_in_top_20_genes"", 5)
)

# Identify cells with high mitochondrial gene content
adata.obs[""mt_outlier""] = is_outlier(adata, ""pct_counts_mt"", 3) | (adata.obs[""pct_counts_mt""] > 8)

# Print the number of cells before filtering
print(f""Total number of cells before filtering: {adata.n_obs}"")

# Filter out the low-quality cells
adata = adata[(~adata.obs[""outlier""]) & (~adata.obs[""mt_outlier""])].copy()

# Print the number of cells remaining after filtering
print(f""Total number of cells after filtering: {adata.n_obs}"")

#--------------------------------
# Doublet Detection
#--------------------------------

# Ensure a 'sample' column exists for batch processing, as required by scrublet.
# If not present, create a dummy sample column.
if ""sample"" not in adata.obs:
    adata.obs[""sample""] = ""sample1""

# Run Scrublet for doublet detection. This simulates artificial doublets
# from the data and calculates a score for each real cell based on its
# similarity to the simulated doublets.
sc.pp.scrublet(adata, batch_key=""sample"")

# Compute a UMAP embedding for visualization purposes. This is necessary
# to visualize doublet scores on a 2D projection of the data.
# Note: This is a preliminary UMAP for visualization, not the final one for analysis.
sc.pp.neighbors(adata)
sc.tl.umap(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- QC Plots ---
# Visualize the distribution of total counts per cell after filtering.
sns.displot(adata.obs[""total_counts""], bins=100, kde=False)

# Create a violin plot to show the distribution of mitochondrial gene percentages.
sc.pl.violin(adata, ""pct_counts_mt"")

# Create a scatter plot to visualize total counts vs. number of genes.
sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt"")


# --- Doublet Plots ---
# Plot a histogram of the doublet scores to help determine a threshold for calling doublets.
plt.figure(figsize=(8, 4))
plt.hist(adata.obs[""doublet_score""], bins=50, color=""skyblue"", edgecolor=""black"")
plt.xlabel(""Doublet Score"")
plt.ylabel(""Frequency"")
plt.title(""Doublet Score Distribution"")
plt.show()


# Visualize the UMAP embedding colored by doublet score and predicted doublet status.
# This helps to see if predicted doublets form distinct clusters or are located
# between major cell type clusters.
sc.pl.umap(adata, color=[""doublet_score"", ""predicted_doublet""], wspace=0.5)

#--------------------------------
# Data Output
#--------------------------------

# Save the filtered and processed AnnData object. This file now includes
# doublet scores and predictions, ready for downstream analysis.
adata.write(""filtered_quality_control_with_doublet_detection.h5ad"")","https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html, Human added code",F
scRNA-seq,Quality control,"The single-cell analysis was performed in R using the Seurat package. An initial filtering step was applied during object creation to remove genes and cells with very low detection rates. The percentage of mitochondrial gene expression was calculated as a primary quality metric for each cell. A subsequent, stringent filtering step was implemented using fixed numerical cutoffs for both the number of detected genes and the mitochondrial percentage to isolate a high-quality cell population. The outcome of this process was confirmed by visualizing the distributions of these quality metrics using violin and scatter plots.","The single-cell RNA sequencing analysis was conducted in the R programming language using the Seurat package. Raw gene expression data was loaded from the 10x Genomics `filtered_feature_bc_matrices` output directory.

An initial Seurat object was created from the raw count matrix. During this initialization step, preliminary filtering was performed to remove low-quality data points. Genes that were detected in fewer than three cells were excluded from the analysis, and cells that expressed fewer than 200 unique genes were also removed.

For quality control, the percentage of reads mapping to the mitochondrial genome was calculated for each cell. This was achieved by identifying all genes with the prefix ""MT-"", a common convention for mitochondrial genes in the human reference genome, and computing their proportional expression relative to the total counts per cell.

A subsequent filtering step was applied to retain only high-quality cells. Cells were selected for downstream analysis if they met the following criteria: the number of detected genes (`nFeature_RNA`) was between 200 and 2,500, and the percentage of mitochondrial reads (`percent.mt`) was less than 5%. This step removes potential empty droplets, low-quality cells, and likely doublets or multiplets, as well as cells with high mitochondrial content indicative of cellular stress or apoptosis.

To verify the results of the quality control process, the distributions of key metrics in the filtered dataset were visualized. Violin plots were generated to show the number of genes per cell, the total UMI counts per cell, and the mitochondrial read percentage. Additionally, scatter plots were created to examine the relationships between the total UMI counts and both the mitochondrial percentage and the number of detected genes, confirming the removal of outlier cell populations.","Seurat, dplyr, ggplot2",R,"#----------------
# Package Load
#----------------
library(Seurat)   # Core package for single-cell analysis
library(dplyr)     # For data manipulation and piping
library(ggplot2)   # For advanced plotting customization (optional)

#----------------
# Data Input & Requirements
#----------------
# Requirements: 
#   - 10X Genomics data in filtered_gene_bc_matrices format
#   - Directory path must point to uncompressed matrix files (matrix.mtx, genes.tsv, barcodes.tsv)
#   - Mitochondrial genes prefixed with ""MT-"" (human genome hg19 convention)

data_dir <- ""path/to/filtered_gene_bc_matrices/hg19/""  # SET YOUR DATA PATH HERE
pbmc.data <- Read10X(data.dir = data_dir)  # Load raw count matrix

#----------------
# Major Analysis Tasks
#----------------
# Initialize Seurat object with quality filters:
#   - min.cells=3: Genes must appear in ≥3 cells
#   - min.features=200: Cells must express ≥200 genes
pbmc <- CreateSeuratObject(
  counts = pbmc.data, 
  project = ""pbmc3k"", 
  min.cells = 3, 
  min.features = 200
)

# Calculate mitochondrial QC metrics:
#   - Identifies genes starting with ""MT-"" prefix
#   - Computes percentage of mitochondrial reads per cell
pbmc[[""percent.mt""]] <- PercentageFeatureSet(pbmc, pattern = ""^MT-"")

# Apply QC filtering thresholds:
#   - Retain cells with 200-2500 detected genes (nFeature_RNA)
#   - Exclude cells with >5% mitochondrial reads (percent.mt)
pbmc <- subset(
  pbmc, 
  subset = nFeature_RNA > 200 & 
           nFeature_RNA < 2500 & 
           percent.mt < 5
)

#----------------
# Plotting
#----------------
# Violin plots for key QC metrics:
#   - nFeature_RNA: Genes per cell
#   - nCount_RNA: UMI counts per cell
#   - percent.mt: Mitochondrial percentage
VlnPlot(
  pbmc, 
  features = c(""nFeature_RNA"", ""nCount_RNA"", ""percent.mt""), 
  ncol = 3
)

# Scatter plots for QC relationships:
#   - Plot 1: UMI counts vs. mitochondrial percentage
#   - Plot 2: UMI counts vs. detected genes
plot1 <- FeatureScatter(pbmc, feature1 = ""nCount_RNA"", feature2 = ""percent.mt"")
plot2 <- FeatureScatter(pbmc, feature1 = ""nCount_RNA"", feature2 = ""nFeature_RNA"")
plot1 + plot2  # Combine plots using patchwork

#----------------
# Data Output
#----------------
# Save QC-processed object in RDS format:
#   - Preserves all data, metadata, and calculations
#   - Compatible with Seurat::LoadRDS() for reloading
saveRDS(pbmc, file = ""pbmc_QC_filtered.rds"")",https://satijalab.org/seurat/articles/pbmc3k_tutorial,F
scRNA-seq,Normalization,The single-cell analysis was performed in Python using the Scanpy library. The preprocessing workflow involved normalizing the raw gene expression data while preserving the original counts in a separate data layer. Library size correction was achieved by scaling the total transcript counts per cell to a target value. This was followed by a logarithmic transformation to stabilize variance across genes with varying expression levels. The effects of this normalization were then verified by visualizing the distributions of total counts and detected genes using violin and scatter plots.,"The single-cell RNA sequencing analysis was performed in Python using the Scanpy library. The analysis began with a standard dataset of 3,000 peripheral blood mononuclear cells (PBMCs).

Prior to downstream analysis, the raw gene expression counts were preprocessed using a standard normalization workflow. First, the original raw count data was preserved by storing it in a separate layer within the data object. The data was then normalized to account for differences in cellular sequencing depth. This was accomplished by scaling the total number of transcript counts in each cell to a constant value of 10,000. Following this library size correction, a natural log-plus-one transformation was applied to the data. This step stabilizes the variance across genes with different expression levels, making the data more amenable to the assumptions of subsequent linear modeling steps such as principal component analysis.

To verify the effects of the normalization procedure, the distributions of key quality control metrics were visualized. Violin plots were generated to inspect the distributions of the total normalized counts and the number of detected genes per cell. Additionally, a scatter plot was created to examine the relationship between these two metrics after normalization, allowing for a final visual check for any remaining technical biases in the processed data.","scanpy, anndata",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and data handling
import scanpy as sc
import anndata as ad

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting to ensure consistent and clear visuals
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load an example peripheral blood mononuclear cell (PBMC) dataset
# This is a common benchmark dataset in single-cell analysis.
adata = sc.datasets.pbmc3k()

#--------------------------------
# Data Normalization and Preprocessing
#--------------------------------

# Save the raw, unnormalized count data into a separate layer.
# This is a good practice as it preserves the original data for future reference
# or for algorithms that require raw counts.
adata.layers[""counts""] = adata.X.copy()

# Normalize the data for sequencing depth.
# Each cell's total counts are scaled to a target sum (e.g., 10,000).
# This removes technical differences in library size across cells.
sc.pp.normalize_total(adata, target_sum=1e4)

# Apply a log(1+x) transformation to the normalized data.
# This helps to stabilize the variance and makes the data more suitable for
# linear models used in downstream analysis (like PCA or differential expression).
sc.pp.log1p(adata)

# For verification, print a small slice of the transformed data matrix.
print(""Normalized data sample (first 5 cells, first 5 genes):"")
print(adata.X[:5, :5])


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Create violin plots to visualize the distributions of total counts and
# the number of detected genes per cell after normalization. This helps to
# confirm that the normalization has been applied correctly.
sc.pl.violin(
    adata,
    keys=[""total_counts"", ""n_genes_by_counts""],
    jitter=0.4,
    multi_panel=True,
    title=""QC Metrics after Normalization""
)

# Create a scatter plot to examine the relationship between the total counts and
# the number of genes detected in each cell. This can help identify if there
# are any remaining technical artifacts or biases in the data.
sc.pl.scatter(
    adata,
    x=""total_counts"",
    y=""n_genes_by_counts"",
    title=""Total Counts vs Number of Genes after Normalization""
)


#--------------------------------
# Data Output
#--------------------------------

# Save the normalized and processed AnnData object to a file.
# This file can be loaded for subsequent analysis steps, such as
# identifying highly variable genes, dimensionality reduction, and clustering.
adata.write(""pbmc3k_normalized.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,F
scRNA-seq,Normalization,"The single-cell analysis was performed in Python using the Scanpy framework. The preprocessing workflow involved preserving the original raw counts before normalizing the data. This included a library size correction step, where each cell's expression profile was scaled to a uniform total count, followed by a logarithmic transformation to stabilize variance. The successful application of these preprocessing steps was then confirmed by visually inspecting the distributions of total counts and the number of detected genes using violin and scatter plots.","The single-cell analysis was performed using the Scanpy framework in Python. A standard dataset of 3,000 peripheral blood mononuclear cells (PBMCs) was loaded for the analysis.

A standard preprocessing workflow was applied to the raw gene expression counts to prepare the data for downstream analysis. First, to ensure the original data was preserved, the raw count matrix was stored in a separate layer within the AnnData object. The primary data matrix was then subjected to a two-step normalization process. First, to correct for technical variations in sequencing depth between cells, each cell's expression profile was scaled to a uniform total count of 10,000. This is a common procedure known as library size normalization. Second, a natural logarithm transformation (`log(1+x)`) was applied to the depth-normalized data. This variance-stabilizing transformation mitigates the strong dependence of a gene's variance on its mean expression level, making the data more suitable for linear dimensionality reduction and differential expression testing.

To confirm the successful application of these preprocessing steps, the resulting data was visually inspected. Violin plots were generated to assess the distributions of total counts and the number of detected genes per cell after normalization. Additionally, a scatter plot was used to examine the relationship between these two metrics across all cells, serving as a final quality check to ensure no significant technical artifacts remained.","numpy, scanpy, seaborn, matplotlib, scipy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and data handling
import scanpy as sc
import anndata as ad

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting
# This ensures all plots have a consistent appearance.
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load an example peripheral blood mononuclear cell (PBMC) dataset
# This is a common benchmark dataset in single-cell analysis.
adata = sc.datasets.pbmc3k()

#--------------------------------
# Data Normalization and Preprocessing
#--------------------------------

# Save the raw, unnormalized count data into a separate layer.
# This is a good practice as it preserves the original data for future reference
# or for algorithms that require raw counts.
adata.layers[""counts""] = adata.X.copy()

# Normalize the data for sequencing depth.
# Each cell's total counts are scaled to a target sum (e.g., 10,000).
# This removes technical differences in library size across cells.
sc.pp.normalize_total(adata, target_sum=1e4)

# Apply a log(1+x) transformation to the normalized data.
# This helps to stabilize the variance and makes the data more suitable for
# linear models used in downstream analysis (like PCA or differential expression).
sc.pp.log1p(adata)

# For verification, print a small slice of the transformed data matrix.
print(""Normalized data sample (first 5 cells, first 5 genes):"")
print(adata.X[:5, :5])


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Create violin plots to visualize the distributions of total counts and
# the number of detected genes per cell after normalization. This helps to
# confirm that the normalization has been applied correctly.
sc.pl.violin(
    adata,
    keys=[""total_counts"", ""n_genes_by_counts""],
    jitter=0.4,
    multi_panel=True,
    title=""QC Metrics after Normalization""
)

# Create a scatter plot to examine the relationship between the total counts and
# the number of genes detected in each cell. This can help identify if there
# are any remaining technical artifacts or biases in the data.
sc.pl.scatter(
    adata,
    x=""total_counts"",
    y=""n_genes_by_counts"",
    title=""Total Counts vs Number of Genes after Normalization""
)

#--------------------------------
# Data Output
#--------------------------------

# Save the normalized and processed AnnData object to a file.
# This file can be loaded for subsequent analysis steps, such as
# identifying highly variable genes, dimensionality reduction, and clustering.
adata.write(""pbmc3k_normalized.h5ad"")",https://www.sc-best-practices.org/preprocessing_visualization/normalization.html,F
scRNA-seq,Normalization,"The single-cell analysis was conducted in R using the Seurat package. The workflow began with an initial filtering of the data to remove cells and genes with low detection rates. Subsequently, a standard normalization process was applied, which involved correcting for cellular library size differences and applying a logarithmic transformation to the count data. This step makes expression values more comparable across cells. The successful application of the normalization was then confirmed by visually inspecting the distribution of normalized counts per cell with a violin plot.","The single-cell analysis was performed in R using the Seurat package. The workflow began by loading raw gene expression counts from a 10x Genomics `filtered_gene_bc_matrices` directory. An initial Seurat object was created from this data, during which a preliminary quality control filter was applied. This step retained only genes that were expressed in at least three cells and cells that expressed a minimum of 200 unique genes.

To account for technical variations in library size across cells, the raw counts were normalized using Seurat’s `LogNormalize` method. This standard procedure first normalizes the feature counts for each cell by its total counts, then multiplies the result by a scale factor of 10,000, and finally applies a natural log transformation. This process stabilizes the variance across the dataset and makes gene expression values more comparable between cells, which is a critical prerequisite for downstream analyses like dimensionality reduction and clustering. The distribution of the resulting normalized RNA counts per cell was visually inspected using a violin plot to confirm the successful application of the normalization procedure.","Seurat, dplyr, ggplot2",R,"#--------------------------------
# Package Load
#--------------------------------

# Import necessary libraries for single-cell analysis and data manipulation
library(Seurat)   # Main package for single-cell analysis
library(dplyr)    # For data manipulation functions
library(ggplot2)  # For advanced plotting capabilities

#--------------------------------
# Data Input
#--------------------------------

# Specify the directory containing the 10X Genomics count matrices.
# NOTE: You must replace this with the actual path to your data.
data_dir <- ""path/to/filtered_gene_bc_matrices/hg19/""

# Read the 10X Genomics data into a raw count matrix.
raw_data <- Read10X(data.dir = data_dir)

# Create a Seurat object from the raw count matrix.
# Basic filtering is applied at this stage:
# - min.cells = 3: Keep genes that are expressed in at least 3 cells.
# - min.features = 200: Keep cells that have at least 200 detected genes.
seurat_obj <- CreateSeuratObject(
  counts = raw_data,
  project = ""DataNormalization"",
  min.cells = 3,
  min.features = 200
)

#--------------------------------
# Data Normalization
#--------------------------------

# Perform LogNormalization on the data. This is a standard method that:
# 1. Normalizes gene counts for each cell by the total counts for that cell.
# 2. Multiplies the result by a scaling factor (default is 10,000).
# 3. Applies a natural-log transformation (log1p).
seurat_obj <- NormalizeData(
  seurat_obj,
  normalization.method = ""LogNormalize"",
  scale.factor = 10000
)

#--------------------------------
# Plotting / Visualization
#--------------------------------

# Generate a violin plot to visualize the distribution of RNA counts per cell.
# This plot helps to assess the effect of normalization on the data distribution
# across all cells. 'nCount_RNA' is automatically calculated by Seurat.
VlnPlot(seurat_obj, features = ""nCount_RNA"", pt.size = 0.1) +
  ggtitle(""Distribution of Normalized RNA Counts"")

#--------------------------------
# Data Output
#--------------------------------

# Save the normalized Seurat object to an RDS file.
# This file can be loaded later for downstream analysis, such as scaling,
# dimensionality reduction, and clustering.
saveRDS(seurat_obj, file = ""Normalized_Data_Object.rds"")",https://satijalab.org/seurat/articles/pbmc3k_tutorial#identification-of-highly-variable-features-feature-selection,F
scRNA-seq,Feature selection,"The single-cell analysis was conducted in Python with Scanpy, beginning with standard library size normalization and logarithmic transformation of the count data. For feature selection, highly variable genes were identified by modeling the relationship between mean expression and dispersion, using a method analogous to Seurat's implementation. A subset of the most variable genes was selected for downstream analysis. This selection was visually confirmed by plotting gene dispersion against mean expression, highlighting the identified feature set.","The single-cell analysis was conducted in Python using the Scanpy library. The workflow began with a standard 3,000 PBMC dataset. The raw count matrix was first preserved, and the primary data was then subjected to a standard preprocessing pipeline. This involved normalizing the data for sequencing depth by scaling each cell's total counts to a target of 10,000, followed by a variance-stabilizing log-plus-one transformation.

Following normalization, a feature selection step was performed to identify the most biologically informative genes for downstream analysis. We identified highly variable genes (HVGs) using the method implemented in the Seurat R package, as replicated in Scanpy's `highly_variable_genes` function with the `flavor` parameter set to `'seurat'`. This method models the relationship between the mean expression and variance of each gene, identifying those with higher-than-expected variability. The top 2,000 genes with the highest variability were selected for subsequent dimensionality reduction and clustering. The selection was visually confirmed by generating a plot that displayed the mean expression versus the dispersion for all genes, highlighting the selected HVGs to ensure they were appropriately identified.","scanpy, anndata",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and data handling
import scanpy as sc
import anndata as ad

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting
# This ensures all plots have a consistent, clean appearance.
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load an example peripheral blood mononuclear cell (PBMC) dataset
adata = sc.datasets.pbmc3k()

# It's good practice to keep a copy of the raw data.
# The .raw attribute is the conventional place to store it in Scanpy.
adata.raw = adata.copy()


#--------------------------------
# Data Normalization
#--------------------------------

# Normalize the data for sequencing depth by scaling total counts per cell to a target sum.
sc.pp.normalize_total(adata, target_sum=1e4)

# Apply a log(1+x) transformation to the normalized data.
# This stabilizes variance and makes the data more comparable across genes with different expression levels.
sc.pp.log1p(adata)


#--------------------------------
# Major Analysis Task: Feature Selection
#--------------------------------

# Identify highly variable genes (HVGs). These are genes that show high cell-to-cell
# variation and are more likely to be biologically significant.
# The 'seurat' flavor is a commonly used method for this.
# `n_top_genes` specifies how many of the most variable genes to select.
sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat')

# Print the number of genes identified as highly variable for confirmation.
print(f""Number of highly variable genes found: {sum(adata.var.highly_variable)}"")


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Generate a scatter plot to visualize the highly variable genes.
# The plot shows each gene's mean expression versus its dispersion (a measure of variability).
# The selected HVGs are highlighted, which is useful for a visual quality check of the selection process.
sc.pl.highly_variable_genes(adata)


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object, which now contains the normalized data and the list of
# highly variable genes. This object is ready for downstream steps like scaling and
# dimensionality reduction (PCA).
adata.write(""pbmc3k_hvg_selected.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,F
scRNA-seq,Feature selection,"The single-cell analysis was conducted in R using the Seurat package. The workflow began with initial cell and gene filtering, followed by standard log-normalization of the count data to account for library size differences. For feature selection, highly variable genes were identified using a variance-stabilizing transformation (VST) method. A subset of the most variable genes was then selected for downstream analyses. The results of this process were visually inspected on a variable feature plot, which displays gene variance as a function of average expression.","The single-cell analysis was performed using the Seurat package in R. Raw count data was loaded from a 10x Genomics output directory, and an initial Seurat object was created. During this process, a basic quality filter was applied, retaining only genes expressed in at least three cells and cells with at least 200 detected features.

The raw data was then normalized using the `LogNormalize` method. This standard procedure corrects for differences in sequencing depth by normalizing each cell's counts to a total of 10,000 and then applying a log transformation. This step ensures that expression values are comparable across cells.

Following normalization, we performed feature selection to identify a subset of genes with high cell-to-cell variation, which are most likely to highlight biological differences in the data. This was accomplished using the `FindVariableFeatures` function with the `selection.method` parameter set to `vst` (variance-stabilizing transformation). The top 2,000 most variable genes were selected for downstream analysis. To visually inspect the results of this step, a `VariableFeaturePlot` was generated, which displays the relationship between the average expression and variance for each gene. The top 10 most variable genes were labeled on this plot for identification.","Seurat, dplyr, ggplot2",R,"#--------------------------------
# Package Load
#--------------------------------

# Import necessary libraries for single-cell analysis and data manipulation
library(Seurat)   # Main package for single-cell analysis
library(dplyr)    # For data manipulation functions
library(ggplot2)  # For advanced plotting capabilities

#--------------------------------
# Data Input
#--------------------------------

# Specify the directory containing the 10X Genomics count matrices.
# NOTE: You must replace this with the actual path to your data.
data_dir <- ""path/to/filtered_gene_bc_matrices/hg19/""

# Read the 10X Genomics data into a raw count matrix.
raw_data <- Read10X(data.dir = data_dir)

# Create a Seurat object from the raw count matrix with basic initial filtering.
seurat_obj <- CreateSeuratObject(
  counts = raw_data,
  project = ""FeatureSelectionDemo"",
  min.cells = 3,      # Keep genes expressed in at least 3 cells
  min.features = 200  # Keep cells with at least 200 detected genes
)

#--------------------------------
# Preprocessing and Normalization
#--------------------------------

# OPTIONAL QC: Calculate the percentage of mitochondrial genes for each cell.
# This is a common QC metric; high percentages can indicate cell stress.
seurat_obj[[""percent.mt""]] <- PercentageFeatureSet(seurat_obj, pattern = ""^MT-"")

# Normalize the data using the ""LogNormalize"" method.
# This scales the data by library size and applies a log transformation.
seurat_obj <- NormalizeData(
  seurat_obj,
  normalization.method = ""LogNormalize"",
  scale.factor = 10000
)

#--------------------------------
# Major Analysis Task: Feature Selection
#--------------------------------

# Identify highly variable features (genes) using the variance-stabilizing transformation (vst) method.
# These genes show high cell-to-cell variation and are used for downstream analyses like PCA.
seurat_obj <- FindVariableFeatures(
  seurat_obj,
  selection.method = ""vst"",
  nfeatures = 2000 # Select the top 2000 most variable genes
)

#--------------------------------
# Plotting / Visualization
#--------------------------------

# Identify the top 10 most highly variable genes for labeling purposes.
top10 <- head(VariableFeatures(seurat_obj), 10)

# Create a variable feature plot to visualize gene variability.
# The plot shows the relationship between average expression and a measure of dispersion for each gene.
variable_feature_plot <- VariableFeaturePlot(seurat_obj)

# Add labels for the top 10 HVGs to the plot for better interpretation.
labeled_plot <- LabelPoints(plot = variable_feature_plot, points = top10, repel = TRUE)

# Print the final plot to the console.
print(labeled_plot)

#--------------------------------
# Data Output
#--------------------------------

# Save the Seurat object, which now includes the identified variable features.
# This object can be used for the next steps in the analysis, such as scaling and clustering.
saveRDS(seurat_obj, file = ""FeatureSelection_SeuratObject.rds"")",https://satijalab.org/seurat/articles/pbmc3k_tutorial#identification-of-highly-variable-features-feature-selection,F
scRNA-seq,Dimensionality reduction,"The analysis was conducted in Python with Scanpy, starting with data normalization and selection of highly variable genes. These selected features were scaled, and Principal Component Analysis was performed for linear dimensionality reduction. Subsequently, a neighborhood graph was constructed from the principal components and used as input for non-linear dimensionality reduction with UMAP. The resulting low-dimensional embeddings were validated by inspecting the PCA variance and by visualizing the expression of known marker genes to confirm the meaningful separation of distinct cell populations.","The single-cell analysis was performed using the Scanpy library in Python, starting with a standard 3,000 PBMC dataset.

A standard preprocessing pipeline was applied to the data. First, to correct for differences in sequencing depth, raw counts were normalized by scaling each cell's total library size to 10,000, followed by a variance-stabilizing natural log-plus-one transformation. Feature selection was then performed to identify the 2,000 most highly variable genes (HVGs) using a method equivalent to that implemented in the Seurat R package.

Dimensionality reduction was carried out in two stages. First, the data was subset to include only the identified HVGs, and these features were scaled to have zero mean and unit variance. Principal Component Analysis (PCA) was then performed on this scaled data matrix using the ARPACK solver. Following this linear reduction, a neighborhood graph was constructed in the PCA space, considering the top 40 principal components and the 10 nearest neighbors for each cell. This graph was used as input for Uniform Manifold Approximation and Projection (UMAP) to generate a two-dimensional embedding for visualization.

The results of the dimensionality reduction were visually inspected. An ""elbow"" plot of the PCA variance ratio was generated to confirm that the selected principal components captured a significant portion of the data's variance. Both the PCA and UMAP embeddings were visualized as scatter plots, with cells colored by the expression of known marker genes, such as `CST3` and `NKG7`, to confirm that the resulting data structure effectively separated distinct biological cell populations.","scanpy, anndata, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import scanpy as sc
import anndata as ad
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting to ensure consistent and clear visuals
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load the built-in PBMC 3k dataset from Scanpy as an example
adata = sc.datasets.pbmc3k()

#--------------------------------
# Preprocessing & Dimensionality Reduction
#--------------------------------

# --- Normalization and Feature Selection ---
# Normalize total counts per cell to account for differences in sequencing depth
sc.pp.normalize_total(adata, target_sum=1e4)

# Apply a log(1+x) transformation to stabilize the variance
sc.pp.log1p(adata)

# Identify highly variable genes (HVGs), which are most informative for downstream analysis
sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat')

# --- Scaling and PCA ---
# Subset the AnnData object to keep only the highly variable genes
adata = adata[:, adata.var.highly_variable]

# Scale the data so that each gene has zero mean and unit variance.
# This is crucial for PCA and other algorithms sensitive to feature scales.
sc.pp.scale(adata)

# Perform Principal Component Analysis (PCA) on the scaled data.
# This reduces the dimensionality of the data while retaining most of the variation.
sc.tl.pca(adata, svd_solver='arpack')

# --- UMAP Embedding ---
# Compute the neighborhood graph of cells. This is based on the PCA representation
# and is a prerequisite for UMAP.
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)

# Run Uniform Manifold Approximation and Projection (UMAP) to generate a
# non-linear 2D embedding for visualization.
sc.tl.umap(adata)

#--------------------------------
# Plotting / Visualization
#--------------------------------

# Plot the PCA variance ratio to see how much variance is captured by each principal component.
# The log scale helps visualize the ""elbow"" where the explained variance plateaus.
sc.pl.pca_variance_ratio(adata, log=True)

# Create a scatter plot of the first two principal components.
# Cells can be colored by marker gene expression to identify potential cell types.
sc.pl.pca(adata, color=['CST3', 'NKG7'])

# Visualize the UMAP embedding.
# Coloring by marker genes helps to see how cell clusters are separated in the UMAP space.
sc.pl.umap(adata, color='CST3')

#--------------------------------
# Data Output
#--------------------------------

# Save the processed AnnData object, which now contains PCA and UMAP coordinates.
# This file is ready for downstream clustering and cell type annotation.
adata.write(""pbmc3k_pca_umap.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,F
scRNA-seq,Dimensionality reduction,"The analysis was conducted in Python using Scanpy on a pre-processed data object. The dimensionality reduction workflow began with Principal Component Analysis (PCA) on highly variable genes. A neighborhood graph was then constructed from the principal components, which served as the basis for a Uniform Manifold Approximation and Projection (UMAP) embedding. Concurrently, a t-SNE embedding was generated from the PCA representation. The resulting PCA, t-SNE, and UMAP embeddings were visualized and colored by total read counts to assess for the influence of technical variables.","The single-cell analysis was performed in Python using the Scanpy library. The workflow began with a pre-processed data object that had already undergone quality control, normalization, and feature selection. The analysis was explicitly set to proceed using the log-normalized expression values stored within this object.

To reduce the high dimensionality of the gene expression data, a multi-step dimensionality reduction process was performed. First, **Principal Component Analysis (PCA)** was run on the subset of previously identified highly variable genes using the ARPACK solver. This step identified the principal components that capture the greatest sources of variation in the data.

Following PCA, a **neighborhood graph** was constructed based on the first 40 principal components, connecting each cell to its 15 nearest neighbors. This graph represents the manifold structure of the data and serves as the foundation for non-linear dimensionality reduction techniques.

Finally, two separate two-dimensional embeddings were generated for visualization purposes. **t-distributed Stochastic Neighbor Embedding (t-SNE)** was computed directly from the PCA representation. Concurrently, **Uniform Manifold Approximation and Projection (UMAP)** was calculated from the pre-computed neighborhood graph. Both methods create low-dimensional representations that aim to preserve the local and global structure of the data, respectively. To ensure that the resulting embeddings were not driven by technical artifacts, the PCA, t-SNE, and UMAP plots were all visualized with cells colored by their total read counts.","scanpy, seaborn, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import scanpy as sc
import seaborn as sns
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 0
sc.settings.set_figure_params(dpi=80, facecolor=""white"", frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load the preprocessed AnnData object. This file should have undergone
# prior QC, normalization, and feature selection.
# A backup URL is provided in case the local file is not found.
adata = sc.read(
    ""s4d8_feature_selection.h5ad"",
    backup_url=""https://figshare.com/ndownloader/files/40016014""
)

#--------------------------------
# Major Analysis Tasks: Dimensionality Reduction
#--------------------------------

# Ensure the analysis uses the log-normalized data stored in a specific layer.
# This step makes the choice of data layer explicit.
adata.X = adata.layers[""log1p_norm""]

# --- Principal Component Analysis (PCA) ---
# Perform PCA on the highly variable genes to capture the main axes of variation.
sc.pp.pca(adata, svd_solver=""arpack"", use_highly_variable=True)

# --- t-SNE & UMAP Prerequisite: Neighborhood Graph ---
# Compute the neighborhood graph, which is required for both t-SNE and UMAP.
# This is calculated on the PCA representation of the data.
sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)

# --- t-distributed Stochastic Neighbor Embedding (t-SNE) ---
# Run t-SNE to generate a 2D embedding for visualization.
# It uses the PCA representation (`use_rep=""X_pca""`) as input.
sc.tl.tsne(adata, use_rep=""X_pca"")

# --- Uniform Manifold Approximation and Projection (UMAP) ---
# Compute the UMAP embedding based on the neighborhood graph.
sc.tl.umap(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the PCA result, coloring cells by total counts to check for technical artifacts.
sc.pl.pca_scatter(adata, color=""total_counts"", title=""PCA: First 2 Components"")

# Visualize the t-SNE embedding, also colored by total counts.
sc.pl.tsne(adata, color=""total_counts"", title=""t-SNE Embedding"")

# Visualize the UMAP embedding, colored by total counts.
sc.pl.umap(adata, color=""total_counts"", title=""UMAP Embedding"")


#--------------------------------
# Data Output
#--------------------------------

# Save the final AnnData object, which now contains the results of PCA, t-SNE, and UMAP.
# This object is ready for clustering and cell type annotation.
adata.write(""s4d8_dimensionality_reduction.h5ad"")
",https://www.sc-best-practices.org/preprocessing_visualization/dimensionality_reduction.html,F
scRNA-seq,Dimensionality reduction,"The analysis was performed in R with Seurat, involving initial filtering, log-normalization, and feature selection using a variance-stabilizing transformation to identify highly variable genes. The data was then scaled, and Principal Component Analysis was performed on the selected features. An elbow plot was used to determine the significant principal components, which were subsequently used to construct a nearest-neighbor graph. Finally, a UMAP embedding was generated from this graph for two-dimensional visualization of the cell populations.","The single-cell analysis was performed in R using the Seurat package. Raw gene expression data was loaded from a 10x Genomics `filtered_feature_bc_matrices` directory, and an initial Seurat object was created with preliminary filters to retain genes expressed in three or more cells and cells with at least 200 detected genes.

The data was then processed through a standard Seurat workflow. First, counts were normalized using the `LogNormalize` method, which corrects for library size differences by scaling each cell’s total expression to 10,000, followed by a natural-log transformation. Next, feature selection was performed to identify the top 2,000 most highly variable genes (HVGs) using the variance-stabilizing transformation (`vst`) method. The expression levels of all genes were then scaled to have a mean of zero and a variance of one, ensuring that highly-expressed genes do not dominate downstream analyses.

Dimensionality reduction was performed on the preprocessed data. Principal Component Analysis (PCA) was run on the scaled expression values of the previously identified HVGs. To determine the optimal number of principal components (PCs) to use for downstream analysis, an elbow plot was generated to visualize the variance explained by the top 50 PCs. Based on an evaluation of this plot, the first 10 PCs were selected to capture the most significant biological variation while minimizing technical noise. These 10 PCs were then used to construct a nearest-neighbor graph, which served as the input for Uniform Manifold Approximation and Projection (UMAP) to generate a two-dimensional embedding for cell population visualization. The results were inspected by plotting the first two principal components and the final UMAP embedding.","Seurat, ggplot2, patchwork",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for single-cell analysis and plotting
library(Seurat)      # Main package for single-cell workflows
library(ggplot2)     # For creating custom, publication-quality plots
library(patchwork)   # For easily combining multiple ggplot2 plots

#--------------------------------
# Data Input
#--------------------------------

# Specify the directory containing the 10X Genomics count matrices.
# NOTE: You must replace this with the actual path to your data.
data_dir <- ""path/to/filtered_gene_bc_matrices/hg19/""

# Read the 10X Genomics data into a raw count matrix.
raw_data <- Read10X(data.dir = data_dir)

# Create a Seurat object from the raw count matrix with basic initial filtering.
seurat_obj <- CreateSeuratObject(
  counts = raw_data,
  project = ""DimRed_DimDetermination"",
  min.cells = 3,
  min.features = 200
)

#--------------------------------
# Preprocessing and Normalization
#--------------------------------

# 1. Normalize the data using the ""LogNormalize"" method.
seurat_obj <- NormalizeData(
  seurat_obj,
  normalization.method = ""LogNormalize"",
  scale.factor = 10000
)

# 2. Identify highly variable features (HVGs).
seurat_obj <- FindVariableFeatures(
  seurat_obj,
  selection.method = ""vst"",
  nfeatures = 2000
)

# 3. Scale the data. This shifts the expression of each gene so that the mean
# expression across cells is 0 and the variance is 1.
all_genes <- rownames(seurat_obj)
seurat_obj <- ScaleData(seurat_obj, features = all_genes)


#--------------------------------
# Major Analysis Tasks: Dimensionality Reduction
#--------------------------------

# --- Principal Component Analysis (PCA) ---
# Run PCA on the scaled data, using only the highly variable features.
seurat_obj <- RunPCA(seurat_obj, features = VariableFeatures(seurat_obj))

# --- UMAP Embedding ---
# Note: Choosing the number of PCs is a critical step. The Elbow Plot in the
# visualization section helps with this decision. Here, we use 10 as an example.
# First, compute the neighborhood graph based on the selected PCs.
seurat_obj <- FindNeighbors(seurat_obj, dims = 1:10)

# Then, run UMAP to get a non-linear embedding for visualization.
seurat_obj <- RunUMAP(seurat_obj, dims = 1:10)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- PCA Plot ---
# Visualize the first two principal components. Coloring by 'nCount_RNA' can help
# reveal if technical factors (like library size) are driving the main sources of variation.
pca_plot <- DimPlot(seurat_obj, reduction = ""pca"") +
  ggtitle(""PCA: PC1 vs PC2"")

# --- Elbow Plot for Dimensionality Determination ---
# An Elbow Plot helps to determine the number of significant PCs to use for downstream
# analysis. The ""elbow"" of the plot represents the point of diminishing returns,
# where adding more PCs explains little additional variance.
elbow_plot <- ElbowPlot(seurat_obj, ndims = 50) +
  ggtitle(""Elbow Plot: Variance Explained vs. PC"")

# --- UMAP Plot ---
# Visualize the UMAP embedding, which is often used for identifying cell clusters.
umap_plot <- DimPlot(seurat_obj, reduction = ""umap"") +
  ggtitle(""UMAP Embedding (Using PCs 1-10)"")

# --- Combined Plot ---
# Use patchwork to arrange the plots for a comprehensive overview.
(pca_plot | elbow_plot) / umap_plot


#--------------------------------
# Data Output
#--------------------------------

# Save the processed Seurat object. It now contains the scaled data,
# PCA results, and UMAP embedding, ready for clustering and annotation.
saveRDS(seurat_obj, file = ""DimRed_DimDetermined_SeuratObject.rds"")
",https://satijalab.org/seurat/articles/pbmc3k_tutorial#identification-of-highly-variable-features-feature-selection,F
scRNA-seq,Clustering,"The analysis was conducted in Python with Scanpy, starting with data normalization, selection of highly variable genes, and feature scaling. Dimensionality reduction was performed via Principal Component Analysis, followed by the construction of a k-nearest neighbor graph in the PCA space. This graph was used both to generate a UMAP embedding for visualization and as input for graph-based clustering using the Leiden algorithm to partition cells into distinct communities. The resulting cluster assignments were then visualized on the UMAP embedding to assess their separation.","The single-cell analysis was performed using the Scanpy library in Python. A standard dataset of 3,000 peripheral blood mononuclear cells (PBMCs) was used as input.

A standard preprocessing pipeline was applied to the raw count data. First, counts were normalized for sequencing depth by scaling the total counts per cell to a target sum of 10,000, followed by a variance-stabilizing log-plus-one transformation. Feature selection was then performed to identify the 2,000 most highly variable genes using a method analogous to that in the Seurat R package. The data was then subset to include only these highly variable genes, which were subsequently scaled to have zero mean and unit variance.

Following preprocessing, dimensionality reduction was performed. Principal Component Analysis (PCA) was conducted on the scaled data matrix. The resulting PCA representation was used to construct a k-nearest neighbor graph, with the number of neighbors (`n_neighbors`) set to 10 and the number of principal components (`n_pcs`) set to 40. A two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding was also generated from this neighbor graph for visualization purposes.

To identify cell populations, graph-based clustering was performed on the nearest-neighbor graph using the Leiden algorithm. The algorithm was run using the `'igraph'` implementation for two iterations to partition the cells into distinct clusters. The final clustering assignments were visualized on the UMAP embedding to assess the separation of the identified cell communities.",scanpy,python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import the Scanpy library for single-cell analysis
import scanpy as sc

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting to ensure consistent and clear visuals
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load the built-in PBMC 3k dataset from Scanpy as an example
adata = sc.datasets.pbmc3k()

# It's good practice to keep a copy of the raw data.
# The .raw attribute is the conventional place to store it in Scanpy.
adata.raw = adata.copy()

#--------------------------------
# Preprocessing & Dimensionality Reduction
#--------------------------------

# --- Normalization, Feature Selection, and Scaling ---
# Normalize total counts per cell to account for sequencing depth differences
sc.pp.normalize_total(adata, target_sum=1e4)

# Apply a log(1+x) transformation to stabilize the variance
sc.pp.log1p(adata)

# Identify highly variable genes (HVGs) for downstream analysis
sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat')

# Subset the AnnData object to keep only the highly variable genes
adata = adata[:, adata.var.highly_variable]

# Scale the data so that each gene has zero mean and unit variance
sc.pp.scale(adata)

# --- PCA and Neighborhood Graph ---
# Perform Principal Component Analysis (PCA) on the scaled data
sc.tl.pca(adata, svd_solver='arpack')

# Compute the neighborhood graph of cells based on the PCA representation.
# This graph is the basis for both clustering and UMAP embedding.
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)

# --- UMAP for Visualization ---
# Generate a UMAP embedding from the neighbors graph. This is for visualization purposes.
sc.tl.umap(adata)

#--------------------------------
# Major Analysis Task: Clustering
#--------------------------------

# Apply the Leiden algorithm to find communities (clusters) in the neighborhood graph.
# The `flavor='igraph'` and a fixed number of iterations are used for performance and reproducibility.
sc.tl.leiden(adata, flavor='igraph', n_iterations=2)

#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the UMAP embedding, with cells colored by their assigned Leiden cluster.
# This plot allows for a visual assessment of the clustering results.
sc.pl.umap(adata, color=['leiden'], title=""Leiden Clustering on PBMC3k"", size=20)

#--------------------------------
# Data Output
#--------------------------------

# Save the processed AnnData object. It now contains the clustering results
# in `adata.obs['leiden']` and is ready for cell type annotation.
adata.write(""pbmc3k_clustered.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,F
scRNA-seq,Clustering,"The analysis was conducted in Python with Scanpy on a pre-processed data object. A k-nearest neighbor graph was constructed in the existing Principal Component Analysis space to represent the data's manifold structure. This graph served as the common input for two subsequent steps: graph-based clustering using the Leiden algorithm to identify cell communities, and generation of a UMAP embedding for visualization. The final cluster assignments were then visualized on the corresponding UMAP embedding to facilitate the interpretation of cell populations.","The single-cell analysis was performed in Python using the Scanpy library. The workflow commenced with a pre-processed AnnData object that had already undergone quality control, normalization, and initial dimensionality reduction via Principal Component Analysis (PCA).

To identify distinct cell populations, we first constructed a k-nearest neighbor (k-NN) graph in the PCA space. This was achieved by considering the first 30 principal components and identifying the 15 nearest neighbors for each cell. This k-NN graph, which represents the manifold structure of the data, served as the input for community detection.

Graph-based clustering was then performed using the Leiden algorithm with a `resolution` parameter set to 1.0. This method partitions the k-NN graph to group cells with similar transcriptomic profiles into distinct clusters. For visualization of these clusters, a two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding was generated from the same k-NN graph. The final clustering assignments were visualized on the UMAP plot, with cluster labels placed directly onto the corresponding cell populations to facilitate interpretation.","scanpy, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 0
sc.settings.set_figure_params(dpi=80, facecolor=""white"", frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load the preprocessed AnnData object. This file should contain
# dimensionality reduction results (e.g., PCA).
# A backup URL is provided in case the local file is not found.
adata = sc.read(
    ""s4d8_dimensionality_reduction.h5ad"",
    backup_url=""https://figshare.com/ndownloader/files/40016014""
)

#--------------------------------
# Major Analysis Task: Clustering
#--------------------------------

# --- Neighborhood Graph ---
# Compute the neighborhood graph of cells based on the PCA representation.
# This graph is the foundation for both clustering and UMAP.
sc.pp.neighbors(adata, n_neighbors=15, n_pcs=30)

# --- Leiden Clustering ---
# Apply the Leiden algorithm to partition the neighborhood graph into clusters.
# The 'resolution' parameter controls the number of clusters found; higher
# values lead to more, smaller clusters.
sc.tl.leiden(adata, resolution=1.0, key_added=""leiden"")

# --- UMAP for Visualization ---
# Compute the UMAP embedding from the neighborhood graph. This is done
# here to ensure it's available for plotting, in case it wasn't run previously.
sc.tl.umap(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the UMAP embedding, with cells colored by their assigned Leiden cluster.
# The `legend_loc='on data'` option places cluster labels directly on the plot.
sc.pl.umap(adata, color=[""leiden""], title=""Leiden Clustering"", legend_loc='on data')


#--------------------------------
# Data Output
#--------------------------------

# Save the updated AnnData object. It now contains the clustering results
# stored in `adata.obs['leiden']` and is ready for marker gene analysis.
adata.write(""s4d8_clustering.h5ad"")
",https://www.sc-best-practices.org/cellular_structure/clustering.html,F
scRNA-seq,Clustering,"The analysis was performed in R with Seurat, encompassing initial filtering, quality control, log-normalization, and feature selection using a variance-stabilizing transformation. The data was then scaled, and Principal Component Analysis was performed on the highly variable genes. The resulting principal components were used to construct a k-nearest neighbor graph, which was partitioned to identify cell clusters. For visualization, a UMAP embedding was generated from the same principal components, and the final cluster assignments were displayed on the plot.","The single-cell analysis was conducted in R using the Seurat package. Raw gene expression data was loaded from a 10x Genomics `filtered_feature_bc_matrices` directory. An initial Seurat object was created, applying a preliminary filter to retain genes expressed in at least three cells and cells expressing a minimum of 200 genes.

A standard preprocessing workflow was then applied. First, the percentage of mitochondrial gene expression was calculated for each cell as a quality control metric. The data was subsequently normalized using the `LogNormalize` method, where counts for each cell were scaled by a factor of 10,000 and log-transformed. For feature selection, the top 2,000 most highly variable genes were identified using the variance-stabilizing transformation (`vst`) method. The expression of all genes was then scaled to have a mean of zero and a variance of one across all cells.

Dimensionality reduction was performed by running Principal Component Analysis (PCA) on the scaled expression values of the highly variable genes. Based on the results of the PCA, the first 10 principal components were selected to capture the most significant biological variation. These components were used to construct a k-nearest neighbor (k-NN) graph, which in turn was partitioned to identify cell clusters using a graph-based clustering approach with a resolution parameter of 0.5. Finally, a two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding was generated from the same 10 principal components for visualization of the cell clusters. The final clustering results were visualized on the UMAP plot.","Seurat, dplyr, ggplot2, patchwork",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for single-cell analysis, data manipulation, and plotting
library(Seurat)      # Main package for single-cell RNA-seq analysis
library(dplyr)       # For data manipulation functions
library(ggplot2)     # For creating custom, publication-quality plots
library(patchwork)   # For easily combining multiple ggplot2 plots

#--------------------------------
# Data Input
#--------------------------------

# Specify the directory containing the 10X Genomics count matrices.
# NOTE: You must replace this with the actual path to your data.
data_dir <- ""path/to/filtered_gene_bc_matrices/hg19/""

# Read the 10X Genomics data into a raw count matrix.
raw_data <- Read10X(data.dir = data_dir)

# Create a Seurat object from the raw count matrix with basic initial filtering.
seurat_obj <- CreateSeuratObject(
  counts = raw_data,
  project = ""ClusteringDemo"",
  min.cells = 3,
  min.features = 200
)

#--------------------------------
# Preprocessing, QC, and Normalization
#--------------------------------

# Calculate the percentage of mitochondrial genes for each cell as a QC metric.
seurat_obj[[""percent.mt""]] <- PercentageFeatureSet(seurat_obj, pattern = ""^MT-"")

# Normalize the data to account for differences in library size.
seurat_obj <- NormalizeData(
  seurat_obj,
  normalization.method = ""LogNormalize"",
  scale.factor = 10000
)

# Identify highly variable features (genes) to be used for downstream analysis.
seurat_obj <- FindVariableFeatures(
  seurat_obj,
  selection.method = ""vst"",
  nfeatures = 2000
)

# Scale the data, which is a standard step before running PCA.
seurat_obj <- ScaleData(seurat_obj, features = rownames(seurat_obj))


#--------------------------------
# Major Analysis Tasks: Dimensionality Reduction & Clustering
#--------------------------------

# --- PCA ---
# Run Principal Component Analysis on the highly variable features.
seurat_obj <- RunPCA(seurat_obj, features = VariableFeatures(seurat_obj))

# --- Clustering ---
# Build a K-nearest neighbor (KNN) graph based on the PCA space.
# The number of PCs (dims) used is a critical parameter.
seurat_obj <- FindNeighbors(seurat_obj, dims = 1:10)

# Apply graph-based clustering to partition the cells.
# The `resolution` parameter controls the number of clusters identified.
seurat_obj <- FindClusters(seurat_obj, resolution = 0.5)

# --- UMAP for Visualization ---
# Run UMAP to generate a non-linear 2D embedding for visualization.
seurat_obj <- RunUMAP(seurat_obj, dims = 1:10)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize QC metrics with violin plots.
VlnPlot(seurat_obj, features = c(""nFeature_RNA"", ""nCount_RNA"", ""percent.mt""), ncol = 3)

# Visualize the PCA results.
DimPlot(seurat_obj, reduction = ""pca"") + ggtitle(""PCA Plot"")

# Visualize the UMAP embedding with cells colored by their assigned cluster.
DimPlot(seurat_obj, reduction = ""umap"", label = TRUE, pt.size = 0.8) +
  ggtitle(""UMAP Plot with Cluster Labels"")


#--------------------------------
# Data Output
#--------------------------------

# Save the processed Seurat object, which now contains clustering results.
# This object is ready for downstream marker analysis and cell type annotation.
saveRDS(seurat_obj, file = ""Clustering_SeuratObject.rds"")
",https://satijalab.org/seurat/articles/pbmc3k_tutorial#identification-of-highly-variable-features-feature-selection,F
scRNA-seq,Annotation,"The analysis was conducted in Python with Scanpy, involving a standard workflow of normalization, feature selection, dimensionality reduction, and Leiden clustering to identify cell communities. Cell type annotation was then performed by assessing the expression of canonical marker genes for each cluster, a process facilitated by a dot plot. Based on these expression patterns, clusters were manually assigned cell type identities. These final annotations were then visualized on the UMAP embedding to display the distribution of the identified cell populations.","The single-cell analysis was conducted in Python using the Scanpy library. A standard dataset of 3,000 peripheral blood mononuclear cells (PBMCs) was used for the workflow.

The raw data was processed through a standard pipeline beginning with normalization, where each cell’s library size was scaled to a total of 10,000 counts, followed by a variance-stabilizing log-plus-one transformation. Feature selection was then performed to identify the 2,000 most highly variable genes (HVGs). The dataset was subset to these HVGs, and the expression values were scaled to have zero mean and unit variance. Principal Component Analysis (PCA) was performed on the scaled data, and the resulting top 40 principal components were used to construct a k-nearest neighbor graph with `k=10`. This graph was used both to generate a two-dimensional UMAP embedding for visualization and as the basis for clustering. Cell communities were identified by applying the Leiden algorithm to the nearest-neighbor graph.

Cell type annotation was performed by cross-referencing the identified clusters with the expression of canonical marker genes. The average expression of markers for major PBMC populations—including `MS4A1` (B cells), `CD3D` (T cells), `NKG7` (NK cells), and `CD14` (Monocytes)—was assessed for each cluster using a dot plot. Based on these expression patterns, each cluster was manually assigned a cell type identity. The final cell type annotations were visualized on the UMAP embedding to confirm the distinct separation of the identified cell populations.","scanpy, annodata",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and data handling
import scanpy as sc
import anndata as ad

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting to ensure consistent and clear visuals
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load the built-in PBMC 3k dataset from Scanpy as an example
adata = sc.datasets.pbmc3k()

#--------------------------------
# Preprocessing & Clustering
#--------------------------------

# --- Normalization, Feature Selection, and Scaling ---
# Normalize total counts per cell to account for sequencing depth differences
sc.pp.normalize_total(adata, target_sum=1e4)

# Apply a log(1+x) transformation to stabilize the variance
sc.pp.log1p(adata)

# Identify highly variable genes (HVGs) for downstream analysis
sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat')

# Subset the AnnData object to keep only the highly variable genes
adata = adata[:, adata.var.highly_variable]

# Scale the data so that each gene has zero mean and unit variance
sc.pp.scale(adata)

# --- PCA, Neighborhood Graph, and UMAP ---
# Perform Principal Component Analysis (PCA) on the scaled data
sc.tl.pca(adata, svd_solver='arpack')

# Compute the neighborhood graph, which is the basis for clustering and UMAP
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)

# Generate a UMAP embedding for visualization
sc.tl.umap(adata)

# --- Leiden Clustering ---
# Apply the Leiden algorithm to find communities (clusters) of cells
sc.tl.leiden(adata)

#--------------------------------
# Major Analysis Task: Cell-Type Annotation
#--------------------------------

# Define a dictionary of canonical marker genes for common PBMC types.
# These genes are used to identify cell types based on their expression patterns.
marker_genes = {
    'B cells': ['MS4A1', 'CD79A'],
    'T cells': ['CD3D', 'CD3E'],
    'NK cells': ['NKG7', 'GNLY'],
    'Monocytes': ['CD14', 'LYZ']
}

# Manually define a mapping from cluster IDs to cell type names.
# NOTE: This mapping is created after inspecting the dot plot below and should be
# adjusted based on the specific results of your analysis.
cluster_to_cell_type = {
    '0': 'T cells',
    '1': 'Monocytes',
    '2': 'NK cells',
    '3': 'B cells',
    '4': 'T cells' # Example of merging multiple clusters into one cell type
}

# Create a new column in the observation metadata to store the cell type annotations.
# The .map() function applies the dictionary to the 'leiden' column.
adata.obs['cell_type'] = adata.obs['leiden'].map(cluster_to_cell_type).astype('category')


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Generate a dot plot to visualize the expression of marker genes across clusters.
# This plot is essential for assigning cell types to clusters.
sc.pl.dotplot(adata, marker_genes, groupby='leiden', standard_scale='var',
              title='Marker Expression by Leiden Clusters')

# Visualize the final cell type annotations on the UMAP embedding.
# This plot shows the spatial distribution of the identified cell types.
sc.pl.umap(adata, color='cell_type', title='Cell Type Annotation', legend_loc='on data')

#--------------------------------
# Data Output
#--------------------------------

# Save the fully processed and annotated AnnData object.
# This file can be used for further downstream analysis, such as differential
# expression between cell types or cell-cell interaction studies.
adata.write(""pbmc3k_annotated.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html,F
scRNA-seq,Annotation,"The analysis was conducted in Python with Scanpy, involving a standard workflow of normalization, feature selection, dimensionality reduction, and Leiden clustering to identify cell communities. Cell type annotation was then performed by assessing the expression of canonical marker genes for each cluster, a process facilitated by a dot plot. Based on these expression patterns, clusters were manually assigned cell type identities. These final annotations were then visualized on the UMAP embedding to display the distribution of the identified cell populations.","The single-cell analysis was performed in Python using the Scanpy library. The workflow started with a pre-processed AnnData object which had already undergone dimensionality reduction and clustering, with cluster assignments stored in the `leiden` metadata column.

The primary analysis task was the manual annotation of cell types based on these pre-computed clusters. A dictionary was defined to map the numerical cluster IDs to specific biological cell type names, such as ""B cell,"" ""T cell,"" and ""Myeloid cell."" This mapping was applied to the `leiden` cluster assignments for each cell, creating a new metadata column named `manual_annotation`. Any cluster ID not present in the defined dictionary was automatically assigned the label ""Unknown"" to ensure all cells were accounted for.

To visualize the results of this annotation, a Uniform Manifold Approximation and Projection (UMAP) embedding was generated if one was not already present in the data object. The final cell type assignments were then displayed on the UMAP plot, with each cell colored according to its annotated type and labels placed directly on the corresponding cell populations for clear identification.","scanpy, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 0
sc.settings.set_figure_params(dpi=80, facecolor=""white"", frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load the preprocessed and clustered AnnData object.
# This file is expected to contain clustering results (e.g., in `adata.obs['leiden']`).
# A backup URL is provided in case the local file is not found.
adata = sc.read(
    ""s4d8_clustering.h5ad"",
    backup_url=""https://figshare.com/ndownloader/files/40016014""
)

#--------------------------------
# Major Analysis Task: Cell-Type Annotation
#--------------------------------

# Define a dictionary to manually map cluster IDs to cell type names.
# This mapping is based on prior biological knowledge or marker gene analysis.
manual_labels = {
    ""0"": ""B cell"",
    ""1"": ""T cell"",
    ""2"": ""Myeloid cell"",
    ""3"": ""NK cell"",
    ""4"": ""Dendritic cell""
}

# Create a new column 'manual_annotation' in the observation metadata.
# The .map() function applies the dictionary. The lambda function with .get()
# ensures that any cluster ID not in the dictionary is assigned ""Unknown"".
adata.obs[""manual_annotation""] = adata.obs[""leiden""].map(
    lambda x: manual_labels.get(x, ""Unknown"")
)

# Print a count of cells assigned to each annotated cell type for a quick summary.
print(""Manual annotation summary:"")
print(adata.obs[""manual_annotation""].value_counts())


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Ensure a UMAP embedding exists for visualization.
# This check prevents errors if the loaded object doesn't have UMAP coordinates.
if ""X_umap"" not in adata.obsm.keys():
    sc.pp.neighbors(adata) # UMAP requires a neighbor graph
    sc.tl.umap(adata)

# Visualize the UMAP embedding, with cells colored by their manual annotation.
# This plot provides a spatial view of the identified cell populations.
sc.pl.umap(
    adata,
    color=""manual_annotation"",
    title=""Manual Cell Type Annotation"",
    legend_loc=""on data""
)

#--------------------------------
# Data Output
#--------------------------------

# Save the updated AnnData object, which now includes the manual cell type annotations.
# This file is ready for downstream analyses focused on specific cell populations.
adata.write(""s4d8_manual_annotation.h5ad"")
",https://www.sc-best-practices.org/cellular_structure/annotation.html,F
scRNA-seq,Annotation,"The analysis was conducted in R with Seurat, involving a standard workflow of preprocessing, dimensionality reduction via PCA, and graph-based clustering to identify cell communities. Cell type annotation was achieved by performing differential expression analysis to find marker genes for each cluster. Based on the expression of canonical markers identified through this process, clusters were manually assigned cell type identities. The annotation was validated by visualizing top marker genes on a heatmap and displaying the final cell type labels on the UMAP embedding.","The single-cell analysis was performed using the Seurat package in R. Raw gene expression counts were loaded from a 10x Genomics `filtered_gene_bc_matrices` directory, and an initial Seurat object was created. During this setup, preliminary quality filters were applied, retaining only genes expressed in three or more cells and cells with a minimum of 200 detected genes.

The data underwent a standard preprocessing workflow. This included calculating the percentage of mitochondrial gene expression for each cell, followed by library size normalization using the `LogNormalize` method with a scale factor of 10,000. Feature selection was then performed to identify the top 2,000 most highly variable genes (HVGs) using the variance-stabilizing transformation (`vst`) method. The expression of all genes was subsequently scaled to have zero mean and unit variance.

Dimensionality reduction was carried out by performing Principal Component Analysis (PCA) on the scaled expression data of the HVGs. The first 10 principal components were selected to construct a k-nearest neighbor (k-NN) graph, which was then partitioned using a graph-based clustering algorithm with a resolution of 0.5 to identify cell communities. A two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding was also generated from the same 10 PCs for visualization.

To assign biological identities to these clusters, we performed differential expression analysis to find marker genes for each cluster compared to all other cells. The analysis was configured to identify only positive markers, requiring genes to be detected in at least 25% of cells in the cluster and to show a minimum log-fold change of 0.25. Based on the expression of canonical marker genes identified in this analysis, clusters were manually annotated with cell type labels such as ""Naive CD4 T,"" ""CD14+ Monocytes,"" and ""B cell."" The results were validated by visualizing the expression of the top two marker genes per cluster on a heatmap and by displaying the final cell type annotations on the UMAP plot.","Seurat, dplyr, ggplot2, patchwork",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for single-cell analysis, data manipulation, and plotting
library(Seurat)      # Core package for single-cell RNA-seq analysis
library(dplyr)       # For data manipulation, especially for handling marker gene tables
library(ggplot2)     # For creating custom, publication-quality plots
library(patchwork)   # For easily combining multiple ggplot2 plots

#--------------------------------
# Data Input
#--------------------------------

# Specify the directory containing the 10X Genomics count matrices.
# NOTE: You must replace this with the actual path to your data.
data_dir <- ""path/to/filtered_gene_bc_matrices/hg19/""

# Read the 10X Genomics data into a raw count matrix.
raw_data <- Read10X(data.dir = data_dir)

# Create a Seurat object from the raw count matrix with basic initial filtering.
seurat_obj <- CreateSeuratObject(
  counts = raw_data,
  project = ""ClusterBiomarkers"",
  min.cells = 3,
  min.features = 200
)

#--------------------------------
# Preprocessing and QC
#--------------------------------

# Calculate the percentage of mitochondrial genes for each cell as a QC metric.
seurat_obj[[""percent.mt""]] <- PercentageFeatureSet(seurat_obj, pattern = ""^MT-"")

# Normalize the data to account for differences in library size.
seurat_obj <- NormalizeData(seurat_obj, normalization.method = ""LogNormalize"", scale.factor = 10000)

# Identify highly variable features (genes) for downstream analysis.
seurat_obj <- FindVariableFeatures(seurat_obj, selection.method = ""vst"", nfeatures = 2000)

# Scale the data, a standard step before running PCA.
all_genes <- rownames(seurat_obj)
seurat_obj <- ScaleData(seurat_obj, features = all_genes)

#--------------------------------
# Major Analysis Tasks
#--------------------------------

# --- Dimensionality Reduction & Clustering ---
# Run PCA on the highly variable features.
seurat_obj <- RunPCA(seurat_obj, features = VariableFeatures(seurat_obj))

# Build a K-nearest neighbor (KNN) graph and perform clustering.
seurat_obj <- FindNeighbors(seurat_obj, dims = 1:10)
seurat_obj <- FindClusters(seurat_obj, resolution = 0.5)

# Run UMAP to generate a 2D embedding for visualization.
seurat_obj <- RunUMAP(seurat_obj, dims = 1:10)


# --- Finding Cluster Biomarkers ---
# Find differentially expressed features (cluster biomarkers) for every cluster compared to all remaining cells.
# `only.pos = TRUE` ensures we only get positive markers (genes upregulated in the cluster).
cluster_markers <- FindAllMarkers(seurat_obj, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)


# --- Cell Type Annotation ---
# Manually define new cell type names based on canonical markers found in the literature
# and observed in the `cluster_markers` table.
# NOTE: The order and names must be adjusted based on your specific dataset and clustering results.
new.cluster.ids <- c(""Naive CD4 T"", ""CD14+ Monocytes"", ""Memory CD4 T"", ""B cell"",
                     ""CD8 T"", ""NK cell"", ""Dendritic cell"", ""Platelet"")

# Assign the new names to the cluster levels in the Seurat object.
names(new.cluster.ids) <- levels(seurat_obj)
seurat_obj <- RenameIdents(seurat_obj, new.cluster.ids)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the initial UMAP clusters before annotation.
DimPlot(seurat_obj, reduction = ""umap"", label = TRUE, pt.size = 0.8) +
  ggtitle(""UMAP Plot: Clusters before Cell Type Assignment"")

# --- Biomarker Visualization ---
# Extract the top markers for each cluster to visualize on a heatmap.
# Here, we group by cluster and select the top 2 genes based on log2 fold change.
top_markers <- cluster_markers %>%
               group_by(cluster) %>%
               slice_max(order_by = avg_log2FC, n = 2)

# Create a heatmap of the top marker genes.
DoHeatmap(seurat_obj, features = top_markers$gene) + NoLegend()


# --- Final Annotation Visualization ---
# Visualize the UMAP with the final, manually assigned cell type identities.
DimPlot(seurat_obj, reduction = ""umap"", label = TRUE, pt.size = 0.8) +
  ggtitle(""UMAP Plot: Cell Type Identities"")


#--------------------------------
# Data Output
#--------------------------------

# Save the fully processed and annotated Seurat object.
saveRDS(seurat_obj, file = ""Annotated_SeuratObject.rds"")
",https://satijalab.org/seurat/articles/pbmc3k_tutorial#identification-of-highly-variable-features-feature-selection,F
scRNA-seq,Annotation,"The analysis for automated cell type identification was performed in Python using the CellTypist library on a pre-processed dataset. A pre-trained classification model, optimized for immune cell populations, was used for the annotation. Cell type labels were assigned based on a majority voting consensus from the model's classifiers, and prediction confidence scores were also obtained for each cell. The results were visualized on a UMAP embedding, with parallel plots displaying both the final cell type assignments and their corresponding confidence scores to assess annotation quality.","For automated cell type identification, the analysis was performed in Python using the CellTypist library. The workflow began with a pre-processed single-cell dataset that had already undergone quality control and normalization.

The CellTypist models were first downloaded and updated to ensure the latest version was used. A pre-trained classification model, specifically the `Immune_All_Low.pkl` model, which is optimized for detailed annotation of immune cell populations, was loaded for the analysis.

Automatic cell type annotation was then executed on the input dataset using the `celltypist.annotate` function. The `majority_voting` parameter was enabled, meaning the final cell type label assigned to each cell was determined by a consensus prediction from the ensemble of classifiers within the model. The resulting predicted cell type labels, along with their corresponding confidence scores, were stored in the dataset's cell-level metadata for subsequent analysis.

To visualize the annotation results, a two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding was computed from the data's principal component analysis representation. The final cell type assignments were then visualized on the UMAP plot, with cells colored by their predicted label to show the spatial distribution of the identified populations. A parallel UMAP plot was also generated, with cells colored by their annotation confidence score, to provide a visual assessment of the certainty of the predictions across the dataset.","scanpy, celltypist, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis, automatic annotation, and plotting
import scanpy as sc
import celltypist
from celltypist import models
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
# Verbosity 2 provides more detailed logging, which is useful for tracking progress.
sc.settings.verbosity = 2
sc.settings.set_figure_params(dpi=80, facecolor='white', frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load a preprocessed single-cell dataset (AnnData object).
# A backup URL is provided in case the local file is not found.
adata = sc.read(
    ""demo_2000_cells.h5ad"",
    backup_url=""https://celltypist.cog.sanger.ac.uk/Notebook_demo_data/demo_2000_cells.h5ad""
)
print(""Data shape:"", adata.shape)

#--------------------------------
# Major Analysis Task: Automatic Cell-Type Annotation
#--------------------------------

# --- Model Preparation ---
# Download the latest CellTypist models to ensure access to the most recent annotations.
# `force_update=True` will overwrite existing models if a newer version is available.
print(""Downloading CellTypist models..."")
models.download_models(force_update=True)

# Load a specific pre-trained CellTypist model.
# ""Immune_All_Low.pkl"" is designed for fine-grained annotation of immune cells.
model = models.Model.load(model=""Immune_All_Low.pkl"")

# --- Annotation ---
# Run CellTypist's automatic annotation on the dataset.
# `majority_voting=True` assigns a final label to each cell based on a consensus
# prediction from the underlying classification models.
print(""Starting automatic annotation..."")
predictions = celltypist.annotate(adata, model=model, majority_voting=True)

# Store the prediction results in the AnnData object for easy access.
adata.obs[""celltypist_cell_label""] = predictions.predicted_labels
adata.obs[""celltypist_confidence""] = predictions.confidence

# --- UMAP for Visualization ---
# Compute the neighborhood graph and UMAP embedding. This is necessary for
# visualizing the annotation results in a 2D space.
sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)
sc.tl.umap(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the UMAP embedding, creating two plots side-by-side:
# 1. Cells colored by their predicted cell type label from CellTypist.
# 2. Cells colored by the confidence score of the prediction.
sc.pl.umap(
    adata,
    color=[""celltypist_cell_label"", ""celltypist_confidence""],
    wspace=0.4,
    title=[""CellTypist: Predicted Labels"", ""Annotation Confidence""]
)


#--------------------------------
# Data Output
#--------------------------------

# Save the updated AnnData object, which now includes the automatic cell type
# annotations and confidence scores from CellTypist.
adata.write(""celltypist_annotated.h5ad"")
",https://www.sc-best-practices.org/cellular_structure/annotation.html,F
scRNA-seq,Annotation,"The analysis was performed in Python using scvi-tools and Scanpy for automated cell type annotation. The workflow utilized the CellAssign model, which was provided with a single-cell dataset and a corresponding cell-type marker gene matrix. The model was trained on the expression of these marker genes to predict the probability of each cell belonging to the predefined types. Final cell identities were assigned based on the highest prediction probability. The annotation results were visualized using a probability heatmap and by displaying the assigned cell types on a UMAP embedding.","The single-cell analysis was performed in Python, leveraging the scvi-tools and Scanpy libraries. The workflow focused on automated cell type annotation of a follicular lymphoma dataset using the CellAssign model.

Initially, the follicular lymphoma single-cell dataset and a corresponding cell-type marker gene matrix were downloaded. The marker matrix defined a priori which genes are characteristic of specific cell types. The single-cell data was then subsetted to include only the genes present in this marker matrix, as CellAssign bases its predictions exclusively on this gene set. To account for technical variations in sequencing depth across cells, a library size factor was calculated and added to the data object.

The CellAssign model was then initialized using the gene-subsetted dataset and the marker gene matrix. The model was trained for 400 epochs, and its convergence was monitored by plotting the evidence lower bound (ELBO) over the training period.

Following training, the model was used to predict the probability of each cell belonging to each of the predefined cell types in the marker matrix. A definitive cell type identity was assigned to each cell by selecting the label with the highest probability. The results of the annotation were visualized in several ways. The raw probability matrix was displayed as a heatmap to provide a global view of prediction certainties. To visualize the spatial distribution of the final, assigned cell types, a UMAP embedding was computed on the full dataset, and cells were colored according to their CellAssign-predicted identity.","os, tempfile, matplotlib, pandas, scanpy, scvi, seaborn, torch",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for system operations, data handling, analysis, and plotting
import os
import tempfile
import matplotlib.pyplot as plt
import pandas as pd
import scanpy as sc
import scvi
import seaborn as sns
import torch
from scvi.external import CellAssign

#--------------------------------
# Script Configuration
#--------------------------------

# Set global parameters for plotting and computation
sc.set_figure_params(figsize=(6, 6), frameon=False)
sns.set_theme()
# Set precision for matrix multiplication in PyTorch for performance.
torch.set_float32_matmul_precision(""high"")

#--------------------------------
# Data Input & Preparation
#--------------------------------

# Create a temporary directory to store downloaded data
save_dir = tempfile.TemporaryDirectory()
print(f""Data will be downloaded to temporary directory: {save_dir.name}"")

# Define file paths for the dataset and the marker gene matrix
adata_path = os.path.join(save_dir.name, ""sce_follicular.h5ad"")
marker_path = os.path.join(save_dir.name, ""FL_celltype.csv"")

# Download the follicular lymphoma dataset and its corresponding marker gene file
# using wget. These files are stored in the temporary directory.
print(""Downloading dataset and marker file..."")
os.system(f""wget -q https://ndownloader.figshare.com/files/27458798 -O {adata_path}"")
os.system(f""wget -q https://ndownloader.figshare.com/files/27458831 -O {marker_path}"")

# Load the single-cell dataset as an AnnData object
adata = sc.read(adata_path)
adata.var_names_make_unique()
adata.obs_names_make_unique()

# Load the marker gene matrix. Rows should be genes and columns should be cell types.
marker_gene_mat = pd.read_csv(marker_path, index_col=0)

#--------------------------------
# Preprocessing for CellAssign
#--------------------------------

# Subset the dataset to include only the genes present in the marker matrix.
# CellAssign only uses these genes for its predictions.
adata_subset = adata[:, marker_gene_mat.index].copy()

# Calculate per-cell library sizes and create a size factor.
# This is used by the model to account for differences in sequencing depth.
library_sizes = adata_subset.X.sum(1)
adata_subset.obs[""size_factor""] = library_sizes / library_sizes.mean()

# Set up the AnnData object for the CellAssign model.
# This function registers the necessary information for the model.
scvi.external.CellAssign.setup_anndata(adata_subset, size_factor_key=""size_factor"")


#--------------------------------
# Major Analysis Task: Annotation with CellAssign
#--------------------------------

# Initialize the CellAssign model with the subsetted data and the marker gene matrix.
model = CellAssign(adata_subset, marker_gene_mat)

# Train the model. The number of epochs can be adjusted based on convergence.
print(""Training CellAssign model..."")
model.train(max_epochs=400)

# Predict the soft cell type assignment probabilities for each cell.
# This returns a DataFrame where rows are cells and columns are cell types.
predictions = model.predict()

# Assign the most likely cell type to each cell in the original AnnData object.
# The `.idxmax(axis=1)` function finds the cell type with the highest probability for each cell.
adata.obs['cellassign_prediction'] = predictions.idxmax(axis=1)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Model Convergence Plot ---
# Plot the validation ELBO (Evidence Lower Bound) to check for model convergence.
# A stable or plateauing curve indicates that the model has trained sufficiently.
model.history[""elbo_validation""].plot()
plt.xlabel(""Epoch"")
plt.ylabel(""Validation ELBO"")
plt.title(""CellAssign Convergence Plot"")
plt.show()

# --- Prediction Heatmap ---
# Visualize the raw probability matrix as a heatmap.
# This shows the confidence of assignment for each cell to each possible cell type.
plt.figure(figsize=(10, 10))
sns.heatmap(predictions, cmap=""viridis"")
plt.title(""CellAssign Prediction Probabilities"")
plt.show()

# --- UMAP Visualization of Final Annotations ---
# Run standard dimensionality reduction to visualize the final annotations.
print(""Computing UMAP for visualization..."")
sc.pp.neighbors(adata)
sc.tl.umap(adata)
sc.pl.umap(adata, color='cellassign_prediction', title='Final CellAssign Annotations', legend_loc='on data')


#--------------------------------
# Data Output
#--------------------------------

# Save the final, annotated AnnData object.
adata.write(""cellassign_annotated.h5ad"")

# Clean up the temporary directory
save_dir.cleanup()
",https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scrna/cellassign_tutorial.html,F
scRNA-seq,Integration,"The analysis was performed in Python using Scanpy and BBKNN to correct for batch effects. The workflow began with a batch-aware feature selection, where highly variable genes were identified independently within each batch. Following standard normalization and Principal Component Analysis, batch correction was applied using the Batch-Balanced K-Nearest Neighbors algorithm. This method constructs an integrated neighborhood graph by balancing connections within and across batches. A UMAP embedding was then computed from this integrated graph, and the effective mixing of batches was visually assessed to confirm successful integration.","To address technical variability between experimental batches, the single-cell analysis was performed in Python using the Scanpy and BBKNN libraries.

The workflow began with a pre-processed data object containing a metadata column identifying the batch origin for each cell. To ensure that feature selection was not biased by batch-specific effects, highly variable genes (HVGs) were identified independently within each batch using the `cell_ranger` method, and the top 2,000 genes were retained for analysis. The expression data for these selected HVGs was then normalized by scaling each cell's total counts to 10,000, followed by a log-plus-one transformation and scaling to unit variance and zero mean. The dimensionality of this pre-processed data was then reduced by performing Principal Component Analysis (PCA) and retaining the top 50 principal components.

Batch effect correction was subsequently performed on the PCA-reduced data using the Batch-Balanced K-Nearest Neighbors (BBKNN) algorithm. This method constructs an integrated k-nearest neighbor graph by first identifying a specified number of neighbors for each cell exclusively within its own batch—in this case, 3—before connecting to cells across different batches. To visualize the integration, a Uniform Manifold Approximation and Projection (UMAP) embedding was computed directly from the BBKNN-corrected neighborhood graph. The successful mixing of cells from different batches in the resulting UMAP plot was used to qualitatively assess the effectiveness of the batch correction.","numpy, scanpy, bbknn, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis, batch correction, and plotting
import numpy as np
import scanpy as sc
import bbknn
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 3  # Set to a higher level for more detailed logging
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load your AnnData object from a file.
# NOTE: Replace ""your_data.h5ad"" with the path to your data.
# This dataset should contain a 'batch' column in adata.obs identifying the batch for each cell.
adata = sc.read(""your_data.h5ad"")

#--------------------------------
# Preprocessing & Feature Selection
#--------------------------------

# --- Batch-Aware Feature Selection ---
# Identify highly variable genes (HVGs) using a batch-aware method.
# `batch_key=""batch""` ensures that HVGs are selected per batch, which can
# improve the selection process in the presence of strong batch effects.
sc.pp.highly_variable_genes(
    adata,
    n_top_genes=2000,
    flavor=""cell_ranger"",
    batch_key=""batch""
)
# Subset the data to contain only the selected highly variable genes.
adata = adata[:, adata.var.highly_variable].copy()

# --- Normalization and Scaling ---
# Standard preprocessing steps: normalize, log-transform, and scale the data.
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)
sc.pp.scale(adata)

# --- PCA ---
# Perform Principal Component Analysis (PCA) for dimensionality reduction.
# The resulting PCs will be used as input for BBKNN.
sc.tl.pca(adata, n_comps=50, svd_solver='arpack')


#--------------------------------
# Major Analysis Task: Batch Integration with BBKNN
#--------------------------------

# Run Batch-Balanced K-Nearest Neighbors (BBKNN) to correct for batch effects.
# BBKNN identifies nearest neighbors within each batch first before building the
# final, integrated neighborhood graph.
# `neighbors_within_batch` controls how many neighbors are identified within each batch.
bbknn.bbknn(adata, batch_key='batch', neighbors_within_batch=3)

# --- UMAP for Visualization ---
# Compute the UMAP embedding. Importantly, this is calculated on the BBKNN-corrected
# neighborhood graph, not the original PCA space.
sc.tl.umap(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Create a UMAP plot colored by the 'batch' variable.
# Effective integration should result in cells from different batches mixing together,
# rather than clustering separately by batch.
sc.pl.umap(adata, color=['batch'], title=""UMAP after BBKNN Integration"")


#--------------------------------
# Data Output
#--------------------------------

# Save the integrated AnnData object to a file.
# This object can be used for downstream analyses like clustering and cell type annotation.
adata.write(""bbknn_integrated_data.h5ad"")
",https://www.sc-best-practices.org/cellular_structure/integration.html,F
scRNA-seq,Integration,"The analysis was performed in Python using Scanpy to transfer cell annotations from a reference to a query dataset. After establishing a common feature space, the reference data was processed to create a PCA embedding and corresponding cluster labels. The core integration involved projecting the query data into the reference's PCA space. This projection was used to transfer the reference cluster annotations and UMAP coordinates directly to the query cells. Successful integration was validated by visualizing the co-embedding of the reference and query datasets on a combined UMAP plot, colored by both origin and the transferred labels.","To transfer cell annotations from a well-characterized reference dataset to a query dataset, we performed an integration analysis using the Scanpy library in Python.

The workflow began by loading a pre-processed and annotated reference PBMC dataset and a separate query PBMC dataset. To ensure a common feature space for integration, both datasets were subset to include only the intersection of genes present in both.

The reference dataset was first processed to generate the necessary components for mapping. This included performing Principal Component Analysis (PCA) to create a reduced-dimension representation, followed by the construction of a k-nearest neighbor graph (using `k=15` neighbors and the top 50 PCs). A UMAP embedding was then computed from this neighbor graph, and Louvain community detection was run to generate a set of cluster labels on the reference data.

The core integration was performed using the `sc.tl.ingest` function. This method projected the query dataset into the pre-computed PCA space of the reference dataset. Based on the query cells' positions within this shared embedding, annotations from the reference—specifically, the Louvain cluster labels—were transferred to the query cells. The UMAP coordinates for the query cells were also projected onto the reference UMAP space.

To validate the integration and annotation transfer, the results were visualized. The UMAP embedding of the query data, colored by the newly ingested labels, was compared to the original reference UMAP. Additionally, the reference and query datasets were concatenated and displayed on a combined UMAP plot, colored by both dataset origin and the transferred labels, to visually confirm successful integration and co-embedding of the two datasets.","anndata, scanpy, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import anndata
import scanpy as sc
import matplotlib.pyplot as plt
import numpy as np

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 2  # Set for detailed logging
sc.settings.set_figure_params(dpi=80, facecolor='white', figsize=(5, 5))

#--------------------------------
# Data Input
#--------------------------------

# Load a pre-processed reference dataset (e.g., PBMC3k)
# This dataset should ideally already have annotations and a UMAP embedding.
print(""Loading reference dataset..."")
adata_ref = sc.datasets.pbmc3k_processed()

# Load the query dataset that you want to annotate.
print(""Loading query dataset..."")
adata = sc.datasets.pbmc68k_reduced()

# Ensure both datasets share the same gene set by taking the intersection of their genes.
# This is a critical step for mapping annotations accurately.
var_names = adata_ref.var_names.intersection(adata.var_names)
adata_ref = adata_ref[:, var_names].copy()
adata = adata[:, var_names].copy()
print(f""Using {len(var_names)} common genes for integration."")

#--------------------------------
# Preprocessing & Embedding on Reference Data
#--------------------------------

# The following steps ensure the reference data has the necessary components (PCA, neighbors, UMAP).
# These might already be present in a pre-processed object but are run here for completeness.
print(""Processing reference dataset..."")
sc.pp.pca(adata_ref, n_comps=50)
sc.pp.neighbors(adata_ref, n_neighbors=15, n_pcs=50)
sc.tl.umap(adata_ref)
sc.tl.louvain(adata_ref) # Example clustering on the reference

#--------------------------------
# Major Analysis Task: Integrate Query Data via Ingest
#--------------------------------

# Use sc.tl.ingest to map annotations and embeddings from the reference to the query data.
# 'louvain' labels from the reference will be transferred to the query.
# The UMAP embedding will also be projected onto the query data.
print(""Integrating query data onto reference using sc.tl.ingest..."")
sc.tl.ingest(adata, adata_ref, obs='louvain', embedding_method='umap')

# For consistent visualization, copy the color scheme for the clusters from the reference.
adata.uns['louvain_colors'] = adata_ref.uns['louvain_colors']

#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Reference Visualization ---
# Plot the UMAP of the reference data to see the original clusters.
sc.pl.umap(adata_ref, color='louvain', title='Reference UMAP with Louvain Clusters')

# --- Query Visualization ---
# Plot the UMAP of the query data. The coordinates are projected from the reference,
# and the colors represent the newly ingested labels.
sc.pl.umap(adata, color='louvain', title='Query UMAP with Ingested Annotations', size=30)

# --- Combined Visualization (Optional) ---
# Concatenate the reference and query objects to visualize them together.
# This is useful for assessing the quality of the integration.
print(""Creating a combined UMAP for visualization..."")
# Note: The `ingest` function stores the projected UMAP in `adata.obsm['X_umap_ingest']`.
# We need to rename it to `X_umap` in the query object before concatenating.
adata.obsm['X_umap'] = adata.obsm.pop('X_umap_ingest')
adata_concat = anndata.concat(
    [adata_ref, adata],
    label='batch',
    keys=['reference', 'query'],
    join='outer' # Keep all cells from both objects
)

# Plot the combined data, coloring by batch and by the transferred labels.
sc.pl.umap(
    adata_concat,
    color=['batch', 'louvain'],
    title='Combined UMAP: Reference and Query',
    wspace=0.5,
    size=20
)

#--------------------------------
# Data Output
#--------------------------------

# Save the annotated query dataset to a file.
adata.write(""ingest_annotated_query_data.h5ad"")

# Save the combined dataset for further inspection.
adata_concat.write(""ingest_combined_data.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/integrating-data-using-ingest.html,F
scRNA-seq,Integration,"The analysis was performed in Python using Scanpy to transfer cell annotations from a reference to a query dataset. After establishing a common feature space, the reference data was processed to create a PCA embedding and corresponding cluster labels. The core integration involved projecting the query data into the reference's PCA space. This projection was used to transfer the reference cluster annotations and UMAP coordinates directly to the query cells. Successful integration was validated by visualizing the co-embedding of the reference and query datasets on a combined UMAP plot, colored by both origin and the transferred labels.","To transfer cell type annotations from a labeled reference to an unlabeled query dataset, we performed a projection-based integration using the Scanpy library in Python.

The analysis began by loading a pre-processed and annotated reference PBMC dataset and a separate, unannotated query PBMC dataset. To create a shared feature space for the integration, both datasets were restricted to the intersection of genes common to both.

The reference dataset was processed to establish a reference atlas. This involved performing Principal Component Analysis (PCA) to generate a low-dimensional representation, constructing a k-nearest neighbor graph (with k=15 and using the top 50 PCs), and computing a UMAP embedding. Louvain clustering was performed on the reference data to generate a set of cell population labels for transfer.

The core integration was executed using the `sc.tl.ingest` function. This method projected the query dataset into the pre-existing PCA space of the reference atlas. Based on each query cell's location within this shared space, the reference's Louvain cluster labels were transferred to the query cells. The UMAP coordinates for the query data were also projected from the reference embedding.

To visually assess the accuracy of the label transfer, we compared the UMAP embedding of the query data, colored by the newly ingested labels, with the original reference UMAP. Furthermore, the reference and query datasets were combined and visualized on a single UMAP plot to confirm that query cells correctly co-localized with their corresponding cell populations in the reference atlas.","os, tempfile, scanpy, scvi, seabor, torch",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import anndata
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 2  # Set for detailed logging
sc.settings.set_figure_params(dpi=80, facecolor='white', figsize=(4, 4))

#--------------------------------
# Data Input
#--------------------------------

# Load a pre-processed reference dataset (e.g., PBMC3k)
# This dataset should ideally already have annotations and a UMAP embedding.
print(""Loading reference dataset..."")
adata_ref = sc.datasets.pbmc3k_processed()

# Load the query dataset that you want to annotate.
print(""Loading query dataset..."")
adata = sc.datasets.pbmc68k_reduced()

# Ensure both datasets share the same gene set by taking the intersection of their genes.
# This is a critical step for mapping annotations accurately.
var_names = adata_ref.var_names.intersection(adata.var_names)
adata_ref = adata_ref[:, var_names].copy()
adata = adata[:, var_names].copy()
print(f""Using {len(var_names)} common genes for integration."")

#--------------------------------
# Preprocessing & Embedding on Reference Data
#--------------------------------

# The following steps ensure the reference data has the necessary components (PCA, neighbors, UMAP).
# These might already be present in a pre-processed object.
print(""Processing reference dataset..."")
sc.pp.pca(adata_ref, n_comps=50)
sc.pp.neighbors(adata_ref, n_neighbors=15, n_pcs=50)
sc.tl.umap(adata_ref)
sc.tl.louvain(adata_ref) # Example clustering on the reference

#--------------------------------
# Major Analysis Task: Integrate Query Data via Ingest
#--------------------------------

# Use sc.tl.ingest to map annotations and embeddings from the reference to the query data.
# 'louvain' labels from the reference will be transferred to the query.
# The UMAP embedding will also be projected onto the query data.
print(""Integrating query data onto reference using sc.tl.ingest..."")
sc.tl.ingest(adata, adata_ref, obs='louvain')

# For consistent visualization, copy the color scheme for the clusters from the reference.
adata.uns['louvain_colors'] = adata_ref.uns['louvain_colors']

#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Reference Visualization ---
# Plot the UMAP of the reference data to see the original clusters.
sc.pl.umap(adata_ref, color='louvain', title='Reference UMAP with Louvain Clusters')

# --- Query Visualization ---
# Plot the UMAP of the query data. The coordinates are projected from the reference,
# and the colors represent the newly ingested labels.
sc.pl.umap(adata, color='louvain', title='Query UMAP with Ingested Annotations', size=30)

# --- Combined Visualization (Optional) ---
# Concatenate the reference and query objects to visualize them together.
# This is useful for assessing the quality of the integration.
print(""Creating a combined UMAP for visualization..."")
adata_concat = anndata.concat(
    [adata_ref, adata],
    label='batch',
    keys=['reference', 'query']
)
# The UMAP coordinates for the query part are from the ingest function.
# We re-compute the reference UMAP to ensure it's in the same object for plotting.
adata_concat.obsm['X_umap'] = np.concatenate(
    [adata_ref.obsm['X_umap'], adata.obsm['X_umap_ingest']]
)

# Plot the combined data, coloring by batch and by the transferred labels.
sc.pl.umap(
    adata_concat,
    color=['batch', 'louvain'],
    title='Combined UMAP: Reference and Query',
    wspace=0.5,
    size=20
)

#--------------------------------
# Data Output
#--------------------------------

# Save the annotated query dataset to a file.
adata.write(""ingest_annotated_query_data.h5ad"")
",https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scrna/harmonization.html,F
scRNA-seq,Integration,"The analysis for batch effect correction was performed in Python using the scvi-tools library. A batch-aware approach was first used to select highly variable genes from the raw count data. An scVI model was then trained using these genes, with the batch origin specified as a covariate, to learn an integrated latent representation. All subsequent downstream analysis, including k-nearest neighbor graph construction and UMAP embedding, was performed directly on this batch-corrected latent space. The quality of the integration was confirmed by visualizing the effective mixing of batches on the UMAP plot.","To correct for technical artifacts arising from different experimental batches, we performed an integration analysis using the single-cell Variational Inference (scVI) model implemented in the scvi-tools Python library.

The analysis began with a multi-batch single-cell lung atlas dataset. For feature selection, we identified the top 2,000 most highly variable genes (HVGs) using a batch-aware approach based on the Seurat v3 method. This selection was performed on the raw count data to ensure that the genes chosen were representative of biological variation across, rather than within, individual batches.

An scVI model was then configured using the raw counts of these HVGs, with the batch origin of each cell specified as a covariate. The model was structured as a neural network with two hidden layers, learning a 30-dimensional latent representation of the data. A negative binomial distribution was used for the gene likelihood, a choice appropriate for UMI-based count data. After training the model, the resulting batch-corrected latent representation for each cell was extracted.

Subsequent downstream analysis was performed exclusively in this integrated latent space. A k-nearest neighbor graph was constructed from the scVI latent embeddings, and a Uniform Manifold Approximation and Projection (UMAP) embedding was then computed from this graph for visualization. The quality of the integration was assessed by visualizing the UMAP embedding with cells colored by their original batch identifier, confirming the effective mixing of cells from different batches and the removal of batch-specific clustering.","os, tempfile, scanpy, scvi, seabor, torch",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for system operations, data handling, analysis, and plotting
import os
import tempfile
import scanpy as sc
import scvi
import seaborn as sns
import torch

#--------------------------------
# Script Configuration
#--------------------------------

# Set global parameters for plotting and computation
sc.set_figure_params(figsize=(6, 6), frameon=False)
sns.set_theme()
# Set precision for matrix multiplication in PyTorch for performance.
torch.set_float32_matmul_precision(""high"")

#--------------------------------
# Data Input & Preparation
#--------------------------------

# Create a temporary directory to store downloaded data
save_dir = tempfile.TemporaryDirectory()
print(f""Data will be downloaded to temporary directory: {save_dir.name}"")

# Define file path for the dataset
adata_path = os.path.join(save_dir.name, ""lung_atlas.h5ad"")

# Download and load the lung atlas dataset.
# The `backup_url` will automatically download the file if not found locally.
print(""Downloading and loading dataset..."")
adata = sc.read(adata_path, backup_url=""https://figshare.com/ndownloader/files/24539942"")
print(adata)

# Store the full, unprocessed dataset in the .raw attribute for reference.
adata.raw = adata

#--------------------------------
# Preprocessing & Feature Selection
#--------------------------------

# Identify highly variable genes (HVGs) using a batch-aware method.
# This approach finds genes that are variable across batches, which is robust
# to batch-specific effects. The `subset=True` argument modifies adata in place.
print(""Selecting highly variable genes..."")
sc.pp.highly_variable_genes(
    adata,
    flavor=""seurat_v3"",
    n_top_genes=2000,
    layer=""counts"",
    batch_key=""batch"",
    subset=True,
)

#--------------------------------
# Major Analysis Task: Batch Integration with scVI
#--------------------------------

# --- Model Setup and Training ---
# Register the AnnData object for scVI analysis. This function specifies
# the data layer, batch information, and other covariates for the model.
scvi.model.SCVI.setup_anndata(adata, layer=""counts"", batch_key=""batch"")

# Initialize the scVI model.
# `n_layers`: number of hidden layers in the neural network.
# `n_latent`: dimensionality of the latent space.
# `gene_likelihood`: ""nb"" (Negative Binomial) is recommended for UMI-based count data.
model = scvi.model.SCVI(adata, n_layers=2, n_latent=30, gene_likelihood=""nb"")

# Train the scVI model. This learns a batch-corrected latent representation of the data.
print(""Training scVI model..."")
model.train()

# --- Post-Integration Steps ---
# Retrieve the learned latent representation from the trained model.
SCVI_LATENT_KEY = ""X_scVI""
adata.obsm[SCVI_LATENT_KEY] = model.get_latent_representation()

# Compute the nearest-neighbor graph on the scVI latent space.
# This graph will be used for downstream clustering and visualization.
sc.pp.neighbors(adata, use_rep=SCVI_LATENT_KEY)

# Compute the UMAP embedding based on the integrated neighbor graph.
sc.tl.umap(adata)

#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the UMAP embedding, coloring cells by their batch ID.
# Effective integration should show cells from different batches mixing together,
# rather than forming separate, batch-specific clusters.
print(""Visualizing integrated data..."")
sc.pl.umap(adata, color=""batch"", title=""UMAP after scVI Integration"")

#--------------------------------
# Data Output
#--------------------------------

# Save the integrated AnnData object to a file.
print(""Saving integrated data object..."")
adata.write(""scvi_integrated_data.h5ad"")

# Clean up the temporary directory
save_dir.cleanup()
",https://docs.scvi-tools.org/en/stable/tutorials/notebooks/scrna/harmonization.html,F
scRNA-seq,Integration,"The analysis was performed in Python using the scANVI model from scvi-tools for semi-supervised integration. Following a batch-aware selection of highly variable genes, an scANVI model was trained. This model incorporated both batch identifiers and partial cell type annotations as covariates, enabling it to learn a batch-corrected latent space structured by the known biological labels. All subsequent analyses, including neighborhood graph construction and UMAP embedding, were performed in this integrated space. The quality of the integration was confirmed by visualizing the effective mixing of batches while preserving the separation of distinct cell types on the UMAP plot.","To perform a semi-supervised integration that corrects for technical batch effects while leveraging partial cell type annotations, we utilized the scANVI model from the scvi-tools library in Python.

The analysis was performed on a multi-batch single-cell lung atlas dataset, which contained pre-existing cell type labels for a subset of the cells. As a preliminary step, feature selection was conducted by identifying the top 2,000 most highly variable genes (HVGs) using a batch-aware implementation of the Seurat v3 method on the raw count data.

The scANVI model was then configured, specifying the batch origin and the partial cell type annotations as inputs. Critically, cells with the label ""Unknown"" were designated as the unlabeled set, enabling the semi-supervised learning approach. The model was trained for 20 epochs, using a strategy that samples a maximum of 100 cells per known label in each training step. This process learns a batch-corrected latent space that is simultaneously structured by the provided cell type information.

Following training, the resulting integrated latent representation was extracted for each cell. All subsequent analyses were performed in this scANVI latent space. A k-nearest neighbor graph was constructed, which then served as the basis for computing a two-dimensional UMAP embedding with a `min_dist` parameter of 0.3. The quality of the integration was assessed by visualizing the final UMAP, where cells were colored by their cell type, to confirm that the model successfully merged disparate batches while maintaining the separation of biologically distinct cell populations.","os, tempfile, scanpy, scvi, seabor, torch",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for system operations, data handling, analysis, and plotting
import os
import tempfile
import scanpy as sc
import scvi
import seaborn as sns
import torch
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set global parameters for plotting and computation
sc.set_figure_params(figsize=(6, 6), frameon=False)
sns.set_theme()
# Set precision for matrix multiplication in PyTorch for performance.
torch.set_float32_matmul_precision(""high"")

#--------------------------------
# Data Input & Preparation
#--------------------------------

# Create a temporary directory to store the downloaded dataset
save_dir = tempfile.TemporaryDirectory()
print(f""Data will be downloaded to temporary directory: {save_dir.name}"")

# Define file path for the dataset
adata_path = os.path.join(save_dir.name, ""lung_atlas.h5ad"")

# Download and load the lung atlas dataset.
# The `backup_url` will automatically download the file if not found locally.
print(""Downloading and loading dataset..."")
adata = sc.read(adata_path, backup_url=""https://figshare.com/ndownloader/files/24539942"")
print(adata)

# Store the full, unprocessed dataset in the .raw attribute for reference.
adata.raw = adata

#--------------------------------
# Preprocessing & Feature Selection
#--------------------------------

# Identify highly variable genes (HVGs) using a batch-aware method.
# This helps focus on biological signal and reduces technical noise.
# The `subset=True` argument modifies adata in place to keep only HVGs.
print(""Selecting highly variable genes..."")
sc.pp.highly_variable_genes(
    adata,
    flavor=""seurat_v3"",
    n_top_genes=2000,
    layer=""counts"",
    batch_key=""batch"",
    subset=True,
)

#--------------------------------
# Major Analysis Task: Semi-Supervised Integration with scANVI
#--------------------------------

# --- Model Setup and Training ---
# Register the AnnData object for scANVI analysis. This function specifies
# the data layer, batch information, and the key for cell type labels.
scvi.model.SCANVI.setup_anndata(adata, labels_key=""cell_type"", batch_key=""batch"")

# Initialize the scANVI model. scANVI extends scVI for semi-supervised tasks.
# It uses both labeled and unlabeled data for training.
# `unlabeled_category=""Unknown""` tells the model which label to treat as unlabeled.
scanvi_model = scvi.model.SCANVI(adata, unlabeled_category=""Unknown"")

# Train the scANVI model. This refines the latent space using the cell type labels.
print(""Training scANVI model..."")
scanvi_model.train(max_epochs=20, n_samples_per_label=100)

# --- Post-Integration Steps ---
# Retrieve the learned latent representation from the trained model.
SCANVI_LATENT_KEY = ""X_scANVI""
adata.obsm[SCANVI_LATENT_KEY] = scanvi_model.get_latent_representation()

# Compute the nearest-neighbor graph on the scANVI latent space.
# This integrated graph is used for downstream visualization.
sc.pp.neighbors(adata, use_rep=SCANVI_LATENT_KEY)

# Compute the UMAP embedding based on the integrated neighbor graph.
sc.tl.umap(adata, min_dist=0.3)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the UMAP embedding, coloring cells by their annotated cell type.
# This plot shows how well scANVI has integrated the batches while preserving cell identities.
print(""Visualizing integrated data..."")
sc.pl.umap(adata, color=[""cell_type""], frameon=False, ncols=1)


#--------------------------------
# Data Output
#--------------------------------

# Save the integrated and annotated AnnData object to a file.
print(""Saving integrated data object..."")
adata.write(""scanvi_integrated_data.h5ad"")

# Clean up the temporary directory
save_dir.cleanup()
",https://www.sc-best-practices.org/cellular_structure/integration.html,F
scRNA-seq,Integration,"The analysis was performed in Python using Scanpy and the BBKNN algorithm for batch correction. Following a standard preprocessing workflow of normalization, feature selection, and scaling, Principal Component Analysis was performed for dimensionality reduction. The BBKNN algorithm was then applied to the PCA representation to construct an integrated neighborhood graph that balances connections within and across batches. For visualization, both UMAP and t-SNE embeddings were computed from this corrected graph. The effective mixing of batches on these embeddings confirmed successful integration.","To correct for technical variability between experimental batches, we performed an integration analysis using the **Batch-Balanced K-Nearest Neighbors (BBKNN)** algorithm in Python, implemented via the Scanpy and BBKNN libraries.

The analysis was demonstrated on a standard PBMC dataset, which was artificially divided into two batches for this workflow. The data was first preprocessed using a standard pipeline. This included library size normalization, where each cell’s total counts were scaled to 10,000, followed by a log-plus-one transformation. Feature selection was then performed to identify the top 2,000 most highly variable genes (HVGs) using the Seurat method. The expression values of these genes were subsequently scaled to a maximum value of 10 and centered. Dimensionality was reduced by performing **Principal Component Analysis (PCA)** and retaining the top 50 components.

Batch integration was carried out on the PCA-reduced data. The **BBKNN algorithm** was used to construct an integrated neighborhood graph. This method identifies nearest neighbors for each cell first within its own batch, using a parameter of `neighbors_within_batch` set to 3, before connecting to cells across batches. This process creates a balanced graph that mitigates batch-specific clustering. For visualization, both **Uniform Manifold Approximation and Projection (UMAP)** and **t-SNE** embeddings were computed from this BBKNN-corrected neighborhood graph. The effectiveness of the integration was evaluated by coloring the cells in these embeddings by their original batch label to confirm successful mixing of the two simulated batches.","scanpy, bbknn, numpy, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis, batch correction, and plotting
import scanpy as sc
import bbknn
import numpy as np
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters and logging verbosity for clear output
sc.settings.set_figure_params(dpi=80, facecolor='white')
sc.settings.verbosity = 2

#--------------------------------
# Data Input & Simulation
#--------------------------------

# Load the built-in PBMC3k dataset as an example
adata = sc.datasets.pbmc3k()

# Simulate two batches by splitting the dataset in half.
# This is for demonstration purposes; in a real scenario, the 'batch'
# column would come from the experimental design.
print(""Simulating two batches for demonstration..."")
num_cells = adata.n_obs
batch_labels = np.array([""batch1""] * num_cells)
batch_labels[int(num_cells / 2):] = ""batch2""
adata.obs[""batch""] = batch_labels

#--------------------------------
# Preprocessing & Dimensionality Reduction
#--------------------------------

# --- Normalization and Feature Selection ---
# Normalize each cell to a target count of 10,000 and apply a log transformation.
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

# Identify highly variable genes (HVGs) to focus on biological variation.
sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat')
adata = adata[:, adata.var.highly_variable].copy()

# --- Scaling and PCA ---
# Scale the data to have zero mean and unit variance.
sc.pp.scale(adata, max_value=10)

# Perform Principal Component Analysis (PCA) for dimensionality reduction.
# The PCs will be used as input for the BBKNN algorithm.
sc.tl.pca(adata, svd_solver='arpack', n_comps=50)

#--------------------------------
# Major Analysis Task: Batch Integration with BBKNN
#--------------------------------

# --- Batch Integration ---
# Run Batch-Balanced K-Nearest Neighbors (BBKNN) to correct for batch effects.
# This function computes a neighborhood graph by finding neighbors within each
# batch separately before merging them into a single, integrated graph.
print(""\nRunning BBKNN for batch integration..."")
bbknn.bbknn(adata, batch_key='batch', n_pcs=50, neighbors_within_batch=3)

# --- Post-Integration Embeddings ---
# Compute UMAP and t-SNE embeddings based on the BBKNN-corrected neighborhood graph.
# These will be used for visualizing the integrated data.
sc.tl.umap(adata)
sc.tl.tsne(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Plot the UMAP embedding, coloring cells by their batch label.
# Successful integration should show cells from 'batch1' and 'batch2' mixing well.
sc.pl.umap(adata, color=[""batch""], title=""UMAP: BBKNN Integrated"", size=30)

# Plot the t-SNE embedding, also colored by batch, as an alternative visualization.
sc.pl.tsne(adata, color=[""batch""], title=""t-SNE: BBKNN Integrated"", size=30)


#--------------------------------
# Data Output
#--------------------------------

# Save the integrated AnnData object to a file.
# This object can now be used for downstream analysis like clustering.
print(""\nSaving integrated data object..."")
adata.write(""pbmc3k_bbknn_integrated.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/basics/integrating-data-using-ingest.html,F
scRNA-seq,Integration,"The analysis was performed in Python using Scanpy and the BBKNN algorithm for batch correction. Following a standard preprocessing workflow of normalization, feature selection, and scaling, Principal Component Analysis was performed for dimensionality reduction. The BBKNN algorithm was then applied to the PCA representation to construct an integrated neighborhood graph that balances connections within and across batches. For visualization, both UMAP and t-SNE embeddings were computed from this corrected graph. The effective mixing of batches on these embeddings confirmed successful integration.","To correct for experimental batch effects, an integration analysis was performed in Python using the **Scanpy** and **BBKNN** libraries.

The workflow was demonstrated using a standard PBMC dataset, which was artificially split into two batches. The data was preprocessed through a standard pipeline, including library size normalization via scaling total counts to 10,000, followed by a log-plus-one transformation. Feature selection was performed to identify the top 2,000 most **highly variable genes (HVGs)** using the Seurat method. The expression values of these genes were then scaled to a maximum value of 10 and centered. For dimensionality reduction, **Principal Component Analysis (PCA)** was performed, and the top 50 principal components were retained.

Batch integration was performed on this PCA-reduced data using a graph-based correction method, **Batch-Balanced K-Nearest Neighbors (BBKNN)**. This algorithm constructs a unified neighborhood graph by first identifying nearest neighbors within each batch (using `neighbors_within_batch` set to 3) before connecting across batches, thereby mitigating batch-specific artifacts. For visualization, both **Uniform Manifold Approximation and Projection (UMAP)** and **t-SNE** embeddings were computed from the resulting BBKNN-corrected graph. The success of the integration was confirmed by visualizing these embeddings with cells colored by their batch of origin, which demonstrated robust mixing between the two simulated batches.","scanpy, matplotlib, numpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis, batch correction, and plotting
import scanpy as sc
import bbknn
import numpy as np
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters and logging verbosity for clear output
sc.settings.set_figure_params(dpi=80, facecolor='white')
sc.settings.verbosity = 2

#--------------------------------
# Data Input & Simulation
#--------------------------------

# Load the built-in PBMC3k dataset as an example
adata = sc.datasets.pbmc3k()

# Simulate two batches by splitting the dataset in half.
# This is for demonstration purposes; in a real scenario, the 'batch'
# column would come from the experimental design.
print(""Simulating two batches for demonstration..."")
num_cells = adata.n_obs
batch_labels = np.array([""batch1""] * num_cells)
batch_labels[int(num_cells / 2):] = ""batch2""
adata.obs[""batch""] = batch_labels

#--------------------------------
# Preprocessing & Dimensionality Reduction
#--------------------------------

# --- Normalization and Feature Selection ---
# Normalize each cell to a target count of 10,000 and apply a log transformation.
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

# Identify highly variable genes (HVGs) to focus on biological variation.
sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor='seurat')
adata = adata[:, adata.var.highly_variable]

# --- Scaling and PCA ---
# Scale the data to have zero mean and unit variance.
sc.pp.scale(adata, max_value=10)

# Perform Principal Component Analysis (PCA) for dimensionality reduction.
# The PCs will be used as input for the BBKNN algorithm.
sc.tl.pca(adata, svd_solver='arpack', n_comps=50)

#--------------------------------
# Major Analysis Task: Batch Integration with BBKNN
#--------------------------------

# --- Batch Integration ---
# Run Batch-Balanced K-Nearest Neighbors (BBKNN) to correct for batch effects.
# This function computes a neighborhood graph by finding neighbors within each
# batch separately before merging them into a single, integrated graph.
print(""Running BBKNN for batch integration..."")
bbknn.bbknn(adata, batch_key='batch', n_pcs=50, neighbors_within_batch=3)

# --- Post-Integration Embeddings ---
# Compute UMAP and t-SNE embeddings based on the BBKNN-corrected neighborhood graph.
# These will be used for visualizing the integrated data.
sc.tl.umap(adata)
sc.tl.tsne(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Plot the UMAP embedding, coloring cells by their batch label.
# Successful integration should show cells from 'batch1' and 'batch2' mixing well.
sc.pl.umap(adata, color=[""batch""], title=""UMAP: BBKNN Integrated"", size=30)

# Plot the t-SNE embedding, also colored by batch, as an alternative visualization.
sc.pl.tsne(adata, color=[""batch""], title=""t-SNE: BBKNN Integrated"", size=30)


#--------------------------------
# Data Output
#--------------------------------

# Save the integrated AnnData object to a file.
# This object can now be used for downstream analysis like clustering.
print(""Saving integrated data object..."")
adata.write(""pbmc3k_bbknn_integrated.h5ad"")
",https://www.sc-best-practices.org/cellular_structure/integration.html,F
scRNA-seq,Integration,"The analysis was performed in Python using Scanpy to correct for batch effects with the Mutual Nearest Neighbors (MNN) method. Each batch was preprocessed independently, including normalization and selection of highly variable genes. The MNN algorithm was then used to integrate the datasets by identifying mutual nearest neighbor pairs, resulting in a single, batch-corrected data object. Subsequent dimensionality reduction, including PCA and UMAP embedding, was performed on this integrated data. The successful mixing of batches on the final UMAP plot confirmed the effectiveness of the correction.","To correct for batch effects, we performed an integration analysis using the **Mutual Nearest Neighbors (MNN)** correction method in Python, as implemented in the Scanpy library.

The workflow was demonstrated using a standard PBMC dataset, which was artificially split into two separate batches. Each batch was preprocessed independently to avoid biases from batch-specific differences. This individual preprocessing included library size normalization by scaling each cell’s total counts to 10,000, followed by a log-plus-one transformation. Within each batch, the top 2,000 most **highly variable genes (HVGs)** were identified to focus the analysis on sources of biological variation.

The two preprocessed batch-specific datasets were then integrated using the MNN correction algorithm. This method identifies pairs of cells (mutual nearest neighbors) across batches that are transcriptionally similar, using these pairs as anchors to align the datasets and correct for technical differences in gene expression. This process resulted in a single, new data object with batch-corrected expression values.

Subsequent dimensionality reduction was performed on this integrated dataset. **Principal Component Analysis (PCA)** was run on the corrected data, followed by the construction of a k-nearest neighbor graph using the top 50 principal components. A **Uniform Manifold Approximation and Projection (UMAP)** embedding was then computed from this integrated graph for visualization. The effectiveness of the batch correction was assessed by visualizing the UMAP embedding with cells colored by their original batch, which confirmed the successful mixing of the two batches into coherent cell populations.","scanpy, matplotlib, numpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis, batch correction, and plotting
import scanpy as sc
import scanpy.external as sce
import matplotlib.pyplot as plt
import numpy as np

#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters for clear and consistent visualization
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input & Simulation
#--------------------------------

# Load an example dataset (PBMC 3k) provided by Scanpy
adata = sc.datasets.pbmc3k()

# Simulate two batches by randomly assigning cells to one of two groups.
# In a real-world scenario, this 'batch' information would come from the experiment metadata.
print(""Simulating two batches for demonstration..."")
np.random.seed(42)
adata.obs['batch'] = np.random.choice(['batch1', 'batch2'], size=adata.n_obs)

# Split the main AnnData object into two separate objects, one for each batch.
# MNN correction is typically performed on a list of batch-specific objects.
adata_batch1 = adata[adata.obs['batch'] == 'batch1'].copy()
adata_batch2 = adata[adata.obs['batch'] == 'batch2'].copy()


#--------------------------------
# Preprocessing
#--------------------------------

# Preprocess each batch individually. This is a common strategy before integration.
print(""Preprocessing each batch individually..."")
for ad in [adata_batch1, adata_batch2]:
    # 1. Normalize total counts per cell to account for library size differences.
    sc.pp.normalize_total(ad, target_sum=1e4)
    # 2. Log-transform the data to stabilize variance.
    sc.pp.log1p(ad)
    # 3. Identify highly variable genes to focus on biological signal.
    sc.pp.highly_variable_genes(ad, n_top_genes=2000, subset=True)


#--------------------------------
# Major Analysis Task: Batch Integration with MNN
#--------------------------------

# --- MNN Integration ---
# Integrate the two batches using Mutual Nearest Neighbors (MNN) correction.
# This method identifies shared cell populations across batches and aligns them.
# The function returns a new, integrated AnnData object.
print(""Running MNN integration..."")
adata_corrected, _ = sce.pp.mnn_correct(adata_batch1, adata_batch2, batch_key='batch')

# --- Post-Integration Steps ---
# Perform PCA on the MNN-corrected data to find the main axes of variation.
sc.tl.pca(adata_corrected, svd_solver='arpack')

# Construct the neighborhood graph on the integrated PCA space.
sc.pp.neighbors(adata_corrected, n_pcs=50)

# Compute the UMAP embedding for visualization.
sc.tl.umap(adata_corrected)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Plot the UMAP embedding of the corrected data, coloring by batch.
# Successful integration should result in cells from 'batch1' and 'batch2'
# mixing together within cell type clusters.
print(""Visualizing integrated data..."")
sc.pl.umap(adata_corrected, color='batch', title='MNN Integrated UMAP')


#--------------------------------
# Data Output
#--------------------------------

# Save the integrated AnnData object to a file.
# This object is now ready for downstream analyses like clustering.
print(""Saving integrated data object..."")
adata_corrected.write(""pbmc3k_mnn_integrated.h5ad"")
",https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.harmony_integrate.html#scanpy.external.pp.harmony_integrate,F
scRNA-seq,Integration,"The analysis was performed in R with Seurat to integrate datasets from different experimental conditions. Each dataset was preprocessed independently, including normalization and selection of highly variable genes. An anchor-based integration workflow was then employed, first identifying anchor cells that represent shared biological states across conditions. These anchors were used to create a single, integrated expression matrix for all subsequent analysis. Downstream, this integrated data was scaled, dimensionality was reduced via PCA and UMAP, and graph-based clustering was performed. The effective mixing of conditions within cell clusters on the UMAP plot confirmed successful integration.","To integrate single-cell datasets from different experimental conditions, an analysis was performed in R using the Seurat package.

The workflow began by loading two separate 10x Genomics datasets: one from a control condition and one from a stimulated condition. Each dataset was initialized as an individual Seurat object, with preliminary filters applied to remove genes expressed in fewer than three cells and cells with fewer than 200 detected features.

Prior to integration, each dataset was preprocessed independently. This involved normalizing the data for library size differences using the `LogNormalize` method with a scale factor of 10,000, followed by the identification of the top 2,000 most highly variable genes (HVGs) in each dataset using the variance-stabilizing transformation (`vst`) method.

The core data integration was performed using Seurat's anchor-based integration workflow. First, `FindIntegrationAnchors` was used to identify ""anchors""—pairs of cells in a shared low-dimensional space (defined by the first 20 principal components) that are mutual nearest neighbors across the two conditions. These anchors represent shared biological states and are used to align the datasets. Subsequently, the `IntegrateData` function used these anchors to create a single, batch-corrected expression matrix. The analysis was then switched to use this newly created ""integrated"" assay for all downstream steps.

Following integration, the combined data was scaled, and dimensionality was reduced by performing Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP) on the first 20 principal components of the integrated data. Cell clusters were then identified on this integrated representation using a graph-based clustering approach with a resolution of 0.5. The success of the integration was visually assessed by generating a UMAP plot where cells were colored by their original experimental condition, confirming that cells from both the control and stimulated groups were well-mixed within the identified cell clusters.","Seurat, dplyr, ggplot2, patchwork",R,"#--------------------------------
# Package Load
#--------------------------------

# Load necessary libraries for single-cell analysis, data manipulation, and plotting
library(Seurat)      # For single-cell RNA-seq analysis and integration
library(dplyr)       # For data manipulation
library(ggplot2)     # For creating custom, publication-quality plots
library(patchwork)   # For easily combining multiple ggplot2 plots

#--------------------------------
# Data Input
#--------------------------------

# Define file paths for the Control and Stimulated datasets.
# NOTE: Replace these with the actual paths to your data directories.
data_dir_control <- ""path/to/control_data/""
data_dir_stim    <- ""path/to/stimulated_data/""

# Read the raw 10X Genomics data for each condition.
data_control <- Read10X(data.dir = data_dir_control)
data_stim    <- Read10X(data.dir = data_dir_stim)

# Create individual Seurat objects for each dataset with basic filtering.
control <- CreateSeuratObject(counts = data_control, project = ""Control"", min.cells = 3, min.features = 200)
stim    <- CreateSeuratObject(counts = data_stim, project = ""Stimulated"", min.cells = 3, min.features = 200)

#--------------------------------
# Preprocessing of Individual Datasets
#--------------------------------

# It's a standard practice to preprocess each dataset individually before integration.
# Here, we normalize and find variable features for both datasets.
object_list <- list(control, stim)
for (i in 1:length(object_list)) {
    object_list[[i]] <- NormalizeData(object_list[[i]], normalization.method = ""LogNormalize"", scale.factor = 10000)
    object_list[[i]] <- FindVariableFeatures(object_list[[i]], selection.method = ""vst"", nfeatures = 2000)
}

#--------------------------------
# Major Analysis Task: Data Integration
#--------------------------------

# --- Find Integration Anchors ---
# Identify ""anchors"" or mutual nearest neighbors between the datasets. These anchors
# represent shared cellular states across different conditions.
# `dims = 1:20` specifies that the first 20 PCs are used for finding anchors.
integration_anchors <- FindIntegrationAnchors(object.list = object_list, dims = 1:20)

# --- Integrate Data ---
# Use the identified anchors to create an integrated (batch-corrected) data matrix.
integrated_data <- IntegrateData(anchorset = integration_anchors, dims = 1:20)

# Set the default assay to ""integrated"" to ensure that downstream analysis
# (like scaling, PCA, UMAP) is performed on the corrected data.
DefaultAssay(integrated_data) <- ""integrated""


#--------------------------------
# Post-Integration Analysis
#--------------------------------

# --- Scaling and Dimensionality Reduction ---
# Scale the integrated data. This is necessary for PCA.
integrated_data <- ScaleData(integrated_data, verbose = FALSE)

# Run PCA on the scaled, integrated data to reduce dimensionality.
integrated_data <- RunPCA(integrated_data, npcs = 30, verbose = FALSE)

# Run UMAP for non-linear dimensionality reduction and visualization.
integrated_data <- RunUMAP(integrated_data, reduction = ""pca"", dims = 1:20)

# --- Clustering (Optional) ---
# Perform clustering on the integrated data to identify cell populations.
integrated_data <- FindNeighbors(integrated_data, reduction = ""pca"", dims = 1:20)
integrated_data <- FindClusters(integrated_data, resolution = 0.5)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Generate a UMAP plot, coloring cells by their original condition ('orig.ident').
# Successful integration should show cells from both conditions mixing well within clusters.
DimPlot(integrated_data, reduction = ""umap"", group.by = ""orig.ident"", label = TRUE) +
  ggtitle(""UMAP Plot: Integrated Data by Condition"")


#--------------------------------
# Data Output
#--------------------------------

# Save the final, integrated Seurat object for future analyses.
saveRDS(integrated_data, file = ""Integrated_Seurat_Object.rds"")
",https://satijalab.org/seurat/articles/integration_introduction#perform-integration,F
scRNA-seq,Trajectory inference,"The analysis was performed in Python using Scanpy to infer developmental trajectories with Partition-based Graph Abstraction (PAGA). Following standard preprocessing and dimensionality reduction, cells were partitioned into discrete states using the Louvain clustering algorithm. PAGA was then applied to create an abstracted graph where nodes represent cell clusters and edges represent connectivity. For visualization, a PAGA-initialized force-directed layout was generated, ensuring the single-cell embedding reflects the global topological structure inferred by PAGA.","To infer developmental trajectories from single-cell data, we performed an analysis using the **Partition-based Graph Abstraction (PAGA)** method, implemented in the Scanpy Python library.

The analysis was conducted on the Paul et al. (2015) dataset, a benchmark for hematopoietic differentiation. The data was first processed using a standard workflow that included library size normalization, log transformation, and the selection of highly variable genes. The dimensionality of the data was subsequently reduced by **Principal Component Analysis (PCA)**, and a k-nearest neighbor graph was constructed using the top 20 principal components and `k=10` neighbors.

To define discrete cellular states, cells were first partitioned into clusters using the **Louvain algorithm** with a resolution of 1.0. The PAGA algorithm was then applied to these clusters to create an abstracted graph representation of the data. In this graph, each node corresponds to a cell cluster, and the weighted edges represent a statistical measure of connectivity between them.

For visualization, a two-dimensional embedding was generated using the Fruchterman-Reingold layout algorithm (`draw_graph`), with the initial positions of the cell clusters guided by the PAGA graph topology. This PAGA-initialization ensures that the final single-cell layout reflects the global connectivity structure identified by the PAGA analysis. The abstracted PAGA graph was visualized with an edge-weight threshold of 0.03 to prune weak connections, and the final continuous trajectory was visualized on the PAGA-initialized single-cell embedding.","scanpy, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis and plotting
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set global settings for clear visualization and detailed logging
sc.settings.verbosity = 3
sc.settings.set_figure_params(dpi=80, facecolor='white')

#--------------------------------
# Data Input
#--------------------------------

# Load the Paul et al. (2015) dataset, a common benchmark for trajectory inference.
adata = sc.datasets.paul15()

# Ensure the data matrix is in a float format for numerical stability.
adata.X = adata.X.astype('float64')

#--------------------------------
# Preprocessing
#--------------------------------

# Apply a standard preprocessing recipe. This includes normalization per cell,
# log transformation, and selection of highly variable genes.
print(""Preprocessing data..."")
sc.pp.recipe_zheng17(adata)

# Perform PCA for dimensionality reduction.
sc.tl.pca(adata, svd_solver='arpack')

# Compute the k-nearest neighbor graph, which is required for clustering and PAGA.
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=20)


#--------------------------------
# Major Analysis Task: Trajectory Inference
#--------------------------------

# --- Clustering ---
# Cluster cells using the Louvain algorithm. The resulting clusters will be used
# to build the abstracted graph in PAGA.
print(""Clustering cells..."")
sc.tl.louvain(adata, resolution=1.0)

# --- PAGA (Partition-based Graph Abstraction) ---
# Compute the PAGA graph, which represents the connectivity between cell clusters.
# This provides a high-level, abstracted view of the developmental trajectory.
print(""Computing PAGA graph..."")
sc.tl.paga(adata, groups='louvain')

# --- UMAP Initialization for Visualization ---
# Compute a UMAP-like embedding, but initialize the positions using the PAGA graph.
# This helps the final visualization better reflect the global topology inferred by PAGA.
sc.tl.draw_graph(adata, init_pos='paga')


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Plot the PAGA abstracted graph. Edges represent the likelihood of a connection
# between clusters. The `threshold` prunes weak connections for clarity.
print(""Visualizing PAGA graph and trajectory..."")
sc.pl.paga(
    adata,
    color=['louvain'],
    threshold=0.03,
    title='PAGA Graph for Trajectory Inference'
)

# Plot the single-cell embedding initialized with PAGA.
# This layout shows the fine-grained trajectory of individual cells, guided by the
# coarse-grained PAGA structure.
sc.pl.draw_graph(
    adata,
    color='louvain',
    legend_loc='on data',
    title='Trajectory Inference via PAGA Initialization'
)


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object, which now contains the PAGA results and trajectory layout.
print(""Saving trajectory data object..."")
adata.write(""paul15_paga_trajectory.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/trajectories/paga-paul15.html,F
scRNA-seq,Quality control,"The single-cell data was preprocessed in Python using Scanpy. The workflow began with quality control, where cells were filtered based on fixed thresholds for mitochondrial gene percentage, total counts, and the number of detected genes. Following normalization for sequencing depth and a logarithmic transformation, feature selection was performed. Highly variable genes were identified by modeling their mean expression against normalized dispersion. The effectiveness of these quality control and preprocessing steps was confirmed through visualizations, including violin and scatter plots.","To prepare the single-cell RNA sequencing data for analysis, we performed a comprehensive quality control and preprocessing workflow using the Scanpy library in Python. The analysis began with the loading of a raw gene-barcode matrix from a 10x Genomics PBMC dataset.

For quality control, we first calculated several metrics for each cell. Mitochondrial gene content was quantified by identifying genes with the ""MT-"" prefix and calculating the percentage of total counts derived from these genes. Cells exhibiting signs of low quality or technical artifacts were then removed based on a series of filtering criteria. Specifically, we excluded cells with a mitochondrial gene percentage greater than 5%, a total UMI count exceeding 5,000, or more than 2,500 detected genes. Following this cell-level filtering, any genes that were no longer expressed in the remaining cell population were also removed from the dataset.

The filtered data was subsequently preprocessed to account for technical variability. First, the data was normalized for sequencing depth by scaling the total counts in each cell to a target of 10,000. A natural log-plus-one transformation was then applied to stabilize the variance across genes with different expression levels. For feature selection, we identified the top 2,000 most highly variable genes (HVGs) using the 'seurat' method, which models the relationship between a gene's mean expression and its normalized dispersion. The distributions of the quality control metrics and the results of the HVG selection were visually inspected using violin and scatter plots, respectively, to confirm the effectiveness of the preprocessing steps.","numpy, matplotlib, scanpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for numerical operations, plotting, and single-cell analysis
import numpy as np
import matplotlib.pyplot as plt
import scanpy as sc

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 3
sc.settings.set_figure_params(dpi=80, facecolor=""white"")

#--------------------------------
# Data Input
#--------------------------------

# Load the dataset from a 10X Genomics matrix directory.
# NOTE: Replace the path with the location of your pbmc3k_v1 data directory.
# `cache=True` will save a processed version of the data for faster loading next time.
adata = sc.read_10x_mtx(""tutorial_data/pbmc3k_v1/"", cache=True)


#--------------------------------
# Quality Control (QC) and Filtering
#--------------------------------

# --- QC Metrics Calculation ---
# Identify mitochondrial genes, which are often a sign of cell stress if highly expressed.
adata.var[""mt""] = adata.var_names.str.startswith(""MT-"")

# Calculate standard quality control metrics.
sc.pp.calculate_qc_metrics(
    adata, qc_vars=[""mt""], percent_top=None, log1p=False, inplace=True
)

# --- Filtering ---
# Define outlier cells based on QC metrics. These thresholds may need adjustment
# depending on the specific dataset.
adata.obs[""outlier_mt""] = adata.obs.pct_counts_mt > 5
adata.obs[""outlier_total""] = adata.obs.total_counts > 5000
adata.obs[""outlier_ngenes""] = adata.obs.n_genes_by_counts > 2500

# Print the number of outliers detected for each metric.
print(f""Found {sum(adata.obs['outlier_mt'])} cells with high mitochondrial content."")
print(f""Found {sum(adata.obs['outlier_total'])} cells with large total counts."")
print(f""Found {sum(adata.obs['outlier_ngenes'])} cells with a large number of genes."")

# Remove outlier cells from the dataset based on the flags defined above.
# The `~` operator inverts the boolean selection.
adata = adata[~adata.obs[""outlier_mt""], :]
adata = adata[~adata.obs[""outlier_total""], :]
adata = adata[~adata.obs[""outlier_ngenes""], :]

# Filter out genes that are no longer expressed in any cells after cell filtering.
sc.pp.filter_genes(adata, min_cells=1)


#--------------------------------
# Preprocessing & Feature Selection
#--------------------------------

# Normalize each cell to a target count of 10,000 and apply a log transformation.
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

# Identify highly variable genes (HVGs). This step calculates mean expression
# and residual variances, which are used in the plot below.
sc.pp.highly_variable_genes(adata, flavor=""seurat"", n_top_genes=2000)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- QC Plots ---
# Plot violin plots of the main QC metrics to visualize their distributions.
sc.pl.violin(
    adata,
    [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],
    jitter=0.4,
    multi_panel=True,
)

# --- HVG Plot ---
# Create a scatter plot to visualize highly variable genes.
# This helps confirm that the feature selection has worked as expected.
fig, ax = plt.subplots(figsize=(6, 6))
hvgs = adata.var[""highly_variable""]

# Plot all genes in grey
ax.scatter(
    adata.var[""means""], adata.var[""dispersions_norm""], s=3, edgecolor=""none""
)
# Highlight the selected highly variable genes in red
ax.scatter(
    adata.var[""means""][hvgs],
    adata.var[""dispersions_norm""][hvgs],
    c=""tab:red"",
    label=""selected genes"",
    s=3,
    edgecolor=""none"",
)
ax.set_xscale(""log"")
ax.set_yscale(""log"")
ax.set_xlabel(""Mean Expression"")
ax.set_ylabel(""Normalized Dispersion"")
ax.set_title(""Highly Variable Genes"")
plt.legend()
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the filtered and processed AnnData object to a file.
# This object is ready for downstream analysis like dimensionality reduction and clustering.
print(""Saving filtered data object..."")
adata.write(""pbmc3k_filtered_hvg.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/experimental/pearson_residuals.html,F
scRNA-seq,Feature selection,"To identify biologically informative genes, feature selection was performed in Python using Scanpy's implementation of the analytic Pearson residuals method. The workflow involved modeling the technical relationship between gene expression mean and variance. Pearson residuals were then calculated for each gene, quantifying the deviation from the expected technical noise. A subset of genes with the highest residual variance was selected as highly variable. This selection was visually confirmed on a mean-variance plot, and the data was subsequently subset to these features for downstream analysis.","To identify biologically informative genes for downstream analysis, we performed feature selection using the **analytic Pearson residuals** method, implemented in the Python library Scanpy.

The workflow began with a raw gene-barcode matrix from a 10x Genomics experiment. Initial quality control metrics, such as mitochondrial gene content, were computed for all cells. The core of the feature selection involved modeling the technical relationship between the mean expression and variance of each gene across all cells. The Pearson residual for each gene was then calculated, which quantifies how much its observed variance deviates from the variance predicted by the technical model. The top 2,000 genes with the highest residual variance were selected as **highly variable genes (HVGs)**. This method effectively identifies genes with variability exceeding what is expected from technical noise alone, thereby enriching for genes driving biological heterogeneity. The selection was visually confirmed by plotting the mean expression against the residual variance for all genes. Finally, the data object was subset to retain only these selected HVGs for subsequent dimensionality reduction and clustering.","numpy, matplotlib, scanpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for numerical operations, plotting, and single-cell analysis
import numpy as np
import matplotlib.pyplot as plt
import scanpy as sc

#--------------------------------
# Script Configuration
#--------------------------------

# Set Scanpy global parameters for plotting and verbosity
sc.settings.verbosity = 3  # Show detailed logging
sc.settings.set_figure_params(dpi=80, facecolor=""white"")

#--------------------------------
# Data Input
#--------------------------------

# Load the dataset from a 10X Genomics formatted directory.
# NOTE: Replace the path with the location of your data directory.
adata = sc.read_10x_mtx(""data/10x_genomics_directory/"", cache=True)


#--------------------------------
# Quality Control (QC)
#--------------------------------

# Identify mitochondrial genes for potential QC filtering later on.
adata.var['mt'] = adata.var_names.str.startswith(""MT-"")

# Calculate basic quality control metrics for each cell.
sc.pp.calculate_qc_metrics(adata, qc_vars=[""mt""], percent_top=None, inplace=True)


#--------------------------------
# Major Analysis Task: Feature Selection
#--------------------------------

# Identify highly variable genes using the Pearson residuals method.
# This experimental function models technical variability and selects genes
# that deviate significantly, which are likely biologically variable.
sc.experimental.pp.highly_variable_genes(
    adata, flavor=""pearson_residuals"", n_top_genes=2000
)

# Print the number of highly variable genes found for confirmation.
print(f""Number of highly variable genes selected: {sum(adata.var['highly_variable'])}"")


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Create a scatter plot to visualize the feature selection results.
# The plot shows mean expression vs. the residual variance calculated by the model.
fig, ax = plt.subplots(figsize=(6, 6))
hvgs = adata.var[""highly_variable""]

# Plot all genes in grey
ax.scatter(
    adata.var[""mean_counts""],
    adata.var[""residual_variances""],
    s=3,
    c='gray',
    label='All genes',
    alpha=0.5
)
# Highlight the selected highly variable genes in red
ax.scatter(
    adata.var[""mean_counts""][hvgs],
    adata.var[""residual_variances""][hvgs],
    s=3,
    c='red',
    label='Highly variable genes'
)

ax.set_xscale(""log"")
ax.set_yscale(""log"")
ax.set_xlabel(""Mean Expression"")
ax.set_ylabel(""Residual Variance"")
ax.set_title(""Feature Selection: Pearson Residuals"")
plt.legend()
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Subset the AnnData object to keep only the highly variable genes.
adata = adata[:, adata.var[""highly_variable""]]

# Save the processed AnnData object to a file.
# This object is ready for downstream analysis like dimensionality reduction.
print(""Saving filtered data object..."")
adata.write(""pearson_residuals_hvg.h5ad"")
",https://scanpy.readthedocs.io/en/stable/tutorials/experimental/pearson_residuals.html,F
scRNA-seq,Trajectory inference,"The analysis was performed in Python using Scanpy to infer developmental trajectories via Diffusion Pseudotime (DPT). Following standard preprocessing and clustering to define discrete cell states, a diffusion map was computed to model cellular transitions. A root cell was identified to define the origin of the process, and DPT was then calculated for all cells based on their diffusion distance from this root. The resulting continuous pseudotime trajectory was visualized on a t-SNE embedding and examined across the discrete cell clusters to validate the inferred developmental progression.","To infer a continuous developmental trajectory from single-cell data, we performed a pseudotime analysis using the **Diffusion Pseudotime (DPT)** method, implemented in the Scanpy Python library.

The analysis was performed on a bone marrow dataset. The data was first preprocessed using a standard workflow, which included filtering out genes expressed in fewer than 20 cells, normalizing for library size, and applying a log transformation. The top highly variable genes were selected for downstream analysis. **Principal Component Analysis (PCA)** was then performed, and a k-nearest neighbor graph was constructed in the PCA space using the top 10 components. For visualization and to define discrete cell states, cells were clustered using the **Louvain algorithm**, and a **t-SNE** embedding was computed.

The core trajectory inference began with the computation of a **diffusion map**, which models the probability of transitioning between cells on the expression manifold. To establish the origin of the developmental process, a root cell was identified by selecting the cell with the minimum value along the third diffusion component, a heuristic often used to find the earliest state in differentiation. Finally, **Diffusion Pseudotime (DPT)** was calculated, ordering every cell by its diffusion distance from this root cell. The resulting continuous pseudotime trajectory was visualized on the t-SNE embedding, and the progression of pseudotime across the discrete Louvain clusters was examined using a violin plot.","scampy, pathlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for system operations, data handling, and single-cell analysis
import scanpy as sc
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

#--------------------------------
# Data Input & Preparation
#--------------------------------

# Define the directory to store the data and create it if it doesn't exist.
DATA_DIR = Path(""./data/"")
DATA_DIR.mkdir(parents=True, exist_ok=True)
FILE_NAME = DATA_DIR / ""bone_marrow.h5ad""

# Load the dataset. A backup URL is provided to download the data if not found locally.
print(""Loading dataset..."")
adata = sc.read(
    filename=FILE_NAME,
    backup_url=""https://figshare.com/ndownloader/files/35826944""
)

#--------------------------------
# Preprocessing
#--------------------------------

# Apply a standard preprocessing workflow.
print(""Preprocessing data..."")
# Filter out genes that are not expressed in at least 20 cells.
sc.pp.filter_genes(adata, min_counts=20)
# Normalize each cell by total counts to account for library size differences.
sc.pp.normalize_total(adata)
# Apply a log transformation to the data to stabilize variance.
sc.pp.log1p(adata)
# Identify highly variable genes (HVGs) to focus on biological signal.
sc.pp.highly_variable_genes(adata)
# Perform PCA for dimensionality reduction.
sc.tl.pca(adata)
# Compute the nearest neighbor graph, which is required for clustering and trajectory analysis.
sc.pp.neighbors(adata, n_pcs=10)

#--------------------------------
# Major Analysis Task: Trajectory Inference with DPT
#--------------------------------

# --- Clustering & Visualization Embedding ---
# Cluster cells using the Louvain algorithm to identify cell groups.
sc.tl.louvain(adata)
# Compute a t-SNE embedding for visualization purposes.
sc.tl.tsne(adata)

# --- Diffusion Pseudotime (DPT) ---
# Compute the diffusion map. This represents the data as a graph where edge
# weights reflect the transition probability between cells.
print(""Computing diffusion map and pseudotime..."")
sc.tl.diffmap(adata)

# Identify the root cell for the pseudotime trajectory. Here, we select the cell
# with the minimum value in the 3rd diffusion component as the starting point.
# This choice often corresponds to the earliest cell state in a differentiation process.
root_ixs = adata.obsm[""X_diffmap""][:, 3].argmin()
adata.uns[""iroot""] = root_ixs

# Compute Diffusion Pseudotime (DPT), which orders cells based on their distance
# from the root cell along the diffusion map graph.
sc.tl.dpt(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the t-SNE embedding, colored by the identified cell clusters.
sc.pl.scatter(adata, basis=""tsne"", color=""louvain"", title=""t-SNE by Cluster"")

# Visualize the t-SNE embedding, colored by the calculated DPT pseudotime.
# This plot shows the progression of cells along the inferred trajectory.
sc.pl.scatter(
    adata, basis=""tsne"", color=[""dpt_pseudotime""], color_map=""gnuplot2"",
    title=""t-SNE colored by Pseudotime""
)

# Create a violin plot to compare the pseudotime distributions across the different cell clusters.
sc.pl.violin(
    adata, keys=[""dpt_pseudotime""], groupby=""louvain"", rotation=45,
    title=""Pseudotime Distribution by Cluster""
)


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object, which now contains the DPT trajectory results.
print(""Saving trajectory data object..."")
adata.write(""bone_marrow_dpt_trajectory.h5ad"")

print(""\nPseudotime analysis completed!"")
", https://www.sc-best-practices.org/trajectories/pseudotemporal.html,F
scRNA-seq,Trajectory inference,"The analysis was performed in R using Monocle 2 to reconstruct developmental trajectories. Following initial preprocessing, ordering genes for trajectory construction were identified via differential expression analysis between experimental conditions. The DDRTree algorithm was then used to reduce dimensionality and order cells in pseudotime. To dissect molecular programs, further differential expression analyses identified genes changing along the trajectory and branch-specific genes using the Branch Expression Analysis Modeling (BEAM) test. The final trajectory and the expression dynamics of branch-dependent genes were visualized to validate the inferred lineage structure.","To reconstruct the developmental trajectory from single-cell data, we utilized the Monocle 2 package in R.

The analysis workflow began by creating a `CellDataSet` object from the raw gene expression matrix, along with cell and gene metadata. The data was modeled using a negative binomial distribution, which is appropriate for UMI-based counts. Standard preprocessing steps were then applied, which included estimating size factors to normalize for differences in library depth and estimating gene dispersions. A preliminary gene filter was applied to retain only those genes expressed in a minimum of 10 cells for downstream analysis.

To identify genes that would define the developmental trajectory, we performed a differential expression test between the experimental conditions. Genes that were significantly different (q-value < 0.01) between conditions were selected as the set of `ordering_genes`. The dimensionality of the data was then reduced using these ordering genes with the **DDRTree algorithm**, a method specifically designed to resolve complex, branching trajectories. Following the construction of this developmental tree, cells were ordered along the trajectory to assign a pseudotime value to each cell.

To characterize the molecular events occurring along this inferred trajectory, we performed two additional differential expression analyses. First, we identified genes whose expression significantly changed as a function of pseudotime using a natural spline model. Second, to dissect branch-specific gene regulation, we used the **Branch Expression Analysis Modeling (BEAM)** test to identify genes that were differentially expressed at a specific branch point. The final trajectory was visualized with cells colored by pseudotime and experimental condition, and the expression dynamics of branch-dependent genes were plotted to confirm their distinct patterns across the bifurcating lineages.","monocle, ggplot2, Matrix",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for single-cell trajectory analysis
library(monocle)   # Core package for Monocle2 analysis
library(ggplot2)   # For advanced plotting customization
library(Matrix)    # For handling sparse matrices

#--------------------------------
# Data Input
#--------------------------------

# NOTE: Replace file paths with the actual location of your data files.
# Read the gene expression matrix (rows: genes, columns: cells).
expr_matrix <- read.table(""fpkm_matrix.txt"", header = TRUE, row.names = 1)

# Read the cell metadata sheet.
sample_sheet <- read.delim(""cell_sample_sheet.txt"", header = TRUE, row.names = 1)

# Read the gene annotation file.
gene_annotation <- read.delim(""gene_annotations.txt"", header = TRUE, row.names = 1)


#--------------------------------
# Construct CellDataSet Object
#--------------------------------

# Create AnnotatedDataFrame objects to hold the metadata.
pd <- new(""AnnotatedDataFrame"", data = sample_sheet)
fd <- new(""AnnotatedDataFrame"", data = gene_annotation)

# Create the main CellDataSet (CDS) object, which is Monocle's primary data structure.
# `negbinomial.size()` is a common choice for UMI-based scRNA-seq data.
cds <- newCellDataSet(
  as.matrix(expr_matrix),
  phenoData = pd,
  featureData = fd,
  lowerDetectionLimit = 0.5,
  expressionFamily = negbinomial.size()
)


#--------------------------------
# Preprocessing and Normalization
#--------------------------------

# Estimate size factors for normalization and gene dispersions for downstream analysis.
cds <- estimateSizeFactors(cds)
cds <- estimateDispersions(cds)

# Filter out low-quality genes. First, detect genes expressed above a minimum threshold.
cds <- detectGenes(cds, min_expr = 0.1)

# Then, subset the list to genes that are expressed in at least 10 cells.
expressed_genes <- row.names(subset(fData(cds), num_cells_expressed >= 10))


#--------------------------------
# Major Analysis Tasks: Trajectory Inference
#--------------------------------

# --- Selecting Genes for Ordering ---
# Identify genes that will be used to order cells in pseudotime.
# Here, we perform differential expression between conditions (e.g., 'Media' column).
# Genes that change significantly between conditions are good candidates for ordering.
diff_test_res <- differentialGeneTest(cds[expressed_genes, ], fullModelFormulaStr = ""~Media"")
ordering_genes <- row.names(subset(diff_test_res, qval < 0.01))

# Set the identified ordering genes in the CDS object.
cds <- setOrderingFilter(cds, ordering_genes)


# --- Trajectory Construction ---
# Reduce the dimensionality of the data using the DDRTree algorithm.
# This method is well-suited for datasets with complex, branching trajectories.
cds <- reduceDimension(cds, max_components = 2, method = ""DDRTree"")

# Order the cells along the trajectory to calculate pseudotime.
cds <- orderCells(cds)


# --- Differential Expression Along Pseudotime ---
# Identify genes whose expression changes as a function of pseudotime.
# The `sm.ns` function models expression using a natural smoothing spline.
diff_genes_pseudotime <- differentialGeneTest(cds[expressed_genes, ], fullModelFormulaStr = ""~sm.ns(Pseudotime)"")
sig_genes_pseudotime <- subset(diff_genes_pseudotime, qval < 0.1)

# Print the top genes that vary along the trajectory.
print(""Top genes changing as a function of pseudotime:"")
print(head(sig_genes_pseudotime[order(sig_genes_pseudotime$qval), ]))


# --- Branch Analysis (BEAM) ---
# Identify genes that are differentially expressed at a branch point in the trajectory.
# NOTE: The `branch_point` parameter must be adjusted based on your specific trajectory.
branch_point <- 1
beam_res <- BEAM(cds, branch_point = branch_point, cores = 1)
beam_sig_genes <- row.names(subset(beam_res, qval < 1e-4))


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Visualize the dispersion of the ordering genes.
plot_ordering_genes(cds)

# Plot the main cell trajectory, colored by different variables.
# Color by Pseudotime to see the progression.
plot_cell_trajectory(cds, color_by = ""Pseudotime"")

# Color by experimental condition to see how it maps onto the trajectory.
plot_cell_trajectory(cds, color_by = ""Media"")

# Plot the expression of branch-dependent genes along the branched trajectory.
plot_genes_branched_pseudotime(
    cds[beam_sig_genes, ],
    branch_point = branch_point,
    color_by = ""Pseudotime""
)


#--------------------------------
# Data Output
#--------------------------------

# Save the final Monocle object, which contains all analysis results.
saveRDS(cds, file = ""monocle_trajectory_object.rds"")
",https://cole-trapnell-lab.github.io/monocle-release/docs/#constructing-single-cell-trajectories,F
scRNA-seq,Trajectory inference,"The analysis was performed in R with Monocle3 to reconstruct developmental trajectories. Following preprocessing with PCA, the data was aligned to correct for batch effects, and a UMAP embedding was computed on this integrated representation. A principal graph was then learned on the UMAP embedding to connect discrete cell clusters and infer the developmental manifold. Cells were subsequently ordered in pseudotime along this graph. A graph-based differential expression test identified genes whose expression changed significantly along the trajectory. The final trajectory and expression dynamics of top trajectory-dependent genes were visualized to validate the inferred process.","To reconstruct developmental trajectories from a single-cell dataset of *C. elegans* embryos, we used the Monocle3 package in R.

The analysis workflow started by loading the gene expression matrix, cell metadata, and gene annotations into a `cell_data_set` object, the primary data structure in Monocle3. A standard preprocessing pipeline was then applied, which included cell-level normalization and **Principal Component Analysis (PCA)** with the top 50 principal components retained. To correct for technical variability between experimental batches, the PCA-reduced data was aligned using the `align_cds` function. A **UMAP** embedding was then computed on this batch-corrected representation for visualization.

For trajectory inference, cells were first grouped into discrete clusters to identify distinct cell states. A principal graph was then learned on the UMAP embedding, which connects these clusters to represent the underlying developmental manifold. Following the construction of this graph, cells were ordered along the trajectory to calculate a **pseudotime** value for each cell, effectively arranging them from early to late developmental stages.

To identify genes associated with this developmental process, a differential expression analysis was performed using the `graph_test` function. This test identified genes whose expression levels changed significantly as cells progressed along the inferred trajectory graph. The final trajectory was visualized on the UMAP plot, with cells colored by their pseudotime value. Additionally, the expression dynamics of the top six most significant trajectory-dependent genes were plotted against pseudotime to characterize the molecular changes occurring during development.","monocle3, ggplot2",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for single-cell trajectory analysis
library(monocle3)  # Core package for Monocle3 analysis
library(ggplot2)   # For advanced plotting customization

#--------------------------------
# Data Input
#--------------------------------

# Load the C. elegans embryo dataset as an example.
# NOTE: Modify these paths if using your own data.
expression_matrix <- readRDS(url(""https://depts.washington.edu:/trapnell-lab/software/monocle3/celegans/data/packer_embryo_expression.rds""))
cell_metadata <- readRDS(url(""https://depts.washington.edu:/trapnell-lab/software/monocle3/celegans/data/packer_embryo_colData.rds""))
gene_annotation <- readRDS(url(""https://depts.washington.edu:/trapnell-lab/software/monocle3/celegans/data/packer_embryo_rowData.rds""))

# Create the main cell_data_set (CDS) object, which is Monocle3's primary data structure.
cds <- new_cell_data_set(
  expression_matrix,
  cell_metadata = cell_metadata,
  gene_metadata = gene_annotation
)


#--------------------------------
# Preprocessing and Dimensionality Reduction
#--------------------------------

# Apply a standard preprocessing workflow to the CDS object.
# This function normalizes, performs PCA, and prepares the data for downstream analysis.
cds <- preprocess_cds(cds, num_dim = 50)

# If the data has multiple batches, correct for batch effects.
# `alignment_group` should point to the column in cell_metadata that specifies the batch.
cds <- align_cds(cds, alignment_group = ""batch"")

# Reduce the dimensionality further using UMAP. The results are stored in the CDS object.
cds <- reduce_dimension(cds)


#--------------------------------
# Major Analysis Tasks: Trajectory Inference
#--------------------------------

# --- Clustering & Trajectory Construction ---
# Cluster cells to identify distinct cell states or types.
cds <- cluster_cells(cds)

# Learn the principal graph that represents the developmental trajectory.
cds <- learn_graph(cds)

# Order the cells along the trajectory to calculate pseudotime.
# Monocle3 will prompt to select root node(s) interactively if not specified.
cds <- order_cells(cds)


# --- Differential Expression Analysis Along Trajectory ---
# Identify genes whose expression changes significantly as cells traverse the learned trajectory.
# `graph_test` is a powerful function for finding trajectory-dependent genes.
graph_test_res <- graph_test(cds, neighbor_graph = ""principal_graph"", cores = 4)

# Filter the results to get genes with a q-value below a significance threshold.
significant_genes <- subset(graph_test_res, q_value < 0.05)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Plot the trajectory, coloring cells by the calculated pseudotime.
# This visualizes the developmental progression of the cells.
plot_cells(
  cds,
  color_cells_by = ""pseudotime"",
  label_cell_groups = FALSE,
  label_leaves = FALSE,
  label_branch_points = FALSE
)

# Plot the expression trends of the top 6 most significant trajectory-dependent genes.
# This helps to understand the molecular changes occurring along the trajectory.
plot_genes_in_pseudotime(
  cds,
  genes = rownames(significant_genes)[1:6],
  min_expr = 0.5
)


#--------------------------------
# Data Output
#--------------------------------

# Save the differential expression results to a CSV file.
write.csv(significant_genes, ""differential_expression_results.csv"")

# Save the final Monocle3 object, which contains all analysis results.
saveRDS(cds, file = ""monocle3_trajectory_object.rds"")
",https://cole-trapnell-lab.github.io/monocle3/docs/trajectories/,F
scRNA-seq,Compositional analysis,"The analysis was performed in Python using the scCODA model from the `pertpy` library to identify changes in cell type abundance. The model was configured by specifying sample, condition, and cell type identifiers, with one cell type designated as a stable reference for the compositional analysis. A Bayesian hierarchical model was then fitted to infer the effects of the experimental condition on cell type proportions. Statistically significant changes were identified for cell types whose credible interval for the log-fold change did not include zero. The results were visualized with boxplots and an effects plot summarizing significant changes.","To identify statistically significant shifts in cell type abundances between different experimental conditions, we performed a compositional data analysis using the **scCODA** model, implemented in the `pertpy` Python library.

The analysis was conducted on the Haber et al. (2017) dataset of small intestinal epithelial cells. The `scCODA` model was first initialized and the cell-level data was loaded, specifying the `cell_label` as the cell type identifier, `batch` as the sample identifier, and `condition` as the experimental covariate to be tested. This process automatically aggregates the cell counts to the sample level, creating the compositional data matrix required for the model.

The model was configured to test for changes in cell type abundance associated with the experimental `condition`. To handle the compositional nature of the data, the **Endocrine** cell population was designated as the reference cell type. All reported changes in other cell types are therefore relative to this stable reference. A Bayesian hierarchical model was then fitted using a No-U-Turn Sampler (NUTS) to infer the posterior distributions of cell type abundance changes. To identify significant effects, we determined the 95% credible intervals for the log-fold change of each cell type. Changes were deemed credible if this interval did not contain zero. The results were visualized using boxplots to show the distribution of cell counts across conditions and a final effects barplot to display the significant log-fold changes for each cell type.","pandas, numpy, matplotlib, seaborn, scanpy, pertpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scanpy as sc
import pertpy as pt  # Pertpy includes tools for compositional analysis (scCODA)


#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters for clear and consistent visualization
sns.set_theme()


#--------------------------------
# Data Input
#--------------------------------

# Load the Haber et al. (2017) dataset of small intestinal epithelial cells.
# This dataset contains cell type labels and condition information suitable for this analysis.
print(""Loading Haber et al. 2017 dataset..."")
adata = pt.dt.haber_2017_regions()
print(""Loaded data shape:"", adata.shape)


#--------------------------------
# Illustrative Example of Compositional Data
#--------------------------------

# This section demonstrates the core concept of compositional data analysis.
print(""\n--- Illustrating Compositional Data Concepts ---"")
# Define example cell counts for three cell types in healthy and diseased tissue.
healthy_tissue = [2000, 2000, 2000]
diseased_tissue = [4000, 2000, 2000] # Note: Cell type A has doubled.

# Create a DataFrame with the total cell counts.
example_data_global = pd.DataFrame(
    data=np.array([healthy_tissue, diseased_tissue]),
    index=[""Healthy"", ""Diseased""],
    columns=[""A"", ""B"", ""C""]
)

# Reshape the DataFrame for easy plotting with seaborn.
plot_data_global = example_data_global.reset_index().melt(
    id_vars=""index"", value_vars=[""A"", ""B"", ""C""],
    var_name=""Cell type"", value_name=""count""
)


#--------------------------------
# Major Analysis Task: Compositional Analysis with scCODA
#--------------------------------

# --- Model and Data Initialization ---
# Instantiate the scCODA model from pertpy.
sccoda_model = pt.tl.Sccoda()

# Load the data into the scCODA model structure.
# This function automatically aggregates cell-level data to the sample level.
print(""\n--- Setting up scCODA Analysis ---"")
sccoda_data = sccoda_model.load(
    adata,
    type=""cell_level"",
    generate_sample_level=True,
    cell_type_identifier=""cell_label"",
    sample_identifier=""batch"",
    covariate_obs=[""condition""],
)
print(""scCODA data modalities:"", sccoda_data)

# --- Model Configuration and Execution ---
# Prepare the data for the model, specifying the statistical formula and a reference cell type.
# The model will calculate changes relative to this reference.
sccoda_data = sccoda_model.prepare(
    sccoda_data,
    modality_key=""coda"",
    formula=""condition"",
    reference_cell_type=""Endocrine""
)

# Run the Bayesian inference sampler (NUTS) to fit the model.
# `rng_key` ensures reproducibility.
print(""\nRunning Bayesian inference model..."")
sccoda_model.run_nuts(sccoda_data, modality_key=""coda"", rng_key=1234)

# Set the False Discovery Rate (FDR) threshold for identifying credible effects.
sccoda_model.set_fdr(sccoda_data, est_fdr=0.2)

# Extract and print the credible effects (significant changes) for each cell type.
credible_effects = sccoda_model.credible_effects(sccoda_data, modality_key=""coda"")
print(""\nCredible Effects (Significant Changes):\n"", credible_effects)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Illustrative Example Plots ---
# Plot barplots showing the global abundances from the illustrative example.
fig, ax = plt.subplots(1, 2, figsize=(12, 6))
fig.suptitle(""Illustrative Example: Global Cell Abundances"")
sns.barplot(data=plot_data_global, x=""index"", y=""count"", hue=""Cell type"", ax=ax[0])
ax[0].set_title(""Counts by Status"")
sns.barplot(data=plot_data_global, x=""Cell type"", y=""count"", hue=""index"", ax=ax[1])
ax[1].set_title(""Counts by Cell Type"")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# --- scCODA Diagnostic Plots ---
# Create boxplots of cell type counts across conditions to visualize distributions.
print(""\n--- Visualizing Compositional Data ---"")
sccoda_model.plot_boxplots(
    sccoda_data,
    modality_key=""coda"",
    feature_name=""condition"",
    figsize=(12, 5),
    add_dots=True,
    args_swarmplot={""palette"": [""red""]},
)
plt.show()

# Create a stacked barplot to show cell type proportions for each condition.
sccoda_model.plot_stacked_barplot(
    sccoda_data,
    modality_key=""coda"",
    feature_name=""condition"",
    figsize=(4, 2)
)
plt.show()

# --- scCODA Final Effects Plot ---
# Generate a bar plot of the final effects. This shows the log-fold changes of cell types
# relative to the reference, highlighting significant changes.
sccoda_model.plot_effects_barplot(sccoda_data, ""coda"", ""condition"")
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object and the scCODA results object.
print(""\nSaving results..."")
adata.write(""haber_2017_analyzed.h5ad"")
sccoda_data.write(""haber_2017_sccoda_results.h5ad"")

print(""\nCompositional analysis completed!"")
",https://www.sc-best-practices.org/conditions/compositional.html,F
scRNA-seq,Compositional analysis,"The analysis was performed in Python using the Milo method from the `pertpy` library to test for differential cell abundance. Following standard preprocessing and construction of a k-nearest neighbor graph, overlapping cell neighborhoods were defined across the transcriptional manifold. The number of cells from each experimental sample was counted within these neighborhoods. A negative binomial generalized linear model was then fit to these counts to identify neighborhoods with significant shifts in cell abundance between conditions. The results were visualized with a beeswarm plot displaying the log-fold change and significance for each neighborhood.","To test for shifts in cell state abundance between experimental conditions, we performed a differential abundance analysis using the **Milo** method, implemented in the `pertpy` Python library.

The analysis was conducted on the Haber et al. (2017) dataset of small intestinal epithelial cells. The raw count data was first log-normalized. To focus the analysis on sources of biological variation, the top 3,000 **highly variable genes (HVGs)** were selected. Dimensionality was reduced by performing **Principal Component Analysis (PCA)** on these HVGs, and a **k-nearest neighbor (k-NN) graph** was constructed in the PCA space using the top 30 principal components and `k=10` neighbors.

For the core Milo analysis, we first defined a set of overlapping cell neighborhoods based on the k-NN graph. These neighborhoods were seeded from a representative subset of 10% of the total cells. The number of cells from each experimental sample was then counted within each of these neighborhoods. To identify neighborhoods with statistically significant changes in cell abundance between the *Salmonella*-infected and control conditions, we fit a negative binomial generalized linear model to the neighborhood cell counts, using the design formula `~condition`. This test identifies fine-grained shifts in cell state density across the transcriptional manifold. The results were visualized using a beeswarm plot to display the log-fold change and statistical significance for each neighborhood.","pandas, numpy, matplotlib, seaborn, scanpy, pertpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scanpy as sc
import seaborn as sns
import pertpy as pt  # Pertpy provides both scCODA and Milo

#--------------------------------
# Data Input & Preprocessing
#--------------------------------

# Load the Haber et al. (2017) dataset.
print(""Loading Haber et al. 2017 dataset..."")
adata = pt.dt.haber_2017_regions()
print(""Loaded data shape:"", adata.shape)

# --- Standard scRNA-seq preprocessing ---
# Save raw counts into a new layer for reference.
adata.layers[""counts""] = adata.X.copy()

# Create log-transformed counts for analysis.
adata.layers[""logcounts""] = np.log1p(adata.layers[""counts""])
adata.X = adata.layers[""logcounts""].copy()

# Identify highly variable genes to focus on biological signal.
sc.pp.highly_variable_genes(adata, n_top_genes=3000, subset=False)

# Perform PCA on the highly variable genes.
sc.pp.pca(adata)

# Construct the k-nearest neighbor (KNN) graph. This is essential for defining neighborhoods.
sc.pp.neighbors(adata, n_neighbors=10, n_pcs=30)

# Compute a UMAP embedding for visualization.
sc.tl.umap(adata)


#--------------------------------
# Major Analysis Task: Differential Abundance with Milo
#--------------------------------

# --- Model and Data Initialization ---
# Instantiate the Milo model from pertpy.
milo = pt.tl.Milo()

# Load the preprocessed AnnData object into the Milo framework.
# This creates a MuData object, which can hold multiple data modalities.
print(""\n--- Setting up Milo Analysis ---"")
mdata = milo.load(adata)

# --- Define Cell Neighborhoods ---
# Define cell neighborhoods based on the KNN graph. These are overlapping groups
# of cells that represent a fine-grained partitioning of the data.
# `prop=0.1` means 10% of cells are sampled as ""index"" cells to define the neighborhoods.
milo.make_nhoods(mdata, prop=0.1)

# --- Count Cells in Neighborhoods ---
# Count the number of cells from each sample ('batch') within each neighborhood.
# This creates the count matrix that will be used for differential abundance testing.
milo.count_nhoods(mdata, sample_col=""batch"")

# --- Build Neighborhood Graph ---
# Create a graph connecting overlapping neighborhoods for visualization and analysis.
milo.build_nhood_graph(mdata)

# --- Differential Abundance Testing ---
# Perform the DA test to compare conditions (e.g., Salmonella vs. Control).
# The design formula `~condition` tells the model to test for changes related to the 'condition' variable.
print(""\nPerforming differential abundance testing..."")
milo.da_nhoods(mdata, design=""~condition"", model_contrasts=""conditionSalmonella-conditionControl"")

# Retrieve the DA results from the Milo object.
milo_results = mdata[""milo""].obs.copy()
print(""\nDifferential Abundance (DA) results (first 5 neighborhoods):"")
print(milo_results.head())


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP of Raw Data ---
# Visualize the initial UMAP colored by condition and batch to see the overall data structure.
sc.pl.umap(adata, color=[""condition"", ""batch""], ncols=2, wspace=0.4, title=[""Condition"", ""Batch""])

# --- Neighborhood Size Distribution ---
# Plot a histogram of the number of cells in each neighborhood.
print(""\n--- Visualizing Milo Results ---"")
nhood_size = adata.obsm[""nhoods""].toarray().sum(axis=0)
plt.figure(figsize=(6, 4))
plt.hist(nhood_size, bins=20, edgecolor=""black"")
plt.xlabel(""# cells in each neighborhood"")
plt.ylabel(""Number of neighborhoods"")
plt.title(""Distribution of Neighborhood Sizes"")
plt.show()

# --- Neighborhood Graph ---
# Visualize the graph of overlapping neighborhoods.
with plt.rc_context({""figure.figsize"": [10, 10]}):
    milo.plot_nhood_graph(mdata, alpha=0.1, min_size=5, plot_edges=False)
plt.show()

# --- DA Beeswarm Plot ---
# Create a beeswarm plot to visualize the DA results across all neighborhoods.
# This plot shows the log-fold change and significance for each neighborhood.
milo.plot_da_beeswarm(mdata)
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the MuData object, which contains the original data and all Milo results.
print(""\nSaving results..."")
mdata.write(""haber_2017_milo_results.h5mu"")

print(""\nDifferential abundance analysis with Milo completed!"")
",https://www.sc-best-practices.org/conditions/compositional.html,F
scRNA-seq,Enrichment analysis,"The analysis was performed in Python using the `decoupler` package to identify enriched biological pathways. Following standard preprocessing and feature selection, a ranked list of genes was generated. This was achieved by performing a differential expression analysis comparing a target cell population under a specific condition against all other cells, using the resulting t-statistics for ranking. Gene Set Enrichment Analysis (GSEA) was then performed on this ranked list using filtered gene sets from a pathway database. The resulting normalized enrichment scores were used to identify the key biological processes active in the target cell population.","To identify biological pathways enriched in specific cell populations under experimental conditions, we performed a gene set enrichment analysis (GSEA) using the `decoupler` package in Python.

The analysis began with a dataset of peripheral blood mononuclear cells (PBMCs), which was preprocessed using a standard workflow. Raw counts were preserved, and the data was normalized for sequencing depth, followed by a log-plus-one transformation. The top 4,000 highly variable genes (HVGs) were identified using the Seurat v3 method to focus the analysis on sources of biological variation. For visualization purposes, dimensionality was reduced via Principal Component Analysis (PCA), and a UMAP embedding was generated.

To generate a ranked list of genes for enrichment analysis, we first performed a differential expression analysis. Cells were grouped by a combination of their cell type and experimental condition (e.g., stimulated vs. control). A Student's t-test was then used to compare our target group of interest—stimulated FCGR3A+ Monocytes—against all other groups. The resulting gene-level t-statistics for the previously identified HVGs were extracted to create a ranked list reflecting the magnitude and direction of expression change.

Finally, we performed GSEA on this ranked list. Gene sets were derived from the Reactome pathway database (MSigDB C2 collection). To ensure robust statistical analysis, these pathways were filtered to retain only those containing between 15 and 500 genes. The `decoupler` GSEA function was then used to calculate a normalized enrichment score (NES) for each pathway, quantifying the degree to which genes in a pathway were overrepresented at the top or bottom of our ranked list. The top 20 enriched pathways were visualized to identify the key biological processes active in stimulated FCGR3A+ Monocytes.","numpy, pandas, scanpy, decoupler, matplotlib, seaborn",python,"#!/usr/bin/env python
""""""
Example script for gene set enrichment and pathway analysis for single‐cell RNA‐seq data.
This script reads an AnnData object, preprocesses the data, performs a t‐test to rank genes,
extracts DE t-statistics for a selected cell type/condition, loads a Reactome GMT file, runs GSEA via decoupler,
and plots the top enriched pathways.
""""""

#------------------------------------------------------------------------------#
#                           Import necessary modules                           #
#------------------------------------------------------------------------------#
from __future__ import annotations
import os
from pathlib import Path
import numpy as np
import pandas as pd
import scanpy as sc
import decoupler
import matplotlib.pyplot as plt
import seaborn as sns

#------------------------------------------------------------------------------#
#                           Script Configuration                               #
#------------------------------------------------------------------------------#
# Set global parameters for plotting and figure aesthetics
sns.set(style=""whitegrid"")
sc.settings.set_figure_params(dpi=200, frameon=False, figsize=(4, 4))

#------------------------------------------------------------------------------#
#                              Data Input                                      #
#------------------------------------------------------------------------------#
# Read the PBMC dataset from a .h5ad file.
# A backup URL is provided for reproducibility if the file is not found locally.
adata = sc.read(
    ""kang_counts_25k.h5ad"",
    backup_url=""https://figshare.com/ndownloader/files/34464122""
)

# Save the raw counts in a separate layer for later use, a common best practice.
adata.layers[""counts""] = adata.X.copy()

# Rename the ""label"" column to ""condition"" for better clarity in downstream steps.
adata.obs = adata.obs.rename(columns={""label"": ""condition""})

#------------------------------------------------------------------------------#
#                           Data Preprocessing                                 #
#------------------------------------------------------------------------------#
# --- Normalization and Feature Selection ---
# Normalize total counts per cell and apply a log transformation.
sc.pp.normalize_total(adata)
sc.pp.log1p(adata)

# Identify the top 4000 highly variable genes using the ""seurat_v3"" method.
# `layer=""counts""` ensures this is calculated on the untransformed data.
sc.pp.highly_variable_genes(
    adata,
    n_top_genes=4000,
    flavor=""seurat_v3"",
    subset=False,
    layer=""counts""
)

# --- Dimensionality Reduction for Visualization ---
# Compute PCA, build a neighbor graph, and generate a UMAP representation.
sc.pp.pca(adata)
sc.pp.neighbors(adata)
sc.tl.umap(adata)

#------------------------------------------------------------------------------#
#                   Major Analysis Task: Enrichment Analysis                   #
#------------------------------------------------------------------------------#

# --- 1. Differential Expression Analysis ---
# Create a combined 'group' column (e.g., ""stim_FCGR3A+ Monocytes"") to perform
# DE analysis on specific cell type/condition combinations.
adata.obs[""group""] = adata.obs[""condition""].astype(str) + ""_"" + adata.obs[""cell_type""]

# Run a t-test to rank genes based on differential expression across all groups.
sc.tl.rank_genes_groups(adata, groupby=""group"", method=""t-test"", key_added=""t-test"")

# Define the target group for GSEA and extract its DE results.
target_group = ""stim_FCGR3A+ Monocytes""
de_df = sc.get.rank_genes_groups_df(adata, group=target_group, key=""t-test"")

# Filter for highly variable genes and create a DataFrame of t-statistics.
hvg_genes = adata.var_names[adata.var[""highly_variable""]]
t_stats = de_df.set_index(""names"").loc[hvg_genes].sort_values(""scores"", key=np.abs, ascending=False)[[""scores""]]
t_stats = t_stats.rename(columns={""scores"": target_group})

# --- 2. Prepare Pathway Gene Sets (Reactome GMT) ---
# Define the path for the GMT file and download if it doesn't exist.
gmt_path = Path(""c2.cp.reactome.v7.5.1.symbols.gmt"")
if not gmt_path.is_file():
    os.system(""wget -O c2.cp.reactome.v7.5.1.symbols.gmt https://figshare.com/ndownloader/files/35233771"")

def gmt_to_decoupler(pth: Path) -> pd.DataFrame:
    """"""Parse a GMT file into a long-format DataFrame compatible with decoupler.""""""
    pathways = {}
    with pth.open(""r"") as f:
        for line in f:
            parts = line.strip().split(""\t"")
            name, _, *genes = parts
            pathways[name] = genes
    # Convert dictionary to a long-format DataFrame
    return pd.DataFrame(
        [(gene_set, gene) for gene_set, genes in pathways.items() for gene in genes],
        columns=[""geneset"", ""genesymbol""]
    )

# Parse the GMT file and filter gene sets by size for robustness.
reactome = gmt_to_decoupler(gmt_path)
geneset_size = reactome.groupby(""geneset"").size()
gsea_genesets = geneset_size.index[(geneset_size > 15) & (geneset_size < 500)]
reactome_filtered = reactome[reactome[""geneset""].isin(gsea_genesets)]

# --- 3. Run GSEA with Decoupler ---
# Run GSEA using the gene-level t-statistics and the filtered Reactome pathways.
scores, norm, pvals = decoupler.run_gsea(
    mat=t_stats.T,
    net=reactome_filtered,
    source=""geneset"",
    target=""genesymbol""
)

# Combine the GSEA results into a single, sorted DataFrame.
gsea_results = pd.concat({
    ""score"": scores.T,
    ""norm_score"": norm.T,
#    ""pval"": pvals.T
#}, axis=1).droplevel(level=1, axis=1).sort_values(""pval"")
}, axis=1).droplevel(level=1, axis=1).sort_values(""norm_score"", ascending=False)
gsea_results.index.name = ""Pathway""

#------------------------------------------------------------------------------#
#                              Plotting / Visualization                        #
#------------------------------------------------------------------------------#
# Plot UMAP embeddings colored by condition and cell type to visualize the data structure.
sc.pl.umap(adata, color=[""condition"", ""cell_type""], frameon=False, ncols=2)

# Plot the top 20 enriched pathways based on their normalized enrichment score.
top20 = gsea_results.head(20).reset_index()

plt.figure(figsize=(10, 8))
sns.barplot(data=top20, x=""norm_score"", y=""Pathway"", palette=""viridis"")
plt.xlabel(""Normalized Enrichment Score"")
plt.ylabel(""Pathway"")
plt.title(f""Top 20 Enriched Pathways in {target_group}"")
plt.tight_layout()
plt.show()

#------------------------------------------------------------------------------#
#                              Data Output                                     #
#------------------------------------------------------------------------------#
# Save the final GSEA results to a CSV file.
print(""\nSaving GSEA results..."")
gsea_results.to_csv(""gsea_results.csv"")

# Save the AnnData object containing the DE analysis results.
adata.write(""kang_processed_with_de.h5ad"")

print(""\nGene set enrichment analysis completed!"")
",https://www.sc-best-practices.org/conditions/gsea_pathway.html,F
scRNA-seq,Cell-cell communication,"Intercellular communication was investigated in Python using the LIANA framework with the CellPhoneDB method on PBMC data. Quality control and normalization preceded the evaluation of ligand‐receptor interactions between designated sender (CD4 T cells, B cells, FCGR3A+ Monocytes) and receiver (CD8 T cells, NK cells) populations. Known ligand‐receptor pairs were statistically assessed to identify significant interactions, with the most robust signals visualized in a dot plot highlighting expression levels and interaction specificity. This integrated workflow elucidated key molecular dialogues underlying immune processes.","To infer potential intercellular communication pathways, we performed a ligand-receptor interaction analysis using the **LIANA** framework in Python, specifically leveraging the **CellPhoneDB** method.

The analysis was conducted on a preprocessed dataset of peripheral blood mononuclear cells (PBMCs). Initial quality control involved removing cells expressing fewer than 200 genes and genes detected in fewer than three cells. The filtered data was then normalized for sequencing depth by scaling each cell’s total counts, followed by a log-plus-one transformation.

Using the normalized expression data, we investigated interactions between a defined set of sender populations (**CD4 T cells, B cells, and FCGR3A+ Monocytes**) and receiver populations (**CD8 T cells and NK cells**). The CellPhoneDB method was applied to systematically evaluate the expression of known ligand-receptor pairs between these specified cell types. This method assesses the specificity of an interaction by calculating a p-value for each pair in each source-target context.

Interactions with a p-value less than or equal to 0.01 were considered statistically significant. The top 20 significant interactions, ranked by the mean expression of the ligand and receptor, were visualized using a dot plot. In this plot, the size of each dot represented the statistical significance of the interaction (with larger dots indicating smaller p-values), while the color indicated the mean expression level of the interacting pair.","scanpy, liana, decoupler, matplotlib, seaborn, pandas, numpy",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scanpy as sc
import liana as li  # For ligand-receptor interaction inference

#--------------------------------
# Data Input
#--------------------------------

# Load the single-cell transcriptomics dataset.
# A backup URL is provided for reproducibility.
print(""Loading Kang et al. PBMC dataset..."")
adata = sc.read(
    ""kang_counts_25k.h5ad"",
    backup_url=""https://figshare.com/ndownloader/files/34464122""
)

# Store the raw counts in a separate layer before normalization.
adata.layers[""counts""] = adata.X.copy()

#--------------------------------
# Preprocessing & Quality Control
#--------------------------------

# Apply basic quality control filters to remove low-quality cells and genes.
print(""Performing quality control..."")
sc.pp.filter_cells(adata, min_genes=200)
sc.pp.filter_genes(adata, min_cells=3)

# Normalize each cell by total counts and apply a log transformation.
# This is a standard preprocessing pipeline for many single-cell analyses.
sc.pp.normalize_total(adata)
sc.pp.log1p(adata)

#--------------------------------
# Major Analysis Task: Ligand-Receptor Interaction Analysis
#--------------------------------

# --- Define Cell Populations of Interest ---
# Specify the cell types that will act as senders (source) and receivers (target)
# in the communication analysis.
source_cell_types = [""CD4 T cells"", ""B cells"", ""FCGR3A+ Monocytes""]
target_cell_types = [""CD8 T cells"", ""NK cells""]

# --- Run LIANA with CellPhoneDB Method ---
# Use the CellPhoneDB method within the LIANA framework to infer interactions.
# `groupby=""cell_type""` specifies the column with cell annotations.
# `use_raw=False` indicates that the normalized data in `adata.X` should be used.
print(""\nRunning ligand-receptor interaction analysis with CellPhoneDB..."")
from liana.method import cellphonedb
cellphonedb(
    adata,
    groupby=""cell_type"",
    use_raw=False,
    return_all_lrs=True,
    verbose=True
)

# --- Filter and Extract Results ---
# The results are stored in the `adata.uns['liana_res']` slot.
results = adata.uns[""liana_res""]

# Filter for significant ligand-receptor interactions based on the p-value.
significant_results = results[results[""cellphone_pvals""] <= 0.01]
print(f""\nFound {len(significant_results)} significant interactions (p-value <= 0.01)."")


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Generate a dot plot to visualize the top significant ligand-receptor interactions.
# - `colour`: The color of the dots represents the mean expression of the LR pair.
# - `size`: The size of the dots represents the significance (p-value).
# - `source_labels` & `target_labels`: Filter the plot to show interactions between specific cell types.
# - `top_n`: Display the top 20 interactions based on mean expression.
print(""\nGenerating dot plot of top interactions..."")
li.pl.dotplot(
    adata=adata,
    colour=""lr_means"",
    size=""cellphone_pvals"",
    inverse_size=True,  # Make smaller p-values correspond to larger dots
    source_labels=source_cell_types,
    target_labels=target_cell_types,
    filterby=""cellphone_pvals"",
    filter_lambda=lambda x: x <= 0.01,
    orderby=""lr_means"",
    orderby_ascending=False,
    top_n=20,
    figure_size=(9, 5),
    size_range=(1, 6)
)
plt.show()

#--------------------------------
# Data Output
#--------------------------------

# Save the significant interaction results to a CSV file for further inspection.
print(""\nSaving significant interaction results..."")
significant_results.to_csv(""liana_significant_interactions.csv"")

# Save the AnnData object, which now contains the LIANA results in the .uns slot.
adata.write(""kang_processed_with_liana.h5ad"")

print(""\nLigand-receptor interaction analysis completed!"")
",https://www.sc-best-practices.org/mechanisms/cell_cell_communication.html,F
scRNA-seq,Cell-cell communication,"Intercellular communication networks were examined in Python using the LIANA framework on a reduced PBMC dataset defined by pre-existing bulk labels. The CellPhoneDB method systematically tested for ligand-receptor interactions using a consensus database, requiring at least 10% cell expression for each gene. Interaction scores based on mean expression were complemented by p-values obtained via cluster label permutation. Filtering for interactions with p-values ≤ 0.05 revealed significant communication between source (CD34⁺, CD56⁺ NK, CD14⁺ Monocyte) and target (CD34⁺, CD56⁺ NK) populations, visualized with complementary dot and tile plots.","To infer potential cell-cell communication networks, we performed a ligand-receptor interaction analysis using the **LIANA** framework in Python. The analysis was conducted on a reduced peripheral blood mononuclear cell (PBMC) dataset, with cell populations defined by pre-existing bulk labels.

The **CellPhoneDB** method was employed within LIANA to systematically test for interactions. This analysis was based on a consensus database of known ligand-receptor pairs. For an interaction to be considered, the corresponding ligand or receptor gene was required to be expressed in at least 10% of the cells within a given population (`expr_prop=0.1`). The method calculates an interaction score based on the mean expression of the ligand in the source cell type and the receptor in the target cell type. A p-value is generated for each potential interaction by permuting the cluster labels to assess the specificity of the interaction.

The results were filtered to retain interactions with a p-value of 0.05 or less. The top significant interactions between specific source (**CD34+**, **CD56+ NK**, **CD14+ Monocyte**) and target (**CD34+**, **CD56+ NK**) populations were visualized using two complementary approaches. First, a dot plot was generated to summarize interaction strength (indicated by color) and statistical significance (indicated by dot size). Second, a tile plot was used to provide a more detailed view of the top 10 most significant interactions, displaying both the mean expression and the proportion of expressing cells for each ligand and receptor in the relevant cell types.","scanpy, liana, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import scanpy as sc
import liana as li
import matplotlib.pyplot as plt

#--------------------------------
# Data Input
#--------------------------------

# Load the pbmc68k_reduced dataset, a built-in example from Scanpy.
print(""Loading pbmc68k_reduced dataset..."")
adata = sc.datasets.pbmc68k_reduced()

#--------------------------------
# Major Analysis Task: Ligand-Receptor Interaction Analysis
#--------------------------------

# --- Run LIANA with CellPhoneDB Method ---
# Use the CellPhoneDB method within the LIANA framework to infer interactions.
# `groupby='bulk_labels'` specifies the column with cell annotations.
# `resource_name='consensus'` uses a curated list of LR pairs.
# `key_added='cpdb_res'` stores the results in the .uns slot of the AnnData object.
print(""\nRunning ligand-receptor interaction analysis with CellPhoneDB..."")
li.cellphonedb(
    adata,
    groupby='bulk_labels',
    resource_name='consensus',
    expr_prop=0.1,  # Minimum fraction of cells required to express ligand/receptor
    verbose=True,
    key_added='cpdb_res'
)

# --- Inspect Results ---
# Print the first few rows of the result DataFrame to check the output.
print(""\nCellPhoneDB Results (first 5 rows):"")
print(adata.uns['cpdb_res'].head())


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP of Cell Clusters ---
# Visualize the cell clusters to get an overview of the data structure.
sc.pl.umap(adata, color='bulk_labels', title='UMAP of PBMC68k Cell Types', frameon=False)

# --- Dot Plot of Top Interactions ---
# Generate a dot plot to visualize significant ligand-receptor interactions.
# - Color represents the interaction score (lr_means).
# - Size represents the statistical significance (p-value).
print(""\nGenerating dot plot of top interactions..."")
li.pl.dotplot(
    adata=adata,
    colour='lr_means',
    size='cellphone_pvals',
    inverse_size=True,  # Smaller p-values result in larger dots
    source_labels=['CD34+', 'CD56+ NK', 'CD14+ Monocyte'],
    target_labels=['CD34+', 'CD56+ NK'],
    figure_size=(8, 7),
    filter_fun=lambda x: x['cellphone_pvals'] <= 0.05,  # Filter for significant interactions
    uns_key='cpdb_res'
)
plt.show()

# --- Tile Plot of Expression Statistics ---
# Generate a tile plot to show detailed expression statistics (mean expression and
# proportion of expressing cells) for the top 10 interactions.
print(""\nGenerating tile plot of expression statistics..."")
li.pl.tileplot(
    adata=adata,
    fill='means',
    label='props',
    label_fun=lambda x: f'{x:.2f}',
    top_n=10,
    orderby='cellphone_pvals',
    orderby_ascending=True,
    source_labels=['CD34+', 'CD56+ NK', 'CD14+ Monocyte'],
    target_labels=['CD34+', 'CD56+ NK'],
    uns_key='cpdb_res',
    source_title='Ligand',
    target_title='Receptor',
    figure_size=(8, 7)
)
plt.show()

#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object, which now contains the LIANA results in the .uns slot.
print(""\nSaving annotated data object..."")
adata.write(""pbmc68k_with_liana_results.h5ad"")

print(""\nLigand-receptor interaction analysis completed!"")
",https://liana-py.readthedocs.io/en/latest/notebooks/basic_usage.html,F
scRNA-seq,Cell-cell communication,"Intercellular communication was interrogated using a multi-level analysis that combined ligand-receptor inference with tensor decomposition in Python via the LIANA and cell2cell libraries. A preprocessed single-cell dataset underwent quality control, normalization, and log-transformation before ligand-receptor interactions were independently inferred per sample with LIANA’s rank aggregation approach. A four-dimensional communication tensor was constructed—comprising sample, sender, receiver, and ligand-receptor dimensions—and subsequently decomposed into six latent factors. Visualization of sender, receiver, and interaction contributions elucidated context-dependent communication patterns across the experimental samples.","To uncover context-dependent shifts in intercellular communication, we performed a multi-level analysis combining ligand-receptor inference with tensor decomposition, using the `liana` and `cell2cell` Python libraries.

The analysis was performed on a pre-processed single-cell dataset. Initial quality control involved filtering out cells with fewer than 200 detected genes and genes present in fewer than three cells. The data was then normalized for library size differences and log-transformed.

First, to identify potential cell-cell interactions, we performed ligand-receptor (LR) inference independently for each experimental sample. Using a consensus database of known LR pairs, we applied LIANA's rank aggregation method to score and rank all potential interactions between cell types within each sample. This step produced a comprehensive catalog of sample-specific communication events.

Next, to identify higher-order patterns of communication that vary across samples, we constructed a four-dimensional communication tensor. The dimensions of this tensor represented the experimental context (sample), the sender cell type, the receiver cell type, and the specific ligand-receptor pair. The value for each point in the tensor was derived from the interaction's magnitude rank from the initial per-sample LIANA analysis. This tensor was then subjected to tensor decomposition, which factorized it into six distinct latent communication patterns. Each of these factors represents a coordinated system of interactions across specific contexts, cell types, and LR pairs. The results were visualized by plotting the contributions of senders, receivers, and LR pairs to each factor, as well as by generating communication network diagrams for factors of interest.","pandas, scanpy, liana, decoupler, cell2cell, seaborn, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scanpy as sc
import liana as li
import decoupler as dc
import cell2cell as c2c
import warnings

#--------------------------------
# Script Configuration
#--------------------------------

# Suppress warnings for a cleaner output during execution
warnings.filterwarnings('ignore')

#--------------------------------
# Data Input
#--------------------------------

# Load a sample dataset. Here we use a built-in dataset from LIANA's testing suite.
# NOTE: Replace this with your own data as needed.
print(""Loading Kang et al. 2018 dataset..."")
adata = li.testing.datasets.kang_2018()

#--------------------------------
# Preprocessing & Quality Control
#--------------------------------

# Apply basic quality control filters.
print(""Performing quality control and preprocessing..."")
sc.pp.filter_cells(adata, min_genes=200)
sc.pp.filter_genes(adata, min_cells=3)

# Normalize and log-transform the data.
sc.pp.normalize_total(adata)
sc.pp.log1p(adata)

#--------------------------------
# Major Analysis Tasks
#--------------------------------

# --- 1. Ligand-Receptor Inference by Sample ---
# Define keys for sample, condition, and cell type columns in adata.obs.
sample_key = 'sample'
condition_key = 'condition'
groupby = 'cell_type'

# Run LIANA's rank aggregation method for each sample individually.
# This infers ligand-receptor interactions within each sample before downstream analysis.
print(""\nRunning ligand-receptor inference for each sample..."")
liana_results = li.mt.rank_aggregate.by_sample(
    adata,
    groupby=groupby,
    resource_name='consensus',  # Use a consensus database of LR pairs
    sample_key=sample_key,
    use_raw=False,
    verbose=True,
    n_perms=None, # Set to an integer (e.g., 1000) for permutation testing
    return_all_lrs=True
)

# Store the LIANA results in the AnnData object.
adata.uns[""liana_res""] = liana_results

# Display the top-ranked interactions.
print(""\nTop aggregated ligand-receptor interactions:"")
print(adata.uns[""liana_res""].sort_values(""magnitude_rank"").head())

# --- 2. Tensor Decomposition for Communication Analysis ---
# Construct a communication tensor. This is a multi-dimensional array representing
# interactions (Context x Sender x Receiver x LR-pair).
print(""\nConstructing communication tensor..."")
tensor = li.multi.to_tensor_c2c(
    adata,
    sample_key=sample_key,
    score_key=""magnitude_rank"",
    how=""outer_cells"" # Use all cells for tensor construction
)

# Perform tensor decomposition to identify latent communication patterns (factors).
# `rank=6` specifies the number of factors to extract.
print(""\nPerforming tensor decomposition..."")
tensor = c2c.analysis.run_tensor_cell2cell_pipeline(
    tensor,
    copy_tensor=True,
    rank=6,
    tf_optimization='regular',
    random_state=0,
    device=""cpu"" # Use 'cuda' if a GPU is available
)

#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP Visualization ---
# Plot a UMAP to visualize the overall structure of the data by condition and cell type.
print(""\n--- Visualizing Results ---"")
sc.pl.umap(adata, color=[condition_key, groupby], frameon=False)

# --- Tensor Factor Plots ---
# Visualize the results of the tensor decomposition. Each factor represents a
# distinct communication pattern across contexts, senders, receivers, and LR pairs.
factors, axes = c2c.plotting.tensor_factors_plot(
    interaction_tensor=tensor,
    fontsize=10
)
plt.show()

# --- Communication Network Plots ---
# Visualize the cell-cell communication networks for specific factors.
# Here, we plot the network associated with 'Factor 6' as an example.
c2c.plotting.ccc_networks_plot(factors, included_factors=['Factor 6'])
plt.show()

#--------------------------------
# Data Output
#--------------------------------

# Save the final plots and data objects.
print(""\nSaving results..."")
# Save the tensor factor plot
factors[0].figure.savefig(""tensor_factors_plot.png"", dpi=300, bbox_inches='tight')
# Save the AnnData object with LIANA results
adata.write(""kang_2018_with_liana.h5ad"")
# Save the tensor object with decomposition results
c2c.io.export_variable_to_file(tensor, 'communication_tensor.c2c')

print(""\nTensor-based communication analysis completed!"")
",https://liana-py.readthedocs.io/en/latest/notebooks/liana_c2c.html,F
scRNA-seq,Cell-cell communication,"Condition-dependent communication and signaling shifts were elucidated on a preprocessed PBMC dataset by integrating pseudobulk differential expression, ligand-receptor interaction analysis, and pathway enrichment. Pseudobulk profiles generated via cell-type aggregation enabled differential expression comparisons between stimulated and control conditions using PyDESeq2. These expression changes informed a LIANA-based analysis to identify deregulated ligand-receptor interactions, while pathway enrichment with Decoupler linked the observed alterations to changes in transcription factor activity. This comprehensive approach delineates the regulatory mechanisms underlying the stimulus-induced modulation of intercellular communication.","To identify condition-dependent changes in intercellular communication and downstream signaling, we performed a multi-level analysis combining pseudobulk differential expression, ligand-receptor interaction analysis, and pathway enrichment.

The analysis was performed on a pre-processed dataset of peripheral blood mononuclear cells (PBMCs). First, to enable sample-level statistical testing, we generated **pseudobulk expression profiles** by summing the raw gene counts for each cell type within each individual sample. This aggregation step creates profiles that are amenable to statistical models designed for bulk RNA-seq data.

Using these pseudobulk profiles, we performed **differential expression analysis (DEA)** for each cell type separately using the PyDESeq2 package. We compared the stimulated ('stim') condition to the control ('ctrl') condition, with 'ctrl' serving as the reference level. Log-fold changes were shrunk to improve the robustness of the estimates. This analysis yielded a comprehensive list of genes that were significantly up- or down-regulated in each cell type in response to stimulation.

To understand how these expression changes might alter cell-cell communication, we performed a **deregulated ligand-receptor (LR) interaction analysis** using the LIANA framework. The cell-type-specific differential expression statistics (log-fold changes and p-values) were used as input to score and rank LR pairs from a consensus database. This approach identifies interactions where the expression of the ligand, the receptor, or both is significantly altered between the stimulated and control conditions, thus pinpointing communication pathways that are modulated by the stimulus. The top deregulated interactions were visualized using a tile plot.

Finally, to infer the intracellular consequences of these expression changes, we performed a **pathway enrichment analysis** focused on transcription factor (TF) activity using the Decoupler package. We used the cell-type-specific DEA t-statistics as input for a Univariate Linear Model (ULM) to estimate the activity changes of TFs, based on a comprehensive TF-target regulatory network from the CollecTRI resource. This allowed us to connect the observed differential gene expression to potential upstream regulatory changes.","numpy, pandas, scanpy, plotnine, liana, decoupler, omnipath, pydeseq2",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scanpy as sc
import liana as li
import decoupler as dc
import omnipath as op
from pydeseq2.dds import DeseqDataSet
from pydeseq2.ds import DeseqStats
import warnings

#--------------------------------
# Script Configuration
#--------------------------------

# Suppress warnings for a cleaner output during execution
warnings.filterwarnings('ignore')

#--------------------------------
# Data Input
#--------------------------------

# Load a sample dataset. Here we use a built-in dataset from LIANA's testing suite.
# NOTE: Replace this with your own data as needed.
print(""Loading Kang et al. 2018 dataset..."")
adata = li.testing.datasets.kang_2018()

#--------------------------------
# Preprocessing & Quality Control
#--------------------------------

# Apply basic quality control filters.
print(""Performing quality control and preprocessing..."")
sc.pp.filter_cells(adata, min_genes=200)
sc.pp.filter_genes(adata, min_cells=3)

# Normalize and log-transform data for visualization and some downstream steps.
sc.pp.normalize_total(adata, target_sum=1e4)
sc.pp.log1p(adata)

#--------------------------------
# Major Analysis Tasks
#--------------------------------

# Define metadata columns of interest for the analysis
sample_key = 'sample'
condition_key = 'condition'
groupby = 'cell_abbr' # Using abbreviated cell types for clarity

# --- 1. Pseudobulk and Differential Expression Analysis (DEA) ---
# Generate pseudobulk profiles by summing counts for each cell type within each sample.
print(""\nGenerating pseudobulk profiles..."")
pdata = dc.get_pseudobulk(
    adata,
    sample_col=sample_key,
    groups_col=groupby,
    layer='counts', # Use raw counts for DESeq2
    mode='sum',
    min_cells=10,
    min_counts=10000
)

# Perform DEA using PyDESeq2 for each cell type.
print(""\nPerforming differential expression analysis with PyDESeq2..."")
dea_results = {}
for cell_group in pdata.obs[groupby].unique():
    # Subset pseudobulk data to the current cell type
    ctdata = pdata[pdata.obs[groupby] == cell_group].copy()

    # Filter out lowly expressed genes for the current cell type
    genes = dc.filter_by_expr(ctdata, group=condition_key, min_count=5, min_total_count=10)
    ctdata = ctdata[:, genes].copy()

    # Build and run the DESeq2 model
    dds = DeseqDataSet(
        adata=ctdata,
        design_factors=condition_key,
        ref_level=[condition_key, 'ctrl'], # Set 'ctrl' as the reference condition
        refit_cooks=True,
        quiet=True
    )
    dds.deseq2()

    # Get statistics and shrink log-fold changes for robustness
    stat_res = DeseqStats(dds, contrast=[condition_key, 'stim', 'ctrl'])
    stat_res.summary()
    stat_res.lfc_shrink(coeff='condition_stim_vs_ctrl')

    dea_results[cell_group] = stat_res.results_df

# Concatenate all DEA results into a single DataFrame.
dea_df = pd.concat(dea_results).reset_index().rename(
    columns={'level_0': groupby, 'level_1': 'gene'}
).set_index('gene')


# --- 2. Deregulated Ligand-Receptor Interaction Analysis ---
# Use LIANA to identify LR interactions that are significantly altered between conditions,
# based on the differential expression statistics calculated above.
print(""\nIdentifying deregulated ligand-receptor interactions..."")
lr_res = li.multi.df_to_lr(
    adata,
    dea_df=dea_df,
    resource_name='consensus',
    expr_prop=0.1,
    groupby=groupby,
    stat_keys=['stat', 'pvalue', 'padj'],
    use_raw=False, # Use log-normalized data for expression proportion check
    complex_col='stat',
    return_all_lrs=False
)

# Sort the results by the interaction statistic.
lr_res = lr_res.sort_values(""interaction_stat"", ascending=False)


# --- 3. Pathway Enrichment Analysis ---
# Use the DEA results to infer transcription factor (TF) activity changes.
print(""\nPerforming pathway enrichment analysis (TF activity)..."")
# Get a TF-target regulatory network from Omnipath's CollecTRI resource.
net = dc.get_collectri()

# Pivot the DEA results to a wide format (cells x genes) with stat values.
dea_wide = dea_df[[groupby, 'stat']].reset_index(names='genes').pivot(
    index=groupby, columns='genes', values='stat'
).fillna(0)

# Run Univariate Linear Model (ULM) from Decoupler to estimate TF activities.
estimates, pvals = dc.run_ulm(mat=dea_wide, net=net)

# Print the top TFs associated with the 'CD14' Monocyte group as an example.
top_tfs = estimates.T.sort_values('CD14', key=abs, ascending=False).head()
print(""\nTop TFs associated with CD14 Monocytes:"")
print(top_tfs)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP Visualization ---
# Plot a UMAP to visualize the overall structure of the data.
print(""\n--- Visualizing Results ---"")
sc.pl.umap(adata, color=[condition_key, sample_key, 'cell_type', groupby], frameon=False, ncols=2)

# --- Tile Plot of Top Interactions ---
# Visualize the top 15 deregulated ligand-receptor interactions.
# Fill color represents expression, while '*' indicates statistical significance.
li.pl.tileplot(
    liana_res=lr_res,
    fill='expr',
    label='padj',
    label_fun=lambda x: '*' if x < 0.05 else np.nan,
    top_n=15,
    orderby='interaction_stat',
    orderby_ascending=False,
    source_title='Ligand',
    target_title='Receptor'
)


#--------------------------------
# Data Output
#--------------------------------

# Save the key results DataFrames to CSV files.
print(""\nSaving analysis results..."")
dea_df.to_csv(""differential_expression_results.csv"")
lr_res.to_csv(""deregulated_lr_interactions.csv"")
estimates.to_csv(""tf_activity_estimates.csv"")

# Save the processed AnnData object.
adata.write(""kang_processed_for_dea.h5ad"")

print(""\nDifferential communication and pathway analysis completed!"")
",https://liana-py.readthedocs.io/en/latest/notebooks/targeted.html,F
scRNA-seq,Enrichment analysis,"Biological signaling pathway activity was inferred at the single-cell level using Python’s Decoupler package on 3,000 preprocessed PBMCs. The curated PROGENy model, featuring 14 core pathways with weighted downstream targets, guided the enrichment analysis. A univariate linear model computed pathway activity scores for each cell based on gene expression data. The TNFa pathway activity was visualized on a UMAP embedding, while violin plots and a comprehensive heatmap illustrated the distribution and z-scaled activity of all pathways across cell types.","To infer the activity of biological signaling pathways at the single-cell level, we performed a pathway enrichment analysis using the **Decoupler** package in Python.

The analysis was conducted on a pre-processed dataset of 3,000 peripheral blood mononuclear cells (PBMCs). We used the curated **PROGENy** model, which provides a set of 14 core signaling pathways and their downstream target genes with signed weights indicating the direction of regulation.

Pathway activity scores were computed for each individual cell by running a **Univariate Linear Model (ULM)**. This method uses the expression data from each cell and the PROGENy gene sets to estimate the activity level for each of the 14 pathways. The resulting pathway activity scores were then visualized to explore their distribution across the dataset. Specifically, the activity of the TNFa pathway was overlaid on the UMAP embedding to identify cells with high activation. Furthermore, violin plots were used to compare the distribution of pathway activities across different cell types, and a comprehensive heatmap was generated to display the z-scaled activity of all 14 pathways across all identified cell populations.","scanpy, decoupler, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for single-cell analysis, pathway analysis, and plotting
import scanpy as sc
import decoupler as dc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters for clear and consistent visualization
sc.set_figure_params(figsize=(5, 5), frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load an example dataset (PBMC 3k) from Decoupler's built-in datasets.
# This dataset is pre-processed with clustering information.
print(""Loading pbmc3k dataset..."")
adata = dc.ds.pbmc3k()

# Display a summary of the loaded dataset.
print(adata)


#--------------------------------
# Major Analysis Task: Pathway Enrichment
#--------------------------------

# --- Load Pathway Gene Sets ---
# Get the PROGENy model, which contains a curated set of pathway gene sets
# with signed weights representing up/down-regulation.
print(""\nLoading PROGENy pathway model..."")
progeny = dc.op.progeny(organism='human')

# --- Compute Enrichment Scores ---
# Run the Univariate Linear Model (ulm) from Decoupler to compute pathway
# enrichment scores for each cell.
# The result is stored in `adata.obsm['ulm_estimate']`.
print(""Computing pathway enrichment scores..."")
dc.run_ulm(data=adata, net=progeny, source='source', target='target', weight='weight', min_n=5)

# Extract the computed enrichment scores into a separate AnnData object for easier plotting.
# This creates a new object where the .X matrix contains the pathway scores.
scores = dc.get_obsm_to_anndata(adata, obsm_key='ulm_estimate')


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP of Cell Clusters ---
# Visualize the original cell clusters from the dataset.
print(""\n--- Visualizing Results ---"")
sc.pl.umap(adata, color='leiden', title='UMAP of PBMC Clusters')

# --- UMAP of Pathway Activity ---
# Visualize the activity of a specific pathway (e.g., TNFa signaling) on the UMAP.
# Red indicates higher pathway activity, and blue indicates lower activity.
sc.pl.umap(scores, color=['TNFa'], cmap='RdBu_r', vcenter=0, title='TNFa Pathway Activity')

# --- Violin Plot of Pathway Activity ---
# Compare the distribution of pathway activity scores across different cell types.
sc.pl.violin(scores, keys=['TNFa'], groupby='celltype', rotation=90)

# --- Matrix Plot of All Pathways ---
# Show a heatmap of all pathway activities across all cell types.
# The scores are z-scaled for better comparability across pathways.
sc.pl.matrixplot(
    adata=scores,
    var_names=scores.var_names,
    groupby='celltype',
    dendrogram=True,
    standard_scale='var',
    colorbar_title='Z-scaled scores',
    cmap='RdBu_r'
)
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object, which now contains the pathway enrichment scores.
print(""\nSaving data with enrichment scores..."")
# Note: The scores are in adata.obsm['ulm_estimate'], and adata.obsm['ulm_pvals']
adata.write(""pbmc3k_with_progeny_scores.h5ad"")

print(""\nPathway enrichment analysis completed!"")
",https://decoupler-py.readthedocs.io/en/latest/notebooks/scell/rna_sc.html,F
scRNA-seq,Enrichment analysis,"Transcriptional changes in distinct cell types associated with COVID-19 were elucidated through a pseudobulk workflow integrating differential expression and pathway enrichment. A single‐cell dataset was aggregated into sample-level profiles per cell type, excluding low-quality samples. Differential expression analysis using PyDESeq2, with a model that accounted for disease status and patient sex, identified key transcriptional shifts between COVID-19 and normal conditions. These results informed a downstream pathway enrichment using Decoupler with curated PROGENy gene sets to estimate the activity of 14 core signaling pathways. Visualizations with volcano and bar plots highlighted significant gene changes and pathway alterations.","To identify cell-type-specific transcriptional changes associated with disease status, we performed a pseudobulk differential expression and pathway analysis workflow using the Scanpy, PyDESeq2, and Decoupler Python libraries.

The analysis began with a single-cell COVID-19 dataset. To facilitate sample-level comparisons, we first generated **pseudobulk expression profiles**. This was achieved by aggregating the raw gene counts, summing them for each distinct cell type within each individual patient sample. Low-quality pseudobulk samples, defined as those with fewer than 10 constituent cells or under 10,000 total counts, were removed.

Using these pseudobulk profiles, we conducted **differential expression analysis (DEA)** for each cell type separately with the PyDESeq2 package. To model the sources of variation accurately, our design formula accounted for both disease status and patient sex (`~ disease + sex`). We specifically tested for genes that were differentially expressed between the 'COVID-19' and 'normal' conditions.

To understand the functional consequences of these gene expression changes, we next performed a **pathway enrichment analysis**. The t-statistics derived from the DEA for each cell type were used as input for the Decoupler package. We estimated pathway activities using a **Univariate Linear Model (ULM)** against the curated **PROGENy** pathway gene sets. This method allowed us to infer the activation or inhibition of 14 core signaling pathways based on the collective changes in their target genes. The results were visualized using volcano plots to highlight significant gene changes and bar plots to display significantly altered pathways.","numpy, scanpy, pandas,decoupler",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import numpy as np
import pandas as pd
import scanpy as sc
import decoupler as dc
import matplotlib.pyplot as plt
from pydeseq2.dds import DeseqDataSet, DefaultInference
from pydeseq2.ds import DeseqStats

#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters for clear and consistent visualization
sc.set_figure_params(figsize=(4, 4), frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Load an example single-cell RNA sequencing dataset.
# Here, we use a COVID-19 dataset from Decoupler's built-in datasets.
print(""Loading covid5k dataset..."")
adata = dc.ds.covid5k()

# Display a summary of the loaded dataset.
print(adata)


#--------------------------------
# Preprocessing and Pseudobulking
#--------------------------------

# --- Pseudobulking ---
# Aggregate single-cell expression data into pseudobulk profiles.
# This sums the counts for each cell type within each individual sample.
print(""\nGenerating pseudobulk profiles..."")
pdata = dc.pp.pseudobulk(
    adata=adata,
    sample_col='individual',
    groups_col='celltype',
    mode='sum' # Sum counts for each gene
)

# --- Pseudobulk QC and Preprocessing ---
# Filter out low-quality pseudobulk samples.
dc.pp.filter_samples(pdata, min_cells=10, min_counts=1000)

# Normalize and preprocess the pseudobulk data for visualization and PCA.
# Note: Raw counts are kept for DESeq2 analysis.
pdata.layers['counts'] = pdata.X.copy()
sc.pp.normalize_total(pdata, target_sum=1e4)
sc.pp.log1p(pdata)
sc.pp.scale(pdata, max_value=10)


#--------------------------------
# Major Analysis Tasks
#--------------------------------

# --- 1. Differential Expression Analysis (DEA) with PyDESeq2 ---
# Define and run the DESeq2 model on the raw pseudobulk counts.
# The design formula `~disease + sex` models the effect of disease while accounting for sex.
print(""\nPerforming differential expression analysis with PyDESeq2..."")
inference = DefaultInference(n_cpus=8)
dds = DeseqDataSet(
    adata=pdata,
    design_factors=['disease', 'sex'],
    refit_cooks=True,
    inference=inference
)
dds.deseq2()

# Extract statistical results for the contrast between COVID-19 and normal conditions.
stat_res = DeseqStats(
    dds,
    contrast=['disease', 'COVID-19', 'normal'],
    inference=inference
)
stat_res.summary()
results_df = stat_res.results_df


# --- 2. Pathway Enrichment Analysis ---
# Use the DEA results to infer pathway activity changes with Decoupler.
print(""\nPerforming pathway enrichment analysis..."")
# Get the PROGENy model, a curated set of pathway gene sets.
progeny = dc.op.progeny(organism='human')

# Run Univariate Linear Model (ulm) to estimate pathway activities from the DE statistics.
pw_acts, pw_padj = dc.run_ulm(
    mat=results_df[['stat']].T.rename(index={'stat': 'disease.vs.normal'}),
    net=progeny
)

# Filter for significantly altered pathways.
significant_mask = (pw_padj.T < 0.05).iloc[:, 0]
significant_pw_acts = pw_acts.loc[:, significant_mask]


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP of Single-Cell Data ---
# Visualize the initial cell type and disease state distribution.
print(""\n--- Visualizing Results ---"")
sc.pl.umap(adata, color=['celltype', 'disease'], ncols=1, title=[""Cell Types"", ""Disease State""])

# --- Pseudobulk QC Plot ---
# Visualize the cell type composition of the filtered pseudobulk samples.
dc.pl.obsbar(adata=pdata, y='celltype', hue='disease', figsize=(6, 3))

# --- PCA of Pseudobulk Data ---
# Explore the major sources of variation across the pseudobulk samples.
sc.tl.pca(pdata)
sc.pl.pca(pdata, color='disease', size=100, title=""PCA of Pseudobulk Samples"")

# --- Volcano Plot of DEA Results ---
# Visualize the differential expression results, highlighting significant genes.
dc.pl.volcano(results_df, x='log2FoldChange', y='pvalue')

# --- Bar Plot of Pathway Enrichment ---
# Visualize the pathway enrichment results for significantly altered pathways.
dc.pl.barplot(data=significant_pw_acts, name='disease.vs.normal', figsize=(3, 3))


#--------------------------------
# Data Output
#--------------------------------

# Save the key results DataFrames and the processed AnnData object.
print(""\nSaving analysis results..."")
results_df.to_csv(""pseudobulk_dea_results.csv"")
significant_pw_acts.to_csv(""significant_pathway_activities.csv"")
pdata.write(""pseudobulk_processed_data.h5ad"")

print(""\nPseudobulk differential analysis completed!"")
",https://decoupler-py.readthedocs.io/en/latest/notebooks/scell/rna_psbk.html,F
scRNA-seq,Enrichment analysis,"Regulatory dynamics during mouse gastrulation were explored using Python-based trajectory and enrichment analyses. A continuous developmental timeline was reconstructed via Diffusion Pseudotime based on a PCA-derived k-nearest neighbor graph, with ""Blood progenitors 1"" defined as the trajectory root and inter-cell connectivity assessed by Partition-based Graph Abstraction. Transcription factor and signaling pathway activities were subsequently inferred using Univariate Linear Models with CollecTRI- and PROGENy-derived gene sets. Activity scores were visualized on UMAP, delineating cell state–specific regulatory programs and developmental transitions.","To investigate the regulatory dynamics underlying developmental processes, we performed a trajectory and enrichment analysis on a single-cell dataset of mouse gastrulation using the **Scanpy** and **Decoupler** libraries in Python.

First, to reconstruct the developmental timeline, we inferred a continuous trajectory using **Diffusion Pseudotime (DPT)**. A k-nearest neighbor graph was constructed based on a pre-computed Principal Component Analysis (PCA) representation, and this was used to compute a diffusion map of the cellular manifold. A root for the trajectory was defined within the ""Blood progenitors 1"" cell population, and DPT was then calculated to order each cell by its diffusion distance from this starting point. The high-level connectivity between cell types was also assessed using **Partition-based Graph Abstraction (PAGA)**.

Next, to characterize the regulatory programs driving this trajectory, we performed two single-cell enrichment analyses using the **Decoupler** framework. First, we inferred **transcription factor (TF) activity** for each cell by applying a Univariate Linear Model (ULM) with a comprehensive TF-target gene network from the CollecTRI resource. In parallel, we inferred the activity of 14 core signaling pathways for each cell using the ULM method with the curated **PROGENy** pathway model. The resulting TF and pathway activity scores were then visualized on the UMAP embedding and compared across cell types to identify regulatory programs associated with specific cell states and developmental transitions.","numpy, scanpy, pandas,decoupler",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import numpy as np
import scanpy as sc
import decoupler as dc

#--------------------------------
# Script Configuration
#--------------------------------

# Set global plotting parameters for clear and consistent visualization
sc.set_figure_params(figsize=(5, 5), frameon=False)
sc.settings.verbosity = 3  # Set for detailed logging

#--------------------------------
# Data Input
#--------------------------------

# Load the mouse gastrulation dataset from Decoupler's built-in datasets.
print(""Loading mouse gastrulation dataset..."")
adata = dc.ds.erygast1k()

#--------------------------------
# Preprocessing & Trajectory Inference
#--------------------------------

# --- Preprocessing ---
# The loaded data is already log-normalized. We proceed to dimensionality reduction.
# Compute neighbors using the pre-computed PCA representation.
print(""\nPreprocessing and computing neighbors..."")
sc.pp.neighbors(adata, use_rep='X_pca')

# --- Trajectory Inference with PAGA/DPT ---
# Compute a diffusion map, which is the basis for DPT.
sc.tl.diffmap(adata)

# Recompute neighbors on the diffusion map to better capture the manifold.
sc.pp.neighbors(adata, use_rep='X_diffmap')

# Run Partition-based Graph Abstraction (PAGA) to get a high-level
# view of the connectivity between cell types.
sc.tl.paga(adata, groups='celltype')

# Define the root of the trajectory for pseudotime calculation.
root_celltype = ""Blood progenitors 1""
iroot = np.flatnonzero(adata.obs['celltype'] == root_celltype)[0]
adata.uns['iroot'] = iroot

# Compute Diffusion Pseudotime (DPT) to order cells along the trajectory.
sc.tl.dpt(adata)


#--------------------------------
# Major Analysis Tasks: Enrichment Analysis
#--------------------------------

# --- 1. Transcription Factor (TF) Activity Analysis ---
print(""\nPerforming TF activity enrichment analysis..."")
# Retrieve the CollecTRI TF-target network for mouse from Decoupler's resources.
collectri = dc.op.collectri(organism='mouse')

# Run the Univariate Linear Model (ulm) to estimate TF activity scores for each cell.
# The results are stored in adata.obsm['ulm_estimate'] and adata.obsm['ulm_pvals'].
dc.run_ulm(data=adata, net=collectri, source='source', target='target', weight='weight', min_n=5)

# Extract the TF activity scores into a separate AnnData object for easier plotting.
# This creates a new object where the .X matrix contains the TF scores.
score_tf = dc.get_obsm_to_anndata(adata, obsm_key='ulm_estimate')


# --- 2. Pathway Activity Analysis ---
print(""\nPerforming pathway activity enrichment analysis..."")
# Retrieve the PROGENy pathway model for mouse.
progeny = dc.op.progeny(organism='mouse')

# Run ULM again to estimate pathway activity scores.
# Note: This overwrites the previous results in adata.obsm['ulm_*'], which is why
# we extracted the TF scores to a separate object first.
dc.run_ulm(data=adata, net=progeny, source='source', target='target', weight='weight', min_n=5)

# Extract the pathway activity scores into another AnnData object.
score_pathway = dc.get_obsm_to_anndata(adata, obsm_key='ulm_estimate')


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP of Raw Data and Trajectory ---
print(""\n--- Visualizing Results ---"")
# Visualize the initial UMAP colored by cell type and developmental stage.
sc.pl.umap(adata, color=['celltype', 'stage'], ncols=1, title=[""Cell Types"", ""Developmental Stage""])

# Visualize the DPT pseudotime ordering on the UMAP.
sc.pl.umap(adata, color=['dpt_pseudotime'], title=""Diffusion Pseudotime"")


# --- TF Activity Visualization ---
# Choose a transcription factor to visualize, e.g., ""Klf3"".
tf = 'Klf3'
# Plot a UMAP colored by the Klf3 enrichment score.
sc.pl.umap(score_tf, color=[tf], cmap='RdBu_r', vcenter=0, title=f""{tf} Activity"")
# Plot a violin plot showing the distribution of the Klf3 score across cell types.
sc.pl.violin(score_tf, keys=[tf], groupby='celltype', rotation=90, ylabel=f'{tf} score')


# --- Pathway Activity Visualization ---
# Choose a pathway to visualize, e.g., ""PI3K"".
pathway = 'PI3K'
# Plot a UMAP colored by the PI3K pathway enrichment score.
sc.pl.umap(score_pathway, color=[pathway], cmap='RdBu_r', vcenter=0, title=f""{pathway} Pathway Activity"")
# Plot a violin plot showing the distribution of the PI3K score across cell types.
sc.pl.violin(score_pathway, keys=[pathway], groupby='celltype', rotation=90, ylabel=f'{pathway} score')


#--------------------------------
# Data Output
#--------------------------------

# Save the final AnnData object, which contains all analysis results.
print(""\nSaving final AnnData object..."")
adata.write(""erygast1k_with_trajectory_and_enrichment.h5ad"")

print(""\nTrajectory and enrichment analysis completed!"")
",https://decoupler-py.readthedocs.io/en/latest/notebooks/scell/rna_pstime.html,F
scRNA-seq,Gene regulatory networks analysis,"Gene regulatory networks were inferred on a multi-modal bone marrow dataset using the SCENIC pipeline. The analysis focused on the gene expression modality by log-transforming raw counts and selecting highly variable genes with a batch-aware Seurat method. A sample (donor s1d1) was exported to a .loom file, incorporating relevant cell-level metadata for quality assessment. GRN inference with the pyscenic grn tool and a curated transcription factor list identified co-expression modules and predicted transcription factor-target links, yielding adjacencies weighted by importance scores. Data quality and network distributions were validated through visualization.","To infer gene regulatory networks (GRNs), we prepared a single-cell dataset for analysis with the **SCENIC** pipeline.

The analysis was performed on a multi-modal bone marrow dataset. The gene expression modality was first isolated, and the raw counts were log-transformed. To focus the analysis on genes most likely to be involved in defining cell identity, feature selection was performed by identifying highly variable genes (HVGs) using a batch-aware implementation of the Seurat method.

For computational demonstration and compatibility with the SCENIC command-line interface, the preprocessed data was subset to a single representative sample (donor `s1d1`). This data was then exported to the `.loom` file format, a requirement for the `pyscenic` tool. The loom file was structured with genes as rows and cells as columns, and included cell-level metadata such as the number of genes detected and the total UMI counts per cell.

The core GRN inference was executed externally using the `pyscenic grn` command-line tool. This step uses the expression matrix in conjunction with a curated list of known human transcription factors to identify co-expression modules and infer potential regulatory links between transcription factors and their target genes. The output of this step is a table of regulatory adjacencies, where each row represents a predicted link between a transcription factor and a target, weighted by an importance score. The quality of the input data was confirmed by visualizing the number of genes detected per cell, and the resulting GRN importance scores were examined to assess the distribution of regulatory link strengths.","pyscenic, loompy, matplotlib, numpy, pandas, scanpy, seaborn","python, bash","#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for system operations, data handling, analysis, and plotting
import warnings
from pathlib import Path
import loompy
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scanpy as sc
import seaborn as sns

#--------------------------------
# Script Configuration
#--------------------------------

# Suppress warnings for a cleaner output during execution
warnings.filterwarnings(""ignore"")
# Set global plotting parameters
sc.set_figure_params(facecolor=""white"")
sns.set_theme()

#--------------------------------
# Data Input
#--------------------------------

# Load the single-cell RNA-seq data.
# NOTE: Replace the path with the actual location of your data file.
adata = sc.read_h5ad(""path/to/your_openproblems_bmmc_multiome_genes_filtered.h5ad"")
print(""Initial dataset shape:"", adata.shape)

#--------------------------------
# Data Preprocessing
#--------------------------------

# Filter for gene expression (GEX) features if the dataset is multi-modal.
rna = adata[:, adata.var[""feature_types""] == ""GEX""].copy()
del adata  # Free up memory

# Apply a log-transformation to the data.
sc.pp.log1p(rna)

# Identify highly variable genes (HVGs) using a batch-aware method.
sc.pp.highly_variable_genes(rna, batch_key=""batch"", flavor=""seurat"")
print(""Preprocessing complete."")


#--------------------------------
# Major Analysis Task: Prepare for SCENIC
#--------------------------------

# --- Step 1: Subset Data for Analysis ---
# For demonstration, we subset the data to a single batch/donor ('s1d1').
# SCENIC can be run on the full dataset, but this reduces computational time for an example.
adata_batch = rna[rna.obs[""batch""] == ""s1d1"", :].copy()
print(f""\nSubsetting data to batch 's1d1'. New shape: {adata_batch.shape}"")


# --- Step 2: Create Loom File for SCENIC ---
# SCENIC requires a specific file format (.loom) for its command-line interface.
# We create this file from our preprocessed AnnData object.
loom_path = ""data/neurips_processed_input.loom""
Path(""data"").mkdir(exist_ok=True) # Ensure the directory exists

# Define row and column attributes required by the loom format.
row_attrs = {""Gene"": np.array(adata_batch.var.index)}
col_attrs = {
    ""CellID"": np.array(adata_batch.obs.index),
    ""nGene"": np.array(np.sum(adata_batch.X > 0, axis=1)).flatten(),
    ""nUMI"": np.array(np.sum(adata_batch.X, axis=1)).flatten(),
}

# Create the loom file. Note that the expression matrix must be transposed.
print(f""Creating loom file at: {loom_path}"")
loompy.create(loom_path, adata_batch.X.T, row_attrs, col_attrs)


#--------------------------------
# External Step: Run pyscenic
#--------------------------------

# The core GRN inference in SCENIC is run via a command-line tool.
# This step is performed outside of this script in a terminal environment.
#
# EXAMPLE COMMAND:
# pyscenic grn data/neurips_processed_input.loom allTFs_hg38.txt \
#          -o adj.csv --num_workers 3
#
# - `data/neurips_processed_input.loom`: The input file we created.
# - `allTFs_hg38.txt`: A list of known transcription factors.
# - `-o adj.csv`: The output file containing gene regulatory network adjacencies.
#
# This command produces a file ('adj.csv') which we load in the next step.
print(""\nSCENIC GRN inference must be run externally via the command line."")


#--------------------------------
# Post-Analysis: Load and Visualize GRN Results
#--------------------------------

# Assuming the 'pyscenic grn' command has been run, we now load its output.
try:
    results_adjacencies = pd.read_csv(""adj.csv"")
    print(f""\nSuccessfully loaded GRN results. Found {results_adjacencies.shape[0]} associations."")
    print(results_adjacencies.head())
except FileNotFoundError:
    print(""\n'adj.csv' not found. Please run the external pyscenic command first."")
    results_adjacencies = None


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- UMAP Visualization ---
# Plot UMAPs to visualize the overall data structure before and after subsetting.
# Note: This assumes 'GEX_X_umap' coordinates are pre-computed in the loaded file.
print(""\n--- Visualizing Data ---"")
sc.pl.embedding(rna, ""GEX_X_umap"", color=[""cell_type"", ""batch""], title=""UMAP: All Cells"")
sc.pl.embedding(adata_batch, ""GEX_X_umap"", color=[""cell_type"", ""batch""], title=""UMAP: Donor s1d1"")

# --- GRN Importance Score Distribution ---
# If GRN results were loaded, plot a histogram of the importance scores.
# This score reflects the strength of the regulatory link between a TF and its target.
if results_adjacencies is not None:
    plt.figure(figsize=(8, 5))
    plt.hist(np.log10(results_adjacencies[""importance""] + 1e-10), bins=50, color=""skyblue"", edgecolor=""black"")
    plt.xlabel(""Log10(Importance Score)"")
    plt.ylabel(""Frequency"")
    plt.title(""Distribution of GRN Importance Scores"")
    plt.show()

# --- Genes Detected per Cell ---
# Plot the distribution of the number of genes detected per cell. This is a useful QC metric.
n_genes_detected_per_cell = np.sum(adata_batch.X > 0, axis=1)
n_genes_series = pd.Series(n_genes_detected_per_cell.A.flatten() if hasattr(n_genes_detected_per_cell, 'A') else n_genes_detected_per_cell.flatten())

fig, ax = plt.subplots(figsize=(8, 5), dpi=100)
sns.histplot(n_genes_series, bins=""fd"", edgecolor=""black"", ax=ax)
ax.set_xlabel(""Number of Genes Detected"")
ax.set_ylabel(""Number of Cells"")
ax.set_title(""Distribution of Genes Detected per Cell (Donor s1d1)"")
plt.tight_layout()
plt.show()

#--------------------------------
# Data Output
#--------------------------------

# Save the subsetted AnnData object used for the analysis.
print(""\nSaving subsetted AnnData object..."")
adata_batch.write(""s1d1_preprocessed_for_scenic.h5ad"")

print(""\nSCENIC data preparation script completed!"")
",https://www.sc-best-practices.org/mechanisms/gene_regulatory_networks.html,F
scRNA-seq,Annotation,"Large-scale copy number variations were inferred using the inferCNV R package on a raw integer expression matrix derived from a pre-processed Seurat object. Cell barcodes were annotated by cluster, with designated normal clusters serving as a diploid baseline. Gene order information arranged analyses along chromosomal coordinates, and expression values were centered against the reference to smooth residual expression. This approach revealed amplifications and deletions across the genome, with the final CNV landscape visualized as a comprehensive heatmap.","To infer large-scale copy number variations (CNVs) from the single-cell transcriptomic data, we used the **inferCNV** R package.

The analysis was performed on a raw integer gene expression matrix derived from the pre-processed Seurat object. A cell annotation file was created, mapping each cell barcode to its corresponding Seurat cluster ID. To serve as a diploid baseline, cells belonging to clusters 0 and 8 were designated as the reference ""normal"" cell populations. The genomic positions of genes were provided via a gene order file to arrange the analysis along chromosomal coordinates.

The core **inferCNV** analysis was executed using the `run` function. Within this pipeline, gene expression values were centered by subtracting the average expression of the reference cell populations. A moving average of residual expression was calculated for each cell in a window of 101 genes across each chromosome. This process smooths the expression signal and reveals large-scale genomic regions with consistently higher or lower expression relative to the reference, which are inferred as amplifications or deletions, respectively. The analysis was run with a noise cutoff of 0.1, as recommended for 10x Genomics data, and cells in the final output were grouped by their pre-assigned cluster annotation. The final results were visualized as a heatmap displaying the inferred CNV landscape across the genome for every cell.","Seurat, infercnv, RColorBrewer",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for inferCNV analysis and data handling
library(infercnv)      # Main package for inferring copy number variations
library(Seurat)        # Used for handling single-cell RNA-seq data
library(RColorBrewer)  # Provides color palettes for plotting

#--------------------------------
# Data Input
#--------------------------------

# Load a pre-processed Seurat object containing your scRNA-seq data.
# NOTE: Replace the path with the actual location of your .RDS file.
pbmc <- readRDS(""../data/pbmc.RDS"")

# Extract the raw count matrix from the Seurat object.
# inferCNV requires a raw integer count matrix with genes as rows and cells as columns.
raw_counts <- pbmc@assays$RNA@counts

#--------------------------------
# Prepare inferCNV Inputs
#--------------------------------

# --- Create Cell Annotation File ---
# This file maps each cell ID to a specific cell type or cluster annotation.
# It should be a two-column, tab-delimited file without headers.
cell_annotations <- data.frame(
  cell_id = colnames(pbmc),
  cell_type = as.character(pbmc@meta.data$seurat_clusters),
  stringsAsFactors = FALSE
)
# Write the annotation file to disk.
write.table(
  cell_annotations,
  file = ""celltype_pbmc.txt"",
  sep = ""\t"",
  row.names = FALSE,
  col.names = FALSE,
  quote = FALSE
)

# --- Gene Order File ---
# This file provides the genomic coordinates for each gene, which inferCNV uses
# to order the genes along chromosomes in the final heatmap.
# It should be a three-column file: Gene, Chromosome, Start Position.
# NOTE: You must provide your own gene order file (e.g., ""gencode_v19_gene_pos.txt"").
gene_order_file_path <- ""gencode_v19_gene_pos.txt""

#--------------------------------
# Major Analysis Task: Run inferCNV
#--------------------------------

# --- Create inferCNV Object ---
# This object is the central data structure for the analysis.
# `ref_group_names` specifies which cell groups to use as the ""normal"" reference
# for calculating relative expression changes.
infercnv_obj <- CreateInfercnvObject(
  raw_counts_matrix = as.matrix(raw_counts),
  annotations_file = ""celltype_pbmc.txt"",
  gene_order_file = gene_order_file_path,
  ref_group_names = c(""0"", ""8"") # Example: Use clusters 0 and 8 as reference
)

# --- Run the inferCNV pipeline ---
# This is the main computational step. It normalizes expression, centers it
# relative to the reference cells, and identifies large-scale chromosomal alterations.
infercnv_obj <- infercnv::run(
  infercnv_obj,
  cutoff = 0.1,                 # Recommended for 10x Genomics data
  out_dir = ""infercnv_output/"", # Directory to store all output files
  cluster_by_groups = TRUE,     # Group cells by their annotation
  denoise = FALSE,              # Optional: apply denoising
  HMM = FALSE,                  # Optional: run Hidden Markov Model for CNV prediction
  no_prelim_plot = TRUE,        # Skip plotting the preliminary heatmap
  analysis_mode = ""subclusters"" # Analyze at the subcluster level for more detail
)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# Define a custom color palette for the final CNV heatmap.
custom_color_pal <- colorRampPalette(c(""#8DD3C7"", ""white"", ""#BC80BD""))(100)

# Generate the final CNV plot. This creates a PDF file in the output directory.
# The plot visualizes inferred amplifications (purple) and deletions (blue-green)
# across the genome for each cell.
infercnv::plot_cnv(
  infercnv_obj,
  plot_chr_scale = TRUE,
  output_filename = ""infercnv_final_plot"",
  output_format = ""pdf"",
  custom_color_pal = custom_color_pal
)

#--------------------------------
# Data Output
#--------------------------------

# The `infercnv::run` function automatically saves the results and plots
# in the specified `out_dir`. The final inferCNV object is also returned,
# which can be saved for later inspection if needed.
saveRDS(infercnv_obj, file = ""infercnv_output/final_infercnv_object.rds"")

print(""\ninferCNV analysis completed! Check the 'infercnv_output' directory for results."")
",https://blog.csdn.net/weixin_56845253/article/details/130551826,F
scRNA-seq,Annotation,"Large-scale copy number alterations were inferred from transcriptomic data using the inferCNV R package on a pre-processed Seurat object. Normal reference populations (clusters 0 and 18) were designated, while tumor cells were downsampled to 3,000 cells for computational efficiency. A raw integer count matrix was centered relative to the reference groups, and CNV patterns were identified via hierarchical clustering (ward.D2). Genomic instability was quantified as a CNV score—the sum of squared residual expression values—and its distribution across cell clusters was examined using boxplots to highlight populations with significant aberrations.","To infer large-scale copy number alterations from the transcriptomic data, we used the **inferCNV** R package.

The analysis was performed on a pre-processed Seurat object. Cells from clusters 0 and 18 were designated as the diploid ""Normal"" reference population, with the remaining cells classified as ""Tumor."" To manage computational resources, the tumor population was randomly downsampled to a maximum of 3,000 cells, which were then combined with all normal reference cells for the analysis. A raw integer count matrix was extracted from this subsetted data.

The core inferCNV analysis was run using clusters 0 and 18 as the reference group. The pipeline was executed with parameters recommended for 10x Genomics data (`cutoff = 0.1`), with denoising enabled to enhance the signal-to-noise ratio. Cells were hierarchically clustered based on their CNV profiles using the ""ward.D2"" method. This process generates a matrix where gene expression for each cell is centered relative to the average expression of the reference populations, revealing large-scale chromosomal regions with aberrant expression patterns indicative of copy number variations.

To quantify the overall genomic instability for each cell, a **CNV score** was calculated from the inferCNV output. This score was computed as the sum of the squared residual expression values across all genes for each cell, providing a single metric of deviation from the diploid reference. The distribution of these CNV scores was then compared across the original cell clusters using boxplots to identify cell populations with the highest degree of genomic alteration.","Seurat, dplyr, tibble, infercnv, ggpubr",R,"#--------------------------------
# Package Load
#--------------------------------

# Load required libraries for analysis, data handling, and plotting
library(Seurat)        # For single-cell data management
library(dplyr)         # For data manipulation
library(tibble)        # For advanced data frame functionalities
library(infercnv)      # Main package for inferring copy number variations
library(ggpubr)        # For creating publication-quality plots

#--------------------------------
# Data Input
#--------------------------------

# Load a pre-processed Seurat object.
# NOTE: Replace the path with the actual location of your .RDS file.
scRNA_obj <- readRDS(""./scRNA_harmony_EP.rds"")

#--------------------------------
# Prepare inferCNV Inputs
#--------------------------------

# --- Annotate and Subset Cells ---
# Annotate cell types based on clustering information.
# Here, clusters ""0"" and ""18"" are defined as ""Normal"" reference cells.
meta_data <- scRNA_obj@meta.data %>%
  mutate(cell_type = case_when(
    RNA_snn_res.0.6 %in% c(""0"", ""18"") ~ ""Normal"",
    TRUE ~ ""Tumor""
  ))

# Get cell IDs for tumor and normal groups.
tumor_cells <- rownames(meta_data)[meta_data$cell_type == ""Tumor""]
normal_cells <- rownames(meta_data)[meta_data$cell_type == ""Normal""]

# To manage computational load, randomly sample a subset of tumor cells.
set.seed(123)
tumor_sample <- sample(tumor_cells, min(3000, length(tumor_cells)), replace = FALSE)

# Combine the sampled tumor cells with all normal cells for the analysis.
selected_cells <- c(tumor_sample, normal_cells)
new_scRNA <- subset(scRNA_obj, cells = selected_cells)


# --- Extract Expression Matrix ---
# inferCNV requires a raw integer count matrix.
exprMatrix <- as.matrix(GetAssayData(new_scRNA, slot = ""counts""))


# --- Create Cell Annotation File ---
# This file maps each cell ID to its cluster ID. inferCNV uses this for grouping.
# NOTE: This is different from the Tumor/Normal annotation used for sampling.
cellAnnot <- new_scRNA@meta.data[, ""RNA_snn_res.0.6"", drop = FALSE]


# --- Gene Order File ---
# This file provides the genomic coordinates for each gene.
# NOTE: You must provide your own gene order file.
gene_order_file_path <- ""path/to/gene_pos.txt""


#--------------------------------
# Major Analysis Task: Run inferCNV
#--------------------------------

# --- Create inferCNV Object ---
# `ref_group_names` specifies which clusters to use as the ""normal"" reference.
infercnv_obj <- CreateInfercnvObject(
  raw_counts_matrix = exprMatrix,
  annotations_file = cellAnnot, # This can be a data.frame directly
  delim = ""\t"",
  gene_order_file = gene_order_file_path,
  ref_group_names = c(""0"", ""18"") # Use normal clusters as reference
)

# --- Run the inferCNV pipeline ---
# This is the main computational step.
infercnv_obj <- infercnv::run(
  infercnv_obj,
  cutoff = 0.1,                 # Recommended for 10x Genomics data
  out_dir = ""./output/cnv_epithelial/"", # Directory for all output files
  cluster_by_groups = FALSE,    # Cluster cells across all groups
  hclust_method = ""ward.D2"",    # Hierarchical clustering method
  denoise = TRUE,               # Apply denoising to improve signal
  HMM = FALSE                   # Disable HMM-based CNV prediction in this run
)


#--------------------------------
# Downstream Analysis: CNV Scoring (Optional)
#--------------------------------

# --- Calculate CNV Score ---
# This section calculates a per-cell CNV score based on the inferCNV output.
# Read the observation and reference matrices generated by infercnv::run.
obs <- read.table(""./output/cnv_epithelial/infercnv.observations.txt"", header = TRUE)
ref <- read.table(""./output/cnv_epithelial/infercnv.references.txt"", header = TRUE)

# Combine, scale, and calculate a squared residual score.
expr <- cbind(obs, ref)
expr_scale <- scale(t(expr))
tmp1 <- sweep(expr_scale, 2, apply(expr_scale, 2, min), FUN = ""-"")
tmp2 <- apply(expr_scale, 2, max) - apply(expr_scale, 2, min)
expr_1 <- t(2 * sweep(tmp1, 2, tmp2, FUN = ""/"") - 1)
cnv_score_df <- as.data.frame(colSums(expr_1 * expr_1))
colnames(cnv_score_df) <- ""cnv_score""

# Format the CNV score DataFrame.
cnv_score_df <- cnv_score_df %>%
  rownames_to_column(var = ""cell"") %>%
  mutate(cell = gsub(""\\."", ""-"", cell)) # Ensure cell names match Seurat object


# --- Merge Scores into Seurat Object ---
# Subset the Seurat object to match the cells with CNV scores and merge the scores.
scRNA_sample <- subset(scRNA_obj, cells = cnv_score_df$cell)
meta_with_cnv <- scRNA_sample@meta.data %>%
  rownames_to_column(var = ""cell"") %>%
  inner_join(cnv_score_df, by = ""cell"") %>%
  column_to_rownames(var = ""cell"")


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- CNV Score Boxplot ---
# Create a boxplot to visualize the distribution of CNV scores across different clusters.
cnv_boxplot <- ggboxplot(
  meta_with_cnv,
  x = ""RNA_snn_res.0.6"",
  y = ""cnv_score"",
  fill = ""RNA_snn_res.0.6""
) +
scale_y_continuous(limits = c(0, 3000)) +
xlab(""Cluster"") +
ylab(""CNV Score"") +
theme(
  panel.border = element_rect(colour = ""black"", fill = NA, size = 0.5),
  legend.position = ""none""
)
print(cnv_boxplot)


#--------------------------------
# Data Output
#--------------------------------

# The `infercnv::run` function automatically saves the main heatmap and other results.
# Here, we explicitly save the final inferCNV object and the CNV score plot.
saveRDS(infercnv_obj, file = ""./output/cnv_epithelial/final_infercnv_object.rds"")

# Save the CNV score boxplot to a PDF file.
ggsave(
  plot = cnv_boxplot,
  filename = ""./output/cnv_epithelial/CNV_score_by_cluster.pdf"",
  width = 7,
  height = 4
)

print(""\ninferCNV analysis and CNV scoring completed!"")
",https://blog.csdn.net/lijianpeng0302/article/details/145998388,F
scRNA-seq,Annotation,"Somatic CNVs were inferred from single-cell transcriptomic data using inferCNVpy in Python. The analysis employed a gene expression matrix with non-malignant cells (B cells, T cells, macrophages, and dendritic cells) as the diploid reference. A rolling average over 250 genes was computed and centered against the reference to reveal chromosomal amplifications and deletions. Subsequent PCA and Leiden clustering, together with UMAP visualization, captured genomic heterogeneity, while per-cell CNV scores quantified deviations from the diploid state. Ultimately, cells were manually classified as ""tumor"" or ""normal"" based on clustering results and chromosome-wide heatmaps.","To infer somatic copy number variations (CNVs) from single-cell transcriptomic data, we utilized the **inferCNVpy** package in Python.

The core analysis was performed on the gene expression matrix, using a comprehensive set of non-malignant cell types—including B cells, T cells, macrophages, and dendritic cells—as the diploid reference population. The inferCNV algorithm calculates a rolling average of gene expression for each cell in a genomic window of 250 genes. This expression profile is then centered relative to the average expression of the reference cell populations. This process effectively highlights large-scale chromosomal regions where gene expression is consistently elevated or suppressed, which are inferred to be copy number amplifications or deletions, respectively.

Following the inference of per-cell CNV profiles, we performed several downstream analyses to interpret the results. First, cells were clustered based exclusively on their CNV profiles using a Principal Component Analysis (PCA) followed by Leiden community detection. This step grouped cells with similar patterns of genomic alterations. A new two-dimensional Uniform Manifold Approximation and Projection (UMAP) embedding was also computed from the CNV profiles to visualize the genomic heterogeneity across the cell population. Additionally, a per-cell CNV score was calculated to quantify the overall magnitude of deviation from the reference diploid state.

Finally, based on a combination of the CNV-based clustering and the chromosome heatmap visualizations, cells were manually classified as either ""tumor"" or ""normal."" The results were visualized using chromosome-wide heatmaps to display the inferred CNV landscape and on the CNV-based UMAP embedding to show the separation of the classified tumor and normal populations.","scanpy, infercnvpy, matplotlib, warnings",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for data handling, analysis, plotting, and single-cell analysis
import scanpy as sc
import infercnvpy as cnv
import matplotlib.pyplot as plt
import warnings

#--------------------------------
# Script Configuration
#--------------------------------

# Suppress warnings for a cleaner output during execution
warnings.simplefilter(""ignore"")
# Set global plotting parameters for clear and consistent visualization
sc.settings.set_figure_params(figsize=(5, 5))

#--------------------------------
# Data Input
#--------------------------------

# Load an example dataset for inferCNV analysis.
# This dataset from Maynard et al., 2020 is suitable for this purpose.
print(""Loading Maynard et al. 2020 dataset..."")
adata = cnv.datasets.maynard2020_3k()

# Display gene annotations to confirm the required columns (chromosome, start, end) are present.
print(adata.var.loc[:, [""ensg"", ""chromosome"", ""start"", ""end""]].head())


#--------------------------------
# Major Analysis Tasks: CNV Inference and Clustering
#--------------------------------

# --- 1. Run inferCNV ---
# This is the main computational step. It infers copy number variations by comparing
# gene expression in target cells to a set of reference (""normal"") cells.
print(""\nRunning infercnvpy analysis..."")
cnv.tl.infercnv(
    adata,
    reference_key=""cell_type"",
    # Define which cell types are used as the normal reference
    reference_cat=[
        ""B cell"", ""Macrophage"", ""Mast cell"", ""Monocyte"", ""NK cell"",
        ""Plasma cell"", ""T cell CD4"", ""T cell CD8"", ""T cell regulatory"",
        ""mDC"", ""pDC""
    ],
    window_size=250 # Size of the sliding window for averaging expression
)

# --- 2. CNV-based Clustering ---
# Cluster cells based on their inferred CNV profiles to identify groups
# of cells with similar large-scale genomic alterations.
print(""\nPerforming CNV-based clustering..."")
# Perform PCA on the CNV matrix.
cnv.tl.pca(adata)
# Compute a neighborhood graph on the PCA results.
cnv.pp.neighbors(adata)
# Run Leiden clustering to find CNV-based clusters.
cnv.tl.leiden(adata)

# --- 3. CNV UMAP and Scoring ---
# Compute a UMAP embedding and a per-cell CNV score based on the CNV profiles.
print(""Computing CNV UMAP and scores..."")
cnv.tl.umap(adata)
cnv.tl.cnv_score(adata)


# --- 4. Tumor vs. Normal Classification ---
# Manually classify cells as ""tumor"" or ""normal"" based on the CNV clusters.
# NOTE: The cluster IDs to be labeled as ""tumor"" need to be determined by inspecting
# the chromosome heatmap and CNV scores from the plots below.
print(""Classifying tumor vs. normal cells..."")
adata.obs[""cnv_status""] = ""normal""
# Example: mark cells in specific cnv_leiden clusters as tumor
tumor_clusters = [""10"", ""16"", ""13"", ""8"", ""12"", ""17"", ""1"", ""14"", ""11""]
adata.obs.loc[adata.obs[""cnv_leiden""].isin(tumor_clusters), ""cnv_status""] = ""tumor""


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Initial Data Visualization ---
# Visualize the original UMAP from transcriptomics data to see cell type distribution.
print(""\n--- Visualizing Results ---"")
sc.pl.umap(adata, color=""cell_type"", title=""Transcriptomics UMAP by Cell Type"")

# --- CNV Heatmaps ---
# Plot a heatmap of inferred CNVs across the genome, grouped by original cell type.
# This shows which cell types exhibit large-scale genomic alterations.
cnv.pl.chromosome_heatmap(adata, groupby=""cell_type"", title=""CNV Heatmap by Cell Type"")

# Plot the CNV heatmap again, but grouped by the new CNV-based Leiden clusters.
# A dendrogram is included to show the hierarchy of clusters.
cnv.pl.chromosome_heatmap(adata, groupby=""cnv_leiden"", dendrogram=True, title=""CNV Heatmap by CNV Cluster"")

# --- CNV UMAPs ---
# Create a multi-panel plot to visualize the CNV-based UMAP embedding.
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(11, 11))
ax4.axis(""off"") # Turn off the unused fourth subplot
# Plot CNV UMAP colored by:
# 1. CNV Leiden cluster
cnv.pl.umap(adata, color=""cnv_leiden"", legend_loc=""on data"", legend_fontoutline=2, ax=ax1, show=False, title=""CNV UMAP by CNV Leiden Cluster"")
# 2. CNV score
cnv.pl.umap(adata, color=""cnv_score"", ax=ax2, show=False, title=""CNV UMAP by CNV Score"")
# 3. Original cell type annotation
cnv.pl.umap(adata, color=""cell_type"", ax=ax3, show=False, title=""CNV UMAP by Original Cell Type"")
plt.tight_layout()
plt.show()


# --- Final Classification Visualization ---
# Visualize the final tumor vs. normal classification on both the CNV-based UMAP
# and the original transcriptomics-based UMAP.
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
fig.suptitle(""Final Tumor/Normal Classification"")
# CNV UMAP
cnv.pl.umap(adata, color=""cnv_status"", ax=ax1, show=False)
# Transcriptomics UMAP
sc.pl.umap(adata, color=""cnv_status"", ax=ax2, show=False)
plt.show()

# Generate separate CNV heatmaps for the classified tumor and normal cells.
cnv.pl.chromosome_heatmap(adata[adata.obs[""cnv_status""] == ""tumor"", :], title=""CNV Heatmap of Tumor Cells"")
cnv.pl.chromosome_heatmap(adata[adata.obs[""cnv_status""] == ""normal"", :], title=""CNV Heatmap of Normal Cells"")


#--------------------------------
# Data Output
#--------------------------------

# Save the final AnnData object, which contains all inferCNVpy analysis results.
print(""\nSaving final AnnData object..."")
adata.write(""maynard2020_with_infercnv.h5ad"")

print(""\ninferCNVpy analysis completed!"")
",https://infercnvpy.readthedocs.io/en/latest/notebooks/tutorial_3k.html,F
scRNA-seq,Trajectory inference,"Developmental trajectories and cell fate decisions were inferred from a bone marrow dataset using the Palantir algorithm in Python. After library size normalization, log transformation, and feature selection of the top 1,500 variable genes, PCA reduction with MAGIC imputation was applied. Diffusion maps were computed (five components retained) to model the cellular manifold. A progenitor cell was designated as the trajectory root, with terminal states defined for dendritic, monocyte, and erythrocyte lineages. Pseudotime values and branching probabilities quantified progression and fate likelihoods, and key markers (CD34, MPO, GATA1, IRF8) were plotted to validate the inferred developmental continuum.","To infer developmental trajectories and cell fate choices, we performed a pseudotime analysis using the **Palantir** algorithm in Python.

The analysis was conducted on a bone marrow dataset. Initial preprocessing included normalizing each cell for library size, followed by a log transformation. Feature selection was performed to identify the top 1,500 most highly variable genes, and the dimensionality of this data was reduced using **Principal Component Analysis (PCA)**. To facilitate smoother visualization of gene expression dynamics, a denoised data matrix was generated using MAGIC imputation.

To model the underlying phenotypic manifold, we computed **diffusion maps** on the PCA-reduced data, retaining five components to represent the data structure. The Palantir algorithm was then applied to infer developmental trajectories. A specific progenitor cell was designated as the start of the trajectory, and distinct cell populations corresponding to Dendritic, Monocyte, and Erythrocyte lineages were defined as terminal states. Palantir calculated a **pseudotime** value for each cell, representing its progression along the developmental continuum, as well as a set of **branching probabilities**, quantifying the likelihood of each cell differentiating toward each of the defined terminal fates. The resulting trajectory was visualized on the diffusion map embedding, and the expression trends of key lineage-specific marker genes, such as *CD34*, *MPO*, *GATA1*, and *IRF8*, were plotted against pseudotime to validate the inferred developmental progressions.","os, warnings, pathlib, numpy, pandas, scanpy, palantir, matplotlib, numba",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for system operations, data handling, analysis, and plotting
import os
import warnings
from pathlib import Path
import numpy as np
import pandas as pd
import scanpy as sc
import palantir
import matplotlib.pyplot as plt
from numba.core.errors import NumbaDeprecationWarning

#--------------------------------
# Script Configuration
#--------------------------------

# Suppress specific warnings for a cleaner output during execution
warnings.filterwarnings(""ignore"", category=NumbaDeprecationWarning)
warnings.filterwarnings(""ignore"", module=""scanpy"", message=""No data for colormapping"")
# Set global plotting parameters
sc.set_figure_params(figsize=(5, 5), frameon=False)

#--------------------------------
# Data Input
#--------------------------------

# Define the directory and file path for the dataset
data_dir = os.path.expanduser(""./"")
download_url = ""https://dp-lab-data-public.s3.amazonaws.com/palantir/marrow_sample_scseq_counts.h5ad""
file_path = os.path.join(data_dir, ""marrow_sample_scseq_counts.h5ad"")

# Read the single-cell dataset. A backup URL is provided to download it if not found locally.
print(""Loading marrow sample dataset..."")
adata = sc.read(file_path, backup_url=download_url)

#--------------------------------
# Data Preprocessing
#--------------------------------

# 1. Normalize counts per cell to account for library size differences.
print(""\nPreprocessing data..."")
sc.pp.normalize_per_cell(adata)

# 2. Log-transform the data. Palantir's wrapper uses a pseudocount of 0.1.
palantir.preprocess.log_transform(adata)

# 3. Identify highly variable genes (HVGs) to focus on biological signal.
sc.pp.highly_variable_genes(adata, n_top_genes=1500, flavor=""cell_ranger"")

# 4. Run PCA to reduce dimensionality and mitigate sparsity.
sc.pp.pca(adata)

# 5. (Optional) Impute gene expression using MAGIC for smoother visualizations of gene trends.
# Note: This creates a dense matrix and can be memory-intensive.
print(""Running MAGIC imputation..."")
imputed_X = palantir.utils.run_magic_imputation(adata)


#--------------------------------
# Major Analysis Task: Trajectory Inference with Palantir
#--------------------------------

# --- Step 1: Diffusion Maps ---
# Compute diffusion maps to estimate a low-dimensional phenotypic manifold.
print(""\nComputing diffusion maps..."")
dm_res = palantir.utils.run_diffusion_maps(adata, n_components=5)

# Determine the multiscale space based on the eigenspectrum of the diffusion operator.
# This helps to select the optimal number of components for representing the manifold.
ms_data = palantir.utils.determine_multiscale_space(adata)


# --- Step 2: Trajectory Calculation ---
# Define terminal cell states. These are the endpoints of the developmental trajectories.
# NOTE: These cell IDs are examples and must be replaced with relevant cells from your dataset.
terminal_states = pd.Series(
    [""DC"", ""Mono"", ""Ery""],
    index=[""Run5_131097901611291"", ""Run5_134936662236454"", ""Run4_200562869397916""]
)

# Define the start cell for the trajectory analysis.
# This is often a cell representing a progenitor or stem cell state.
# NOTE: This cell ID is an example and must be adjusted for your data.
start_cell = ""Run5_164698952452459""

# Run the core Palantir algorithm to calculate pseudotime and cell fate probabilities.
print(""\nRunning Palantir trajectory analysis..."")
pr_res = palantir.core.run_palantir(
    adata,
    start_cell,
    num_waypoints=500,
    terminal_states=terminal_states
)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Diffusion Map Visualization ---
print(""\n--- Visualizing Results ---"")
# Plot the computed diffusion components.
palantir.plot.plot_diffusion_components(adata)
plt.show()

# --- UMAP Visualization ---
# Compute a UMAP embedding for a general overview of the data structure.
sc.pp.neighbors(adata)
sc.tl.umap(adata)
sc.pl.embedding(adata, basis=""umap"", color=""clusters"", frameon=False, title=""UMAP by Cluster"")
plt.show()

# Highlight the chosen terminal state cells on the UMAP.
palantir.plot.highlight_cells_on_umap(adata, terminal_states)
plt.show()

# --- Palantir Results Visualization ---
# Plot the main Palantir results: pseudotime and terminal state probabilities.
palantir.plot.plot_palantir_results(adata, s=3)
plt.show()

# --- Gene Expression Trend Visualization ---
# Plot the imputed expression trends for selected marker genes along pseudotime.
genes = [""CD34"", ""MPO"", ""GATA1"", ""IRF8""]
palantir.plot.plot_gene_trends(adata, genes)
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the processed AnnData object containing all Palantir results.
print(""\nSaving processed data object..."")
output_path = os.path.join(data_dir, ""marrow_sample_palantir_processed.h5ad"")
adata.write(output_path)

print(""\nPalantir trajectory analysis completed!"")
",https://palantir.readthedocs.io/en/latest/notebooks/Palantir_sample_notebook.html,F
scRNA-seq,Trajectory inference,"RNA velocity analysis was conducted using scVelo on a public mouse pancreas dataset to elucidate cellular differentiation dynamics. After filtering genes with a minimum of 20 shared spliced and unspliced counts and selecting the top 2,000 variable genes following normalization and log-transformation, the first and second-order moments were computed for each cell in a 30-neighbor, 30-dimensional PCA space. A stochastic model estimated transcription, splicing, and degradation rates, yielding high-dimensional RNA velocity vectors. These vectors were used to construct a transition graph and derive velocity-based pseudotime, visualized as streamlines on a UMAP, revealing directional flows of development.","To infer the dynamics of cellular differentiation, we performed **RNA velocity analysis** using the **scVelo** package in Python. The analysis was conducted on a publicly available single-cell RNA sequencing dataset of the developing mouse pancreas.

Initial preprocessing of the data involved filtering genes to retain those with a minimum of 20 shared spliced and unspliced counts across cells. The expression data for each cell was then normalized by its total library size, and the top 2,000 most highly variable genes were selected for downstream analysis. The data was subsequently log-transformed. To prepare for velocity estimation, the first and second-order moments of gene expression were computed for each cell across its 30 nearest neighbors in a 30-dimensional principal component space.

The core RNA velocity inference was performed by fitting a stochastic model to the spliced and unspliced mRNA counts of each gene. This model estimates the rates of transcription, splicing, and degradation, thereby inferring a high-dimensional velocity vector that predicts the future transcriptional state for each individual cell. These cell-specific velocity vectors were then used to construct a velocity graph, which represents the likely transitions and connections between cell states. Furthermore, a **velocity-based pseudotime** was calculated to order cells along the dynamic trajectories inferred from the velocity graph.

The results were visualized by projecting the inferred velocity field as streamlines onto a UMAP embedding of the data. This visualization revealed the principal directional flows of cellular development within the pancreas, and the velocity-inferred pseudotime confirmed the continuous progression of cells along these developmental paths.","scanpy, scvelo, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for RNA velocity analysis, data handling, and plotting
import scvelo as scv
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set scVelo global parameters for logging and plotting aesthetics
scv.settings.verbosity = 3          # Set for detailed logging
scv.settings.presenter_view = True  # Use settings suitable for presentation plots
scv.set_figure_params('scvelo')     # Apply the scVelo default plotting style

#--------------------------------
# Data Input
#--------------------------------

# Load the pancreas dataset, a built-in example from scVelo.
# NOTE: To use your own data, you would replace this line, e.g.:
# adata = scv.read('path/to/your/file.loom', cache=True)
print(""Loading pancreas dataset..."")
adata = scv.datasets.pancreas()

#--------------------------------
# Preprocessing
#--------------------------------

# Apply a standard scVelo preprocessing recipe. This function:
# 1. Filters genes based on a minimum number of shared counts.
# 2. Normalizes each cell by its total counts.
# 3. Selects the top 2000 highly variable genes.
# 4. Log-transforms the expression data.
print(""\nPreprocessing data..."")
scv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)

# Compute the first and second-order moments of expression for velocity estimation.
# This step internally performs PCA and computes the neighborhood graph.
scv.pp.moments(adata, n_pcs=30, n_neighbors=30)


#--------------------------------
# Major Analysis Task: RNA Velocity Inference
#--------------------------------

# --- Velocity Estimation ---
# Compute RNA velocities using the default stochastic model. This model estimates
# transcription, splicing, and degradation rates to infer the future state of cells.
print(""\nEstimating RNA velocity..."")
scv.tl.velocity(adata)

# --- Velocity Graph ---
# Construct the velocity graph, which represents likely cell state transitions
# based on the computed velocities.
scv.tl.velocity_graph(adata)

# --- Pseudotime Calculation (Optional) ---
# Calculate velocity-based pseudotime, which orders cells along a trajectory
# inferred from the velocity graph.
print(""Calculating velocity pseudotime..."")
scv.tl.velocity_pseudotime(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Velocity Streamline Plot ---
# Visualize the RNA velocity field as streamlines on a UMAP embedding.
# This provides a smooth representation of the directional flow.
print(""\n--- Visualizing Results ---"")
scv.pl.velocity_embedding_stream(
    adata,
    basis='umap',
    figsize=(8, 8),
    title=""Velocity Streamlines on UMAP""
)

# --- Velocity Arrow Plot ---
# Visualize velocities as individual arrows on the UMAP embedding.
# This shows the direction and magnitude of change for local groups of cells.
scv.pl.velocity_embedding(
    adata,
    arrow_length=3,
    arrow_size=2,
    dpi=120,
    title=""Velocity Vectors on UMAP""
)

# --- Pseudotime Plot ---
# Visualize the calculated velocity pseudotime on the UMAP embedding.
# This plot should show a continuous gradient along the inferred developmental paths.
scv.pl.scatter(
    adata,
    color='velocity_pseudotime',
    cmap='gnuplot',
    size=50,
    title=""Velocity-based Pseudotime""
)

# Ensure all plots are displayed, especially when running as a script.
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object containing all RNA velocity analysis results.
print(""\nSaving processed data object..."")
adata.write(""pancreas_scvelo_processed.h5ad"")

print(""\nscVelo analysis completed!"")
",https://scvelo.readthedocs.io/en/stable/VelocityBasics.html,F
scRNA-seq,Trajectory inference,"RNA velocity was modeled on a developing mouse pancreas single‐cell dataset using the scVelo dynamical model in Python. After filtering low-abundance spliced/unspliced genes and normalizing library sizes, the top 2,000 variable genes were selected, and first- and second-order moments were computed from a 30-dimensional PCA over 30 nearest neighbors. The dynamical model estimated transcriptional kinetics—transcription, splicing, and degradation rates—to derive high-dimensional velocity vectors and a cell-state transition graph. A latent time for each cell was computed to position differentiation states, with the resulting velocity field visualized on UMAP and key transcriptional shifts highlighted in a heatmap.","To quantitatively model cellular dynamics, we performed an **RNA velocity** analysis using the dynamical model implemented in the **scVelo** Python package. The analysis was conducted on a single-cell dataset of the developing mouse pancreas.

Initial data preprocessing involved filtering genes with low spliced/unspliced counts, normalizing each cell by its total library size, and selecting the top 2,000 most highly variable genes. The first and second-order moments of gene expression were then computed for each cell across its 30 nearest neighbors in a 30-dimensional principal component space.

To infer the full transcriptional kinetics, we fit the **dynamical model** to the data on a gene-by-gene basis. This step solves a system of differential equations to explicitly estimate the rates of transcription, splicing, and degradation for each gene. These learned kinetic parameters were subsequently used to compute a robust, high-dimensional velocity vector for each cell. Based on these vectors, a velocity graph representing cell-state transition probabilities was constructed. We then calculated a **latent time** for each cell, which provides a continuous measure of a cell's position along the differentiation trajectory, derived directly from the learned transcriptional dynamics.

The inferred velocity field was visualized as streamlines on a UMAP embedding to illustrate the directional flow of differentiation. To identify the key transcriptional changes driving this process, we visualized the top 300 genes with the highest likelihood scores from the dynamical model on a heatmap, with genes ordered by the timing of their peak expression along the inferred latent time.","scanpy, scvelo, matplotlib",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for RNA velocity analysis, data handling, and plotting
import scvelo as scv
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Set scVelo global parameters for logging and plotting aesthetics
scv.settings.verbosity = 3
scv.settings.presenter_view = True
scv.set_figure_params('scvelo')

#--------------------------------
# Data Input
#--------------------------------

# Load the pancreas dataset, a built-in example from scVelo.
# NOTE: To use your own data, you would replace this line, e.g.:
# adata = scv.read('path/to/your/file.loom', cache=True)
print(""Loading pancreas dataset..."")
adata = scv.datasets.pancreas()

#--------------------------------
# Preprocessing
#--------------------------------

# Apply a standard scVelo preprocessing recipe.
print(""\nPreprocessing data..."")
scv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)

# Compute the first and second-order moments of expression for velocity estimation.
scv.pp.moments(adata, n_pcs=30, n_neighbors=30)


#--------------------------------
# Major Analysis Task: Dynamical RNA Velocity Inference
#--------------------------------

# --- Recover Full Splicing Kinetics ---
# Run the dynamical model to recover the full transcriptional dynamics.
# This step estimates rate parameters for transcription, splicing, and degradation for each gene.
print(""\nRecovering transcriptional dynamics..."")
scv.tl.recover_dynamics(adata)

# --- Velocity Estimation (Dynamical Model) ---
# Compute RNA velocities using the 'dynamical' mode, which leverages the learned kinetics.
# This provides a more robust and quantitative estimation of cell state transitions.
print(""Estimating RNA velocity using the dynamical model..."")
scv.tl.velocity(adata, mode='dynamical')

# --- Velocity Graph and Latent Time ---
# Construct the velocity graph to infer cell-to-cell transition probabilities.
scv.tl.velocity_graph(adata)

# Calculate latent time, a measure of a cell's internal clock based on its transcriptional
# dynamics. It represents the cell's position along the differentiation trajectory.
print(""Calculating latent time..."")
scv.tl.latent_time(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Velocity Streamline Plot ---
# Visualize the RNA velocity field as streamlines on the UMAP embedding.
print(""\n--- Visualizing Results ---"")
scv.pl.velocity_embedding_stream(
    adata,
    basis='umap',
    title=""Dynamical RNA Velocity Stream on UMAP""
)

# --- Latent Time Plot ---
# Visualize the calculated latent time on the UMAP embedding.
# This should show a continuous progression along the inferred developmental paths.
scv.pl.scatter(
    adata,
    color='latent_time',
    color_map='gnuplot',
    size=80,
    title=""Velocity-based Latent Time""
)

# --- Heatmap of Top Dynamic Genes ---
# Visualize the expression dynamics of the top 300 genes with the highest likelihood
# of being explained by the dynamical model. Genes are ordered by their peak expression
# time along the latent time trajectory.
top_genes = adata.var['fit_likelihood'].sort_values(ascending=False).index[:300]
scv.pl.heatmap(
    adata,
    var_names=top_genes,
    sortby='latent_time',
    col_color='clusters',
    n_convolve=100,
    title=""Heatmap of Top Dynamic Genes""
)

# Ensure all plots are displayed.
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object containing all RNA velocity analysis results.
print(""\nSaving processed data object..."")
adata.write(""pancreas_scvelo_dynamical_processed.h5ad"", compression='gzip')

print(""\nscVelo dynamical analysis completed!"")
",https://scvelo.readthedocs.io/en/stable/DynamicalModeling.html,F
scRNA-seq,Trajectory inference,"Cellular dynamics and differentiation trajectories in the developing mouse dentate gyrus were inferred using the scVelo package. Following standard preprocessing—including filtering for genes with sufficient spliced and unspliced counts, library size normalization, and selection of the top 2,000 variable genes—the data underwent log transformation and moment computation in a PCA space. RNA velocity was estimated using a stochastic model to predict future transcriptional states, with velocity vectors constructing a graph of state transitions. These dynamics were then visualized on a UMAP embedding with streamline and arrow plots.","To infer cellular dynamics and differentiation trajectories, we performed an **RNA velocity analysis** using the **scVelo** package in Python. The analysis was conducted on a single-cell dataset of the developing mouse dentate gyrus.

The raw data was first preprocessed using a standard scVelo workflow. This included filtering to retain genes with at least 30 shared spliced and unspliced counts, normalizing each cell by its total library size, and selecting the top 2,000 most highly variable genes for analysis. Following a log transformation, we computed the first and second-order moments of gene expression for each cell across its 30 nearest neighbors within a 30-dimensional principal component space.

The core RNA velocity inference was performed using the default **stochastic model**, which estimates the future transcriptional state of each cell by modeling the balance of unspliced and spliced mRNA. The resulting high-dimensional velocity vectors were then used to construct a velocity graph, which represents the learned probabilities of cell-to-cell state transitions. To visualize the inferred dynamics, the velocity field was projected onto a UMAP embedding of the data using both streamline plots to show the continuous directional flow and arrow plots to indicate the local direction and magnitude of change.","scanpy, scvelo",python,"#--------------------------------
# Import necessary modules
#--------------------------------

# Import libraries for RNA velocity analysis, data handling, and plotting
import scvelo as scv
import scanpy as sc
import matplotlib.pyplot as plt

#--------------------------------
# Script Configuration
#--------------------------------

# Print the scVelo version for reproducibility.
scv.logging.print_version()
# Set scVelo global parameters for logging and plotting aesthetics.
scv.settings.verbosity = 3
scv.settings.presenter_view = True
scv.set_figure_params('scvelo')

#--------------------------------
# Data Input
#--------------------------------

# Load the dentate gyrus dataset, a built-in example from scVelo.
# NOTE: To use your own data, you would replace this line, e.g.:
# adata = scv.read('path/to/your/file.loom', cache=True)
print(""Loading dentate gyrus dataset..."")
adata = scv.datasets.dentategyrus()

#--------------------------------
# Preprocessing
#--------------------------------

# Apply a standard scVelo preprocessing recipe. This function:
# 1. Filters genes based on a minimum number of shared counts.
# 2. Normalizes each cell by its total counts.
# 3. Selects the top 2000 highly variable genes.
# 4. Log-transforms the expression data.
print(""\nPreprocessing data..."")
scv.pp.filter_and_normalize(adata, min_shared_counts=30, n_top_genes=2000)

# Compute the first and second-order moments of expression for velocity estimation.
# This step internally performs PCA and computes the neighborhood graph.
scv.pp.moments(adata, n_pcs=30, n_neighbors=30)


#--------------------------------
# Major Analysis Task: RNA Velocity Inference
#--------------------------------

# --- Velocity Estimation ---
# Compute RNA velocities using the default stochastic model. This model infers
# the future state of individual cells based on the balance of unspliced and spliced mRNA.
print(""\nEstimating RNA velocity..."")
scv.tl.velocity(adata)

# --- Velocity Graph ---
# Construct the velocity graph, which represents likely cell state transitions
# based on the computed velocities. This captures the underlying cellular dynamics.
scv.tl.velocity_graph(adata)


#--------------------------------
# Plotting / Visualization
#--------------------------------

# --- Velocity Streamline Plot ---
# Visualize the RNA velocity field as streamlines on a UMAP embedding.
# This provides a smooth, continuous representation of the directional flow.
print(""\n--- Visualizing Results ---"")
scv.pl.velocity_embedding_stream(
    adata,
    basis='umap',
    figsize=(8, 8),
    dpi=120,
    title=""Velocity Streamlines on UMAP""
)

# --- Velocity Arrow Plot ---
# Visualize velocities as individual arrows on the UMAP embedding.
# This shows the direction and magnitude of change for local groups of cells.
scv.pl.velocity_embedding(
    adata,
    basis='umap',
    arrow_size=2,
    arrow_length=2,
    dpi=120,
    title=""Velocity Vectors on UMAP""
)

# Ensure all plots are displayed, especially when running as a script.
plt.show()


#--------------------------------
# Data Output
#--------------------------------

# Save the AnnData object containing all RNA velocity analysis results.
print(""\nSaving processed data object..."")
adata.write(""dentategyrus_scvelo_processed.h5ad"", compression='gzip')

print(""\nscVelo analysis completed!"")
",https://scvelo.readthedocs.io/en/stable/DifferentialKinetics.html,F
